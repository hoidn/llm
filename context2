<file path="./misc/errorspec.md" project="">
# Error Type Hierarchy and Handling Design

## Purpose
Define the core error types that enable control flow and task adaptation in the intelligent task execution system.

## Error Categories

### 1. Resource Exhaustion
- **Purpose**: Signal when system resource limits are exceeded
- **Characteristics**:
  - Parameterized resource type and limits (e.g., turns, context window)
  - Clear threshold for triggering
  - No partial results preserved
- **Control Flow Impact**: 
  - Signals that task requires more resources than available

### 2. Task Failure
- **Purpose**: Signal that a task cannot be completed as attempted
- **Characteristics**:
  - Generic failure mechanism
  - No internal categorization
  - No partial results preserved
- **Control Flow Impact**:
  - Task terminates
  - Control returns to parent task/evaluator

## Error Handling Principles

### 1. Separation of Concerns
- Errors purely signal failure conditions
- No recovery logic in error objects
- No state/progress tracking
- No partial results

### 2. Control Flow
- Resource Exhaustion → Task too large
- Task Failure → Termination
- No retry logic in components

### 3. Context Independence  
- Errors do not carry execution state.
- No partial results in errors.
- Clean separation from context management.

### Missing Argument Handling
When a task attempts to resolve an input, the evaluator first performs template substitution on any placeholders in the form `{{variable_name}}` within the task definition—using values from its current lexical environment. Additionally, if an `<input>` element specifies a `from` attribute, the evaluator binds that input using the value associated with that environment variable. If a required binding is missing, the evaluator returns a standard `TASK_FAILURE` error with a message such as "Missing required input: variable_name."

## Integration Points

All components in the unified task-execution system use the same generic error signaling.
In particular:
 - The Task System (and its unified execution component) detects resource exhaustion and signals a generic `TASK_FAILURE` with attached metadata.
 - The Memory System continues to provide context without carrying error state.

## Design Decisions & Rationale

1. Minimal Error Categories
   - Only essential control flow signals
   - Clear mapping to system behaviors
   - Simplified error handling

2. Stateless Error Design
   - Separates control flow from state
   - Clean component boundaries
   - Simplified recovery

3. No Complex Recovery
   - Decomposition as consequence not strategy
   - Simplified control flow
   - Clear system behavior

## Context Operation Failures

In scenarios where a task step calls `MemorySystem.getRelevantContextFor()` (or otherwise attempts context assembly), any failure is reported as a standard `TASK_FAILURE`.
Partial results are not preserved; on any failure, intermediate data is discarded.

In short, "context operation failures" are reported solely as `TASK_FAILURE`, and no partial sub-task outputs are retained.

## Dependencies
- Task system must detect resource limits

## Script Execution Errors
Script execution errors (e.g. non-zero exit codes) are captured and passed along to the evaluator for downstream decision-making rather than causing an immediate task failure.
- Evaluator must handle control flow
- Memory system must maintain context
</file>
<file path="./misc/operators.md" project="">
# Sequential and Reduce Operator Specification

## Purpose
Define the structure and semantics of Sequential and Reduce operators for task composition and execution, specifying XML schemas and execution behaviors.

## Memory Structure
```typescript
shortTermMemory: {
    files: Map<string, WorkingFile>;
    dataContext: string;
}
```

## Sequential Operator

### Purpose
Execute a series of tasks with explicit dependencies. Maintains execution order while allowing parallel execution of independent inputs.

### Structure
```xml
<task type="sequential">
    <description>Analyze {{dataset_name}} using provided configuration</description>
    <context_management>
        <inherit_context>none</inherit_context>
        <accumulate_data>true</accumulate_data>
        <accumulation_format>notes_only</accumulation_format>
    </context_management>
    <steps>
        <task>
            <description>Load initial data</description>
            <inputs>
                <input name="data" from="raw_data"/>
            </inputs>
        </task>
        <task>
            <description>Apply configuration from {{config_profile}}</description>
            <inputs>
                <input name="config" from="default_config"/>
            </inputs>
        </task>
    </steps>
</task>
```

### Context Management

#### Operator Default Settings

Each operator type has specific default context management settings that apply when no explicit configuration is provided:

| Operator Type | inherit_context | accumulate_data | accumulation_format | fresh_context |
|---------------|-----------------|-----------------|---------------------|---------------|
| atomic        | full            | false           | notes_only          | enabled       |
| sequential    | full            | true            | notes_only          | enabled       |
| reduce        | none            | true            | notes_only          | enabled       |
| script        | full            | false           | notes_only          | disabled      |
| director_evaluator_loop | none  | true            | notes_only          | enabled       |

#### Context Management Dimensions

The system uses a standardized three-dimensional context management model:

1. **inherit_context**: An enumeration with allowed values:
   - **full** – the full parent context is passed unchanged
   - **none** – no parent context is inherited
   - **subset** – only a subset (as determined by task-specific rules) is inherited

2. **accumulate_data**: A boolean controlling whether outputs from prior steps are accumulated:
   - **true** – previous step outputs are accumulated
   - **false** – no accumulation of step outputs

3. **accumulation_format**: When accumulating data, specifies the storage format:
   - **notes_only** – only summary information is preserved
   - **full_output** – complete step outputs are preserved

4. **fresh_context**: Controls whether new context is generated via associative matching:
   - **enabled** – fresh context is generated
   - **disabled** – no fresh context is generated

#### Context Management Override

These dimensions can be explicitly configured through a standardized XML structure:

```xml
<context_management>
    <inherit_context>full|none|subset</inherit_context>
    <accumulate_data>true|false</accumulate_data>
    <accumulation_format>notes_only|full_output</accumulation_format>
    <fresh_context>enabled|disabled</fresh_context>
</context_management>
```

When the `<context_management>` block is present, its settings override the operator defaults. When it's omitted, the operator-specific defaults apply. This hybrid approach provides both consistency and flexibility.

Settings are merged during template loading, with explicit settings taking precedence over defaults. This ensures that task authors can rely on sensible defaults while still having the ability to customize context behavior when needed.

**Note:** Partial results are now preserved when subtasks fail, with the format determined by the `accumulation_format` setting.

### Execution Semantics
- Tasks execute in specified order
- For tasks with multiple inputs:
  - All input tasks execute in parallel
  - Parent task executes after all inputs complete
- Execution fails if:
  - Required task structure is missing/invalid
  - Any task execution fails (with failure context indicating which task)

## Reduce Operator

### Purpose
Process a list of named inputs through repeated application of inner task and reduction operations.

### Structure
```xml
<task type="reduce">
    <description>Reduction operation description</description>
    <initial_value>
        <!-- Initial accumulator value -->
    </initial_value>
    <inputs>
        <input name="dataset1">Value 1</input>
        <input name="dataset2">Value 2</input>
        <input name="dataset3">Value 3</input>
    </inputs>
    <inner_task>
        <description>Processing for each input</description>
        <inputs>
            <input name="current_data">
                <!-- Current input being processed -->
            </input>
            <input name="metadata">
                <!-- Additional input needed for processing -->
                <task>
                    <description>Load metadata for processing</description>
                </task>
            </input>
        </inputs>
    </inner_task>
    <reduction_task>
        <description>Combine current result with accumulator</description>
        <inputs>
            <input name="current_result">
                <!-- Result from inner_task -->
            </input>
            <input name="accumulator">
                <!-- Current accumulated value -->
            </input>
            <input name="original_input">
                <!-- Original input being processed -->
            </input>
        </inputs>
    </reduction_task>
</task>
```

### Execution Semantics
- For each named input:
  1. Execute inner_task with:
     - Current input
     - Any additional specified inputs
     - When both inheritance and accumulation are enabled in a reduce operator, a basic inheritance model is used without merging accumulated outputs. Advanced dual-context tracking is deferred to future iterations.
  2. Execute reduction_task with:
     - Current inner_task result
     - Current accumulator value
     - Original input
  3. Result becomes new accumulator value
- Maintains strict ordering of input processing
- Context changes managed by memory system
- Execution fails if:
  - Required task structure is missing/invalid 
  - Any inner_task execution fails (with failure context indicating which input)
  - Any reduction_task execution fails (with failure context indicating current state)

## Integration Points

### With Memory System
- System maintains execution context via shortTermMemory
- Files and data context available to all tasks
- Context changes managed by memory system, not tasks

### With Task System
- Responsible for generating valid XML
- Manages task decomposition on failure
- Handles task library matching

## Dependencies
- Error types defined in errorspec.md
- Memory system must handle context
- Task system must support XML generation

## Constraints
- Tasks cannot modify context directly
- XML structure must encode all input dependencies
- All inputs must have unique names within their scope
- Inner tasks can specify multiple inputs

## Subtask Spawning Integration

The Task System supports dynamic subtask spawning through a standardized mechanism:

```typescript
interface SubtaskRequest {
  // Required fields
  type: TaskType;                      // Type of subtask to spawn
  description: string;                 // Description of the subtask
  inputs: Record<string, any>;         // Input parameters for the subtask
  
  // Optional fields
  template_hints?: string[];           // Hints for template selection
  context_management?: {               // Override default context settings
    inherit_context?: 'full' | 'none' | 'subset';
    accumulate_data?: boolean;
    accumulation_format?: 'notes_only' | 'full_output';
    fresh_context?: 'enabled' | 'disabled';
  };
  max_depth?: number;                  // Override default max nesting depth
  subtype?: string;                    // Optional subtype for atomic tasks
}
```

### Context Management Defaults

Subtasks have specific default context management settings:

| Setting | Default Value | Description |
|---------|---------------|-------------|
| inherit_context | subset | Inherits only relevant context from parent |
| accumulate_data | false | Does not accumulate previous step outputs |
| accumulation_format | notes_only | Stores only summary information |
| fresh_context | enabled | Generates new context via associative matching |

These defaults can be overridden through explicit configuration in the SubtaskRequest.

### Data Flow

Subtask spawning uses direct parameter passing rather than environment variables:

1. Parent task returns with `status: "CONTINUATION"` and a `subtask_request` in its notes
2. System validates the request and selects an appropriate template
3. Subtask executes with inputs from the request
4. Subtask result is passed back to the parent task when execution resumes

This approach ensures clear data dependencies and improves debug visibility.

### Depth Control

To prevent infinite recursion and resource exhaustion:

1. **Maximum Nesting Depth**: Default limit of 5 levels of nested subtasks
2. **Cycle Detection**: Prevention of tasks spawning identical subtasks
3. **Resource Tracking**: Monitoring of total resource usage across the subtask chain
4. **Timeout Enforcement**: Overall time limits for the complete subtask chain

These mechanisms ensure that subtask spawning remains controlled and resource-efficient.
</file>
<file path="./process.md" project="">
# Architecture Enhancement Process

This document outlines the process for addressing architectural gaps, ambiguities, or needed enhancements in the system. It captures the methodology we used for the Output Standardization feature and can serve as a template for future architectural work.

## 1. Gap Identification

**Input**: Prioritized list of architectural issues or enhancements
**Output**: Clear understanding of the gap and its impact

We began with Output Standardization identified as a high-priority architectural gap from our prioritized list. The gap was specific: the system lacked a standardized approach for tasks to return structured data (like lists or objects), creating issues with variable binding, type ambiguity, and task composition.

## 2. Initial Approach Exploration

**Input**: Architectural gap description
**Output**: First draft of potential solution approaches

In this phase, we created an initial approach document outlining potential solutions to the Output Standardization issue. This included:
- Key design decisions that needed to be made
- Potential XML syntax for structured outputs
- Basic requirements for JSON parsing and validation
- Integration points with existing architecture

## 3. Clarification of Design Decisions

**Input**: Initial approach document
**Output**: Specific questions requiring architectural decisions

Not all design decisions have obvious answers. For Output Standardization, several key questions emerged:
- Should all outputs use JSON or only those needing structured data?
- How should JSON outputs relate to the existing TaskResult structure?
- Should outputs have formal schema declarations?
- How should backward compatibility be handled?
- What error handling mechanism is appropriate?

## 4. Stakeholder Input on Design Decisions

**Input**: Design decision questions
**Output**: Clear direction on architectural choices

For Output Standardization, specific decisions were made:
- JSON would be optional, focused on tasks needing structured data
- Structured output would live in the existing content field
- Schema declaration would be kept lightweight to avoid over-complication
- Backward compatibility was required
- Error handling needed further exploration

## 5. Comprehensive Approach Documentation

**Input**: Design decisions and architectural requirements
**Output**: Detailed approach document

We created a detailed document outlining:
- Core approach and rationale
- XML template syntax extensions
- JSON detection and parsing algorithm
- Variable binding mechanism
- Integration with other features (e.g., function-based templates)
- Error handling strategy
- Implementation priorities
- Example usage patterns

This document served as a proposal rather than a final decision.

## 6. Approach Review and Documentation Strategy

**Input**: Detailed approach document
**Output**: Review feedback and documentation plan

The approach document underwent review focusing on:
- Alignment with architectural principles
- Technical feasibility
- Potential issues or gaps

For Output Standardization, we determined a two-phase documentation approach:
1. Create an Architecture Decision Record (ADR) to capture the decision
2. Later propagate changes to component-specific documentation

## 7. ADR Creation

**Input**: Reviewed approach and documentation strategy
**Output**: Draft Architecture Decision Record

We drafted a formal ADR following the project's standard ADR template:
- Context (problem statement)
- Decision (solution approach)
- Consequences (positive and negative impacts)
- Implementation guidance
- Related decisions
- Affected documentation

The ADR formalized the design decisions and provided technical implementation details.

## 8. ADR Review

**Input**: Draft ADR
**Output**: Review comments focusing on clarity and potential issues

The ADR underwent critical review focusing on:
- Clarity and freedom from ambiguity
- Potential over-engineering
- Premature design decisions
- Integration with existing architecture
- Implementation feasibility

For Output Standardization, the review identified areas of over-engineering in the schema validation system and unnecessary complexity in error handling.

## 9. Simplification Analysis

**Input**: ADR review comments
**Output**: Specific simplification recommendations

Based on the review, we identified specific areas for simplification:
- Schema validation could be reduced to basic type checking
- TaskResult extension could be minimized
- Error handling could be simplified
- XML syntax could be streamlined

This followed the principle: "Start simple, add complexity only when needed."

## 10. ADR Refinement

**Input**: Simplification recommendations
**Output**: Revised, simplified ADR

We revised the ADR to focus on core functionality:
- Reduced schema complexity to basic type validation
- Simplified TaskResult interface changes
- Streamlined error handling
- Maintained a clear path for future extensions
- Added an explicit "Future Extensions" section highlighting deferred complexity

The refined ADR maintained the core value proposition while reducing implementation complexity.

## 11. Final Review and Approval

**Input**: Revised ADR
**Output**: Approved architectural direction

The final step is reviewing the revised ADR and obtaining approval for the architectural direction. This includes:
- Confirming alignment with architectural principles
- Validating that simplifications maintain core functionality
- Ensuring integration points are well defined
- Approving implementation priority

## 12. Documentation Updates

**Input**: Approved ADR
**Output**: Updated component documentation

After ADR approval, changes must be propagated to affected documentation:
- Update contract documents with new interfaces
- Update component documentation with implementation guidance
- Update schema definitions with new elements
- Add examples demonstrating the new capability

## Key Principles

Throughout the Output Standardization process, several principles guided our work:

1. **Start Simple**: Begin with minimal implementations; add complexity only when needed.

2. **Question Complexity**: Regularly challenge whether proposed solutions are over-engineered.

3. **Maintain Compatibility**: Ensure changes don't break existing functionality.

4. **Look for Patterns**: Align new features with existing architectural patterns.

5. **Document Decisions**: Capture the "why" behind decisions, not just the "what."

6. **Defer When Uncertain**: If a feature isn't clearly needed now, defer it to future iterations.

7. **Review Critically**: Apply constructive criticism to your own work and encourage others to do the same.

## Process Variations

This process can be adapted based on the nature of the architectural enhancement:

- For smaller changes, some steps can be combined or streamlined
- For more complex changes, additional review cycles may be needed
- For urgent issues, temporary solutions might be documented separately from long-term architectural direction

## Conclusion

The iterative process outlined above allowed us to address the Output Standardization gap in a methodical way that balanced immediate needs with long-term architectural health. By starting with an exploration of approaches, getting stakeholder input, creating detailed documentation, conducting critical reviews, and simplifying where possible, we arrived at a clean solution that adds significant value without unnecessary complexity.
</file>
<file path="./plans/atomic_task_subtypes.md" project="">
1. Type Hierarchy Recommendation

mermaid
Copy
graph TD
    Task[Base Task Type]
    Task --> Atomic
    Task --> Sequential
    Task --> Reduce
    Atomic --> Director
    Atomic --> Evaluator
    classDef atomic fill:#cff,stroke:#099;
    class Director,Evaluator atomic
2. XML Implementation Strategy

xml
Copy
<!-- As atomic subtypes -->
<task type="atomic" subtype="director">
    <continuation_policy>latest-only</continuation_policy>
    <output_slot>last_eval_input</output_slot>
</task>

<task type="atomic" subtype="evaluator">
    <input_source>last_eval_input</input_source>
    <validation_rules>strict</validation_rules>
</task>

<!-- Example using template substitution for input binding -->
<task type="atomic">
    <description>Process {{input_data}} with parameters from {{config}}</description>
    <inputs>
        <input name="data" from="input_data"/>
        <input name="settings" from="config"/>
    </inputs>
</task>
Run HTML
3. Advantages of Atomic Subtyping

Aspect	Benefit
Execution Flow	Inherits standard atomic task lifecycle (init → execute → cleanup)
Error Handling	Uses existing atomic error recovery patterns
Resource Tracking	Leverages atomic task's turn counting & context management
Template Matching	Works with current associative matching system
4. Context Management Additions

typescript
Copy
interface AtomicTaskEnv {
    // Shared atomic properties
    turnsUsed: number;
    contextWindowUsage: number;
    
    // Director-Evaluator specific
    lastEvaluation?: {
        output: string;
        timestamp: Date;
        success: boolean;
    };
}
5. Implementation Requirements

a. Subtype Registration

typescript
Copy
// In task-system initialization
registerAtomicSubtype('director', {
    continuationHandler: handleDirectorContinuation,
    maxTurns: 10 // Special limit for directors
});

registerAtomicSubtype('evaluator', {
    outputValidation: strictJSONValidator,
    autoClearContext: true
});
b. Execution Flow Modifications

Copy
[Director Task]
1. Execute as normal atomic task
2. On CONTINUATION:
   a. Store output in lastEvaluation slot
   b. Clear other context variables
   c. Pass control to Evaluator subtype

[Evaluator Task]
1. Receive lastEvaluation data as input
2. Execute validation/processing
3. Return results to parent environment
4. Auto-clear lastEvaluation slot
6. XML Schema Changes (operators.md update)

diff
Copy
<xs:element name="task">
  <xs:complexType>
    <xs:attribute name="type" use="required">
      <xs:simpleType>
        <xs:restriction base="xs:string">
          <xs:enumeration value="atomic"/>
          <xs:enumeration value="sequential"/>
          <xs:enumeration value="reduce"/>
+         <xs:enumeration value="script"/>
        </xs:restriction>
      </xs:simpleType>
    </xs:attribute>
+   <xs:attribute name="subtype">
+     <xs:simpleType>
+       <xs:restriction base="xs:string">
+         <xs:enumeration value="director"/>
+         <xs:enumeration value="evaluator"/>
+       </xs:restriction>
+     </xs:simpleType>
+   </xs:attribute>
  </xs:complexType>
</xs:element>
7. Transition Plan

Phase 1: Add subtype support to atomic tasks

Phase 2: Migrate existing director/evaluator templates

Phase 3: Deprecate top-level director/evaluator types

Phase 4: Update documentation (errorspec.md, operators.md)

## Implementation Implications

The atomic subtyping approach provides precise technical advantages:

- **Execution Model Integration**: Subtypes inherit the established turn tracking, resource management, and cleanup protocols already defined for atomic tasks
- **Architecture Simplification**: Reduces duplicated code by keeping atomic behavior in a single class hierarchy rather than implementing separate task types
- **Integration Cost Reduction**: Requires only minimal changes to the XML schema (adding the `subtype` attribute) rather than introducing new top-level element types
- **Template System Compatibility**: Existing template matching logic continues to work without modification

This approach balances specialization needs with architectural consistency, while maintaining the simplified context model from previous planning.
</file>
<file path="./plans/general_improvements.md" project="">
# Architecture Analysis and Recommendations

## Key Inconsistencies and Gaps

</file>
<file path="./inconsistencies.md" project="">
# Inconsistencies Requiring Subjective Choices

Below are areas where the documents highlight open design questions or competing approaches. Each needs a decision to remove ambiguity.


</file>
<file path="./docstructure.md" project="">
# Documentation Structure

## Directory Layout
```
docs/
├── system/                         # System-level documentation
│   ├── README.md                   # System overview & development sequence
│   ├── docs-guide.md               # Documentation standards, map & navigation
│   ├── architecture/               # Core architecture
│   │   ├── overview.md            # High-level design
│   │   ├── decisions/             # Architecture Decision Records (ADRs)
│   │   └── patterns/              # Core patterns & principles
│   ├── protocols/                  # System-wide protocols
│   │   └── resources.md            # Protocol-level resource definitions
│   └── contracts/                  # System-wide contracts
│       ├── interfaces.md          # External interfaces
│       └── resources.md           # Resource management
│
└── components/                     # Component documentation
    └── [component]/               # Per component (can be nested)
        ├── README.md              # Component overview
        ├── api/                   # Public API documentation
        │   └── interfaces.md      # Public interface definitions
        ├── spec/                  # Formal specifications
        │   ├── requirements.md    # Component requirements
        │   ├── interfaces.md      # Internal interface definitions
        │   ├── types.md          # Type definitions
        │   └── behaviors.md       # Expected behaviors
        └── impl/                  # Implementation details
            ├── design.md         # Design decisions
            ├── protocols.md      # Protocol implementations
            └── examples.md       # Implementation examples
```

## Common Types
[Rest of common types section remains unchanged]

## Cross-Reference Updates
Ensure that all components referencing the Director-Evaluator pattern mention both the dynamic and static variants. In particular, update references to [Pattern:DirectorEvaluator:1.1] to include the static variant with script execution support.

## Document Standards

### System Level Documents

#### README.md
[README.md template remains unchanged]

#### docs-guide.md
```markdown
# Documentation Guide

## Documentation Map
Required:
- Core system specifications
- Component contracts & integration
- Core patterns & protocols
- Key data structures
- Documentation status
- Cross-reference guide

## Standards
Required:
- Writing style guidelines
- Document templates
- Cross-reference syntax
- Version control practices

## Documentation Principles
Required:
1. Single Responsibility
   - Each document covers one concern
   - Clear boundaries between concerns
   - Explicit dependencies
2. Template Consistency
   - All new task templates must use the unified template substitution mechanism.
   - Evaluator's lexically scoped variables are referenced using the `{{variable_name}}` syntax.
   - Input bindings can be explicitly declared using the optional `from` attribute on `<input>` elements.

2. Contract Completeness
   - All requirements stated
   - All guarantees explicit
   - All resources documented

3. Resource Clarity
   - Ownership explicit
   - Lifecycle documented
   - Cleanup requirements specified

## Writing Style
Required:
1. Active voice
2. One sentence per line
3. Explicit section numbering
4. Consistent terminology

## Documentation Map
Required:
- Core system specifications
- Component contracts & integration
- Core patterns & protocols
- Key data structures
- Documentation status

## Version Management
Required:
1. Version Format: MAJOR.MINOR.PATCH
2. Update Rules:
   - MAJOR: Breaking changes
   - MINOR: New features, backward compatible
   - PATCH: Bug fixes, backward compatible

## Navigation
Required:
1. Documentation Map Usage
   - Primary navigation aid
   - Definitive specification locations
   - Cross-reference resolution
   - Documentation status tracking

2. Map Maintenance
   - Regular updates with changes
   - Validation of references
   - Status accuracy verification
   - Gap identification

Note: The Documentation Map in this guide serves as the primary navigation aid
for finding specifications and understanding document relationships.

## Templates
[See individual document templates in this guide]
```

[Rest of document remains unchanged]
</file>
<file path="./progress.md" project="">
# Progress Report for ADRs

Below is the completion level (0-100%) for each ADR as reflected in the current system documentation:

- decisions/completed/010-evaluator-director.md: 100%
- decisions/removed.md: 100%
- decisions/14-operator-ctx-config.md: 100%
- decisions/12-function-based-templates.md: 95%
- decisions/13-json-output.md: 100%
- decisions/9-partial-results.md: 100%
- decisions/8-errors.md: 90%
- decisions/needs_update/003-memory-context-update.md: 100%
- decisions/needs_update/001-memory-system.md: 80%
- decisions/needs_update/004-sequential-context-management.md: 85%
- decisions/11-subtask-spawning.md: 100%
</file>
<file path="./system/contracts/resources.md" project="">
# Resource Management Contracts [Contract:Resources:1.0]

## 1. Resource Types

### 1.1 Turn Counter
**Implementation**: [Component:Handler:1.0]  
**Interface**: [Interface:Handler:ResourceMonitoring:1.0]

The turn counter implements:
- Atomic per-session counter incremented with each LLM interaction
- Non-shared counter state isolated to individual Handler instances
- Hard limits enforced at the Handler level (defaults to system-wide value)
- Warning threshold at 80% of maximum turn count
- Clear metrics reporting through the ResourceMetrics interface
- Automatic cleanup on session termination

### 1.2 Context Window
**Implementation**: [Component:Handler:1.0]  
**Interface**: [Interface:Handler:ContextManagement:1.0]

The context window implementation provides:
- Token-based size calculation for all content
- Fraction-based limits (percentage of model's full context)
- Peak usage tracking for performance optimization
- Warning thresholds at 80% utilization
- No automatic content optimization (delegated to task implementation)
- Clean termination when limits are reached

### 1.3 Memory Resources
**Implementation**: [Component:Memory:3.0]

Memory resources are managed with:
- Isolated memory contexts per session
- Explicit garbage collection triggers
- Size-based eviction policies for context management
- Relevance-based retention for associative memory
- Explicit versioning for all stored artifacts

## 2. Resource Management Protocols

Resource management follows these concrete protocols:

1. **Allocation**: Resources are allocated at Handler initialization with explicit limits
2. **Tracking**: Usage is tracked with each operation and reported via ResourceMetrics
3. **Warning**: Non-fatal warnings are issued at 80% of resource limits
4. **Termination**: Clean termination occurs at hard limits with detailed error information
5. **Release**: Resources are explicitly released when sessions end

## 3. Contract Validation

Implementation must satisfy these technical requirements:

1. **Isolation**: No resource sharing between Handler instances
2. **Atomicity**: Resource operations must be atomic to prevent race conditions
3. **Reporting**: All resource usage must be reported through standard interfaces
4. **Recovery**: Resource exhaustion must trigger clean termination with recovery options
5. **Efficiency**: Resource tracking overhead must be <1% of total operation time
</file>
<file path="./system/contracts/interfaces.md" project="">
# System Interface Contracts

## 1. Component Integration Contracts

### 1.1 Compiler Integration [Contract:Integration:CompilerTask:1.0]
Defined in [Component:Compiler:1.0]
[TBD: Complete contract specification]

### 1.2 Evaluator Integration [Contract:Integration:EvaluatorTask:1.0]
See [Component:Evaluator:1.0] for complete interface.
[TBD: Complete contract specification]

### 1.3 Task System Integration
See [Component:TaskSystem:1.0] in components/task-system/README.md

#### Interfaces
- Task Execution: [Interface:TaskSystem:1.0] 
- Template Management: [Interface:TaskSystem:Templates:1.0]
- XML Processing: [Contract:Tasks:TemplateSchema:1.0]

#### Component Responsibilities
- Evaluator: Responsible for all template variable substitution (resolving {{variable_name}} placeholders)
- Handler: Works with fully resolved content only, no template substitution

### Template Substitution Responsibility
- The Evaluator is exclusively responsible for all template variable substitution
- This includes resolving all {{variable_name}} placeholders and input bindings
- Handlers receive fully resolved content with no remaining template variables
- This separation ensures clean component boundaries and single responsibility

#### Resource Contracts
See [Contract:Resources:1.0]

### 1.4 Memory System Integration [Contract:Integration:TaskMemory:3.0]
See [Component:Memory:3.0]

#### Interfaces
  - Metadata Management: [Interface:Memory:3.0]
    - Task System uses metadata for associative matching; see Appendix A for validation criteria.
  - Index Management: [Interface:Memory:3.0]
    - Global index serves as the bootstrap for matching; updates occur in bulk.

#### Responsibilities
Memory System:
 - Maintains global file metadata index
 - Provides bulk index updates
 - Supplies metadata for associative matching (refer to Appendix A for constraints)
 - NEVER performs file I/O operations (reading, writing, deletion)
 - Does NOT store or process file contents
 - Follows read-only context model (no updateContext capability)

Task System:
 - Uses context for task execution
 - Receives file references via associative matching
 - Delegates file access to Handler tools
 - Must not attempt to update context directly (removed in 3.0)

Handler:
 - Performs ALL file I/O operations
 - For Anthropic models: Configures computer use tools (optional)
 - For other models: Uses appropriate file access mechanisms
 - Manages all direct interaction with file system
 - Works with fully resolved content only, no template substitution

Evaluator:
 - Responsible for all template variable substitution (resolving {{variable_name}} placeholders)
 - Ensures all templates are fully resolved before passing to Handler
 - Applies different resolution rules for function vs. standard templates
 - Detects and handles variable resolution errors

#### Integration Points
 - Context flow from associative matching to task execution
 - File metadata index is used exclusively for matching; file access is handled by Handler tools

#### Interfaces
  - Metadata Management: [Interface:Memory:3.0]
    - Task System uses metadata for associative matching; see Appendix A for validation criteria.
  - Index Management: [Interface:Memory:3.0]
    - Global index serves as the bootstrap for matching; updates occur in bulk.

### 1.5 Delegation Boundaries

#### Handler Responsibilities
- Present unified tool interface to the LLM
- Execute direct tools synchronously
- Transform subtask tool calls into CONTINUATION requests
- Manage tool-specific resources
- Configure model-specific tools (e.g., Anthropic computer use)
- Direct execution without continuation mechanism
- Track resource usage for tool operations

#### Memory System Responsibilities
- Provide context for subtask execution
- Support context inheritance for LLM-to-LLM interaction
- Not involved in tool call execution
- Maintain metadata for associative matching

#### Task System Responsibilities
- Coordinate tool implementations (direct and subtask)
- Process CONTINUATION requests from subtask tools
- Manage execution flow between direct and subtask operations
- Handle template selection for subtask tools

#### Responsibilities
Memory System:
 - Maintains global file metadata index
 - Provides bulk index updates
 - Supplies metadata for associative matching (refer to Appendix A for constraints)
 - NEVER performs file I/O operations (reading, writing, deletion)
 - Does NOT store or process file contents
 - Follows read-only context model (no updateContext capability)

Task System:
 - Uses context for task execution
 - Receives file references via associative matching
 - Delegates file access to Handler tools
 - Must not attempt to update context directly (removed in 3.0)

Handler:
 - Performs ALL file I/O operations
 - For Anthropic models: Configures computer use tools (optional)
 - For other models: Uses appropriate file access mechanisms
 - Manages all direct interaction with file system

#### Integration Points
 - Context flow from associative matching to task execution
 - File metadata index is used exclusively for matching; file access is handled by Handler tools

## 2. Cross-Component Requirements

### 2.1 State Management
See [Pattern:Error:1.0] for error state handling.

Context Management:
- Task context maintained by Memory System
- Context operations provide immediate consistency
- No persistence guarantees for context data
- Context scope limited to current task execution

### 2.2 Error Propagation
Error handling defined in [Pattern:Error:1.0]

### 2.3 Resource Tracking
Resource contracts defined in [Contract:Resources:1.0]

Memory-Specific Resource Considerations:
- Context size limits defined by Handler
- Global index accessed in full only
- No partial index updates or queries
- File content handling delegated to Handler tools

## 3. Contract Validation 

### 3.1 Validation Requirements
- Context Management
  - Context updates must be atomic
  - Context retrieval must be consistent
  - No data persistence required
- Index Management
  - Index updates must be atomic
  - Index must persist across sessions
  - All file paths must be absolute
  - All metadata must be strings

### 3.2 Success Criteria
Context Management:
- Context updates visible to immediate subsequent reads
- No context data persists between sessions
- Context size within Handler limits

Index Management:
- Index survives system restarts
- Bulk updates atomic and consistent
- All paths resolvable by Handler tools

### 3.3 Verification Methods
- Unit tests for context operations
- Integration tests for index persistence
- Validation of file paths with Handler tools
- Context size limit compliance checks
</file>
<file path="./system/contracts/protocols.md" project="">
# System Protocols

## Task Template Schema [Contract:Tasks:TemplateSchema:1.0]

**Note:** This document is the authoritative specification for the XML schema used in task template definitions. All field definitions, allowed enumerations (such as for `<inherit_context>` and `<accumulation_format>`), and validation rules are defined here. For complete validation guidelines, please see Appendix A in [Contract:Resources:1.0].

The task template schema defines the structure for XML task template files and maps to the TaskTemplate interface.

### XML Schema Definition

```xml
<?xml version="1.0" encoding="UTF-8"?>
<xs:schema xmlns:xs="http://www.w3.org/2001/XMLSchema">
  <xs:complexType name="EvaluationResult">
    <xs:sequence>
      <xs:element name="success" type="xs:boolean"/>
      <xs:element name="feedback" type="xs:string" minOccurs="0"/>
    </xs:sequence>
  </xs:complexType>

  <xs:element name="task">
    <xs:complexType>
      <xs:sequence>
        <xs:element name="description" type="xs:string"/>
        <xs:element name="provider" type="xs:string" minOccurs="0"/>
        <xs:element name="output_slot" type="xs:string" minOccurs="0"/>
        <xs:element name="input_source" type="xs:string" minOccurs="0"/>
        <xs:element name="output_format" minOccurs="0">
          <xs:complexType>
            <xs:attribute name="type" use="required">
              <xs:simpleType>
                <xs:restriction base="xs:string">
                  <xs:enumeration value="json"/>
                  <xs:enumeration value="text"/>
                </xs:restriction>
              </xs:simpleType>
            </xs:attribute>
            <xs:attribute name="schema" type="xs:string" use="optional"/>
          </xs:complexType>
        </xs:element>
        <xs:element name="context_management">
          <xs:complexType>
            <xs:sequence>
              <xs:element name="inherit_context">
                <xs:simpleType>
                  <xs:restriction base="xs:string">
                    <xs:enumeration value="full"/>
                    <xs:enumeration value="none"/>
                    <xs:enumeration value="subset"/>
                  </xs:restriction>
                </xs:simpleType>
              </xs:element>
              <xs:element name="accumulate_data" type="xs:boolean"/>
              <xs:element name="accumulation_format">
                <xs:simpleType>
                  <xs:restriction base="xs:string">
                    <!-- When 'full' is specified, complete notes are preserved -->
                    <xs:enumeration value="full"/>
                    <!-- When 'minimal' is specified, only essential metadata is preserved -->
                    <xs:enumeration value="minimal"/>
                  </xs:restriction>
                </xs:simpleType>
              </xs:element>
              <xs:element name="fresh_context">
                <xs:simpleType>
                  <xs:restriction base="xs:string">
                    <xs:enumeration value="enabled"/>
                    <xs:enumeration value="disabled"/>
                  </xs:restriction>
                </xs:simpleType>
              </xs:element>
            </xs:sequence>
          </xs:complexType>
        </xs:element>
        <xs:element name="file_paths" minOccurs="0">
          <xs:complexType>
            <xs:sequence>
              <xs:element name="path" type="xs:string" maxOccurs="unbounded"/>
            </xs:sequence>
          </xs:complexType>
        </xs:element>
        <xs:element name="steps">
          <xs:complexType>
            <xs:sequence>
              <xs:element name="task" maxOccurs="unbounded">
                <xs:complexType>
                  <xs:sequence>
                    <xs:element name="description" type="xs:string"/>
                    <xs:element name="inputs" minOccurs="0">
                      <xs:complexType>
                        <xs:sequence>
                          <xs:element name="input" maxOccurs="unbounded">
                            <xs:complexType>
                              <xs:sequence>
                                <xs:element name="task">
                                  <xs:complexType>
                                    <xs:sequence>
                                      <xs:element name="description" type="xs:string"/>
                                    </xs:sequence>
                                  </xs:complexType>
                                </xs:element>
                              </xs:sequence>
                              <xs:attribute name="name" type="xs:string" use="required"/>
                            </xs:complexType>
                          </xs:element>
                        </xs:sequence>
                      </xs:complexType>
                    </xs:element>
                  </xs:sequence>
                </xs:complexType>
              </xs:element>
            </xs:sequence>
          </xs:complexType>
        </xs:element>
        <xs:element name="inputs" minOccurs="0">             <!-- Maps to inputs -->
          <xs:complexType>
            <xs:sequence>
              <xs:element name="input" maxOccurs="unbounded">
                <xs:complexType>
                  <xs:simpleContent>
                    <xs:extension base="xs:string">
                      <xs:attribute name="name" type="xs:string" use="required"/>
                      <xs:attribute name="from" type="xs:string" use="optional"/>
                    </xs:extension>
                  </xs:simpleContent>
                </xs:complexType>
              </xs:element>
            </xs:sequence>
          </xs:complexType>
        </xs:element>
        <xs:element name="manual_xml" type="xs:boolean" minOccurs="0" default="false"/>      <!-- Maps to isManualXML -->
        <xs:element name="disable_reparsing" type="xs:boolean" minOccurs="0" default="false"/> <!-- Maps to disableReparsing -->
      </xs:sequence>
      <xs:attribute name="ref" type="xs:string" use="optional"/>
      <xs:attribute name="subtype" type="xs:string" use="optional"/>
      <xs:attribute name="type" use="required">
        <xs:simpleType>
          <xs:restriction base="xs:string">
            <xs:enumeration value="atomic"/>
            <xs:enumeration value="sequential"/>
            <xs:enumeration value="reduce"/>
            <xs:enumeration value="script"/>
            <xs:enumeration value="director_evaluator_loop"/>
          </xs:restriction>
        </xs:simpleType>
      </xs:attribute>
    </xs:complexType>
  </xs:element>
  
  <xs:element name="template">
    <xs:complexType>
      <xs:sequence>
        <xs:element name="name" type="xs:string"/>
        <xs:element name="params" type="xs:string"/>
        <xs:element name="returns" type="xs:string" minOccurs="0"/>
        <xs:element name="task" type="TaskType"/>
      </xs:sequence>
    </xs:complexType>
  </xs:element>

  <xs:element name="call">
    <xs:complexType>
      <xs:sequence>
        <xs:element name="template" type="xs:string"/>
        <xs:element name="arg" type="xs:string" maxOccurs="unbounded"/>
      </xs:sequence>
    </xs:complexType>
  </xs:element>
  
  <xs:element name="cond">
    <xs:complexType>
      <xs:sequence>
        <xs:element name="case" maxOccurs="unbounded">
          <xs:complexType>
            <xs:attribute name="test" type="xs:string" use="required"/>
            <xs:sequence>
              <xs:element name="task" minOccurs="1" maxOccurs="1">
                <!-- Task definition inside case -->
              </xs:element>
            </xs:sequence>
          </xs:complexType>
        </xs:element>
      </xs:sequence>
    </xs:complexType>
  </xs:element>
<!-- Director-Evaluator Loop Task Definition -->
<xs:element name="director_evaluator_loop">
  <xs:complexType>
    <xs:sequence>
      <xs:element name="description" type="xs:string"/>
      <xs:element name="max_iterations" type="xs:integer" minOccurs="0"/>
      <xs:element ref="context_management"/>
      <xs:element name="director" type="TaskType"/>
      <xs:element name="evaluator" type="TaskType"/>
      <xs:element name="script_execution" minOccurs="0">
        <xs:complexType>
          <xs:sequence>
            <xs:element name="command" type="xs:string"/>
            <xs:element name="timeout" type="xs:integer" minOccurs="0"/>
            <xs:element name="inputs" type="InputsType"/>
          </xs:sequence>
        </xs:complexType>
      </xs:element>
      <xs:element name="termination_condition" minOccurs="0">
        <xs:complexType>
          <xs:sequence>
            <xs:element name="condition" type="xs:string"/>
          </xs:sequence>
        </xs:complexType>
      </xs:element>
    </xs:sequence>
  </xs:complexType>
</xs:element>
</xs:schema>
```

### Script Execution Support

The XML task template schema now supports defining tasks for script execution within sequential tasks. These tasks enable:
 - Command Specification: Defining external commands (e.g. bash scripts) to be executed.
 - Input/Output Contracts: Passing the director's output as input to the script task, and capturing the script's output for subsequent evaluation.
Script execution errors (e.g. non-zero exit codes) are treated as generic TASK_FAILURE conditions. The evaluator captures the script's stdout and stderr in a designated notes field for downstream decision-making.

Example:
```xml
<task type="sequential">
  <description>Static Director-Evaluator Pipeline</description>
  <context_management>
    <inherit_context>none</inherit_context>
    <accumulate_data>true</accumulate_data>
    <accumulation_format>notes_only</accumulation_format>
  </context_management>
  <steps>
    <task>
      <description>Generate Initial Output</description>
    </task>
    <task type="script">
      <description>Run Target Script</description>
      <inputs>
        <input name="director_output" from="last_director_output"/>
      </inputs>
    </task>
    <task>
      <description>Evaluate Script Output</description>
      <inputs>
        <input name="script_output">
          <task>
            <description>Process output from target script</description>
          </task>
        </input>
      </inputs>
    </task>
  </steps>
</task>
```

### Output Format Specification

Tasks can specify structured output format:

```xml
<task>
  <description>List files in directory</description>
  <output_format type="json" schema="string[]" />
</task>
```

The `schema` attribute provides basic type information:
- "object" - JSON object
- "array" or "[]" - JSON array
- "string[]" - Array of strings
- "number" - Numeric value
- "boolean" - Boolean value

Output validation ensures the result matches the specified type.

## Subtask Spawning Protocol [Protocol:SubtaskSpawning:1.0]

The subtask spawning protocol defines how tasks can dynamically create and execute subtasks.

## LLM Interaction Protocol [Protocol:LLMInteraction:1.0]

The Handler-LLM interaction follows a standardized protocol using the `HandlerPayload` structure:

```typescript
interface HandlerPayload {
  systemPrompt: string;
  messages: Array<{
    role: "user" | "assistant" | "system";
    content: string;
    timestamp?: Date;
  }>;
  context?: string;        // Context from Memory System
  tools?: ToolDefinition[]; // Available tools
  metadata?: {
    model: string;
    temperature?: number;
    maxTokens?: number;
    resourceUsage: ResourceMetrics;
  };
}
```

### Protocol Flow

1. The Task System creates a Handler instance with configuration
2. The Handler creates a HandlerSession to manage conversation state
3. The Evaluator ensures all placeholders are substituted
4. The Handler constructs a HandlerPayload via session.constructPayload()
5. Provider-specific adapters transform the payload to appropriate formats
6. LLM response is processed via handler.processLLMResponse()
7. Tool calls (including user input requests) are handled
8. Session state is updated with new messages
9. Resource usage is tracked and limits enforced

This standardized protocol ensures consistent handling of LLM interactions across different providers while maintaining proper conversation tracking and resource management.

### Request Structure

```typescript
interface SubtaskRequest {
  // Required fields
  type: TaskType;                      // Type of subtask to spawn
  description: string;                 // Description of the subtask
  inputs: Record<string, any>;         // Input parameters for the subtask
  
  // Optional fields
  template_hints?: string[];           // Hints for template selection
  context_management?: {               // Override default context settings
    inherit_context?: 'full' | 'none' | 'subset';
    accumulate_data?: boolean;
    accumulation_format?: 'notes_only' | 'full_output';
    fresh_context?: 'enabled' | 'disabled';
  };
  max_depth?: number;                  // Override default max nesting depth
  subtype?: string;                    // Optional subtype for atomic tasks
  
  /**
   * Optional list of specific file paths to include in subtask context.
   * Takes precedence over associative matching when provided.
   * Paths can be absolute or relative to repo root.
   * Invalid paths will generate warnings but execution will continue.
   */
  file_paths?: string[];
}
```

### Execution Flow

1. **Request Generation**: A parent task returns a result with `status: "CONTINUATION"` and includes a `subtask_request` in its notes.

2. **Request Validation**: The system validates the subtask request structure, ensuring all required fields are present and correctly formatted.

3. **Template Selection**: The system selects an appropriate template based on:
   - The `type` and optional `subtype` fields
   - The `description` field for associative matching
   - Any provided `template_hints`

4. **Depth Control**: The system checks:
   - Current nesting depth against maximum allowed depth (default: 5)
   - Cycle detection to prevent recursive spawning of identical tasks
   - Resource usage across the entire subtask chain

5. **Subtask Execution**: The system executes the subtask with:
   - Direct parameter passing from the `inputs` field
   - Context management according to defaults or overrides
   - Resource tracking linked to the parent task

6. **Result Handling**: The subtask result is passed back to the parent task when execution resumes, with the parent receiving the complete TaskResult structure.

### Error Handling

If a subtask fails, a standardized error structure is generated:

```typescript
{
  type: 'TASK_FAILURE',
  reason: 'subtask_failure',
  message: 'Subtask execution failed',
  details: {
    subtaskRequest: SubtaskRequest;    // The original request
    subtaskError: TaskError;           // The error from the subtask
    nestingDepth: number;              // Current nesting depth
    partialOutput?: string;            // Any partial output if available
  }
}
```

This structure preserves the complete error context, allowing for potential recovery strategies.

### Depth Control Mechanisms

To prevent infinite recursion and resource exhaustion:

1. **Maximum Nesting Depth**: Default limit of 5 levels of nested subtasks
2. **Cycle Detection**: Prevention of tasks spawning identical subtasks
3. **Resource Tracking**: Monitoring of total resource usage across the subtask chain
4. **Timeout Enforcement**: Overall time limits for the complete subtask chain

These mechanisms ensure that subtask spawning remains controlled and resource-efficient.

### Function-Based Templates

The XML schema now supports function-based templates with explicit parameter declarations:

```xml
<template name="analyze_data" params="dataset,config">
  <task>
    <description>Analyze {{dataset}} using {{config}}</description>
  </task>
</template>
```

And function calls with positional arguments:

```xml
<call template="analyze_data">
  <arg>weather_data</arg>
  <arg>standard_config</arg>
</call>
```

This enforces strict scope boundaries - templates can only access explicitly passed parameters.

#### Parameter Resolution

- Parameter names are declared in the comma-separated `params` attribute
- Inside templates, `{{...}}` placeholders only reference declared parameters
- Arguments are evaluated in the caller's environment before being passed to the template
- String arguments can be either variable references or literal values

#### Return Types

Templates can optionally specify a return type using the `returns` attribute:

```xml
<template name="get_file_info" params="filepath" returns="object">
  <task>
    <description>Get metadata for {{filepath}}</description>
    <output_format type="json" schema="object" />
  </task>
</template>
```

This aids in type validation and enables better composition between templates.

### Output Format Specification

Tasks can specify structured output format:

```xml
<task>
  <description>List files in directory</description>
  <output_format type="json" schema="string[]" />
</task>
```

The `schema` attribute provides basic type information:
- "object" - JSON object
- "array" or "[]" - JSON array
- "string[]" - Array of strings
- "number" - Numeric value
- "boolean" - Boolean value

Output validation ensures the result matches the specified type.

### Context Management Configuration

The `<context_management>` element controls how context is managed during task execution:

```xml
<context_management>
    <inherit_context>full|none|subset</inherit_context>
    <accumulate_data>true|false</accumulate_data>
    <accumulation_format>notes_only|full_output</accumulation_format>
    <fresh_context>enabled|disabled</fresh_context>
</context_management>
```

### Context Management Constraints

The following constraints apply to context management settings:

1. **Mutual Exclusivity**: `fresh_context="enabled"` cannot be combined with `inherit_context="full"` or `inherit_context="subset"`
   - If `inherit_context` is "full" or "subset", `fresh_context` must be "disabled"
   - If `fresh_context` is "enabled", `inherit_context` must be "none"

2. **Validation Errors**: Templates violating these constraints will fail validation with clear error messages

Each operator type has specific default settings that apply when the `<context_management>` element is omitted:

| Operator Type | inherit_context | accumulate_data | accumulation_format | fresh_context |
|---------------|-----------------|-----------------|---------------------|---------------|
| atomic        | full            | false           | minimal             | enabled       |
| sequential    | full            | true            | minimal             | enabled       |
| reduce        | none            | true            | minimal             | enabled       |
| script        | full            | false           | minimal             | disabled      |
| director_evaluator_loop | none  | true            | minimal             | enabled       |

When the `<context_management>` element is present, its settings override the operator defaults. Settings are merged during template loading, with explicit settings taking precedence over defaults.

### Field Definitions

- The optional `ref` attribute is used to reference a pre-registered task in the TaskLibrary.
- The optional `subtype` attribute refines the task type (for example, indicating "director", "evaluator", etc.)
- The `output_format` element specifies structured output format and validation requirements.
- The `template` element defines a function-like template with explicit parameters.
- The `call` element invokes a function template with positional arguments.

Example:
```xml
<cond>
  <case test="output.valid == true">
    <task type="atomic" subtype="success_handler">
      <description>Handle success</description>
    </task>
  </case>
  <case test="output.errors > 0">
    <task type="atomic" subtype="error_handler">
      <description>Handle errors</description>
    </task>
  </case>
</cond>
```
All required and optional fields (including `instructions`, `system`, `model`, and `inputs`) are defined by this schema. For full details and allowed values, please see Appendix A in [Contract:Resources:1.0].

### Example Template

```xml
<task>
  <instructions>Analyze the given code for readability issues.</instructions>
  <system>You are a code quality expert focused on readability.</system>
  <model>claude-3-sonnet</model>
  <!-- The criteria element provides a free-form description used for dynamic evaluation template selection via associative matching -->
  <criteria>validate, log</criteria>
  <inputs>
    <input name="code">The code to analyze</input>
  </inputs>
  <output_format type="json" schema="object" />
  <manual_xml>false</manual_xml>
  <disable_reparsing>false</disable_reparsing>
</task>
```

### Validation Rules

1. All required fields must be present
2. Input names must be unique
3. Boolean fields must be "true" or "false"
4. Model must be a valid LLM identifier
5. Output schema must match basic type validation rules
6. Template parameters must match call arguments

### Error Response Schema

```xml
<xs:complexType name="TaskError">
  <xs:choice>
    <xs:element name="resource_exhaustion">
      <xs:complexType>
        <xs:sequence>
          <xs:element name="resource" type="xs:string"/>
          <xs:element name="message" type="xs:string"/>
          <xs:element name="metrics" type="MetricsType" minOccurs="0"/>
        </xs:sequence>
      </xs:complexType>
    </xs:element>
    <xs:element name="task_failure">
      <xs:complexType>
        <xs:sequence>
          <xs:element name="reason" type="TaskFailureReason"/>
          <xs:element name="message" type="xs:string"/>
          <xs:element name="details" type="DetailsType" minOccurs="0"/>
        </xs:sequence>
      </xs:complexType>
    </xs:element>
  </xs:choice>
</xs:complexType>

<xs:simpleType name="TaskFailureReason">
  <xs:restriction base="xs:string">
    <xs:enumeration value="context_retrieval_failure"/>
    <xs:enumeration value="context_matching_failure"/>
    <xs:enumeration value="context_parsing_failure"/>
    <xs:enumeration value="xml_validation_failure"/>
    <xs:enumeration value="output_format_failure"/>
    <xs:enumeration value="execution_timeout"/>
    <xs:enumeration value="execution_halted"/>
    <xs:enumeration value="subtask_failure"/>
    <xs:enumeration value="input_validation_failure"/>
    <xs:enumeration value="unexpected_error"/>
  </xs:restriction>
</xs:simpleType>
```

### Interface Mapping

This schema is used by the TaskSystem component. For implementation details and interface definitions, see:
- TaskTemplate interface in spec/types.md [Type:TaskSystem:TaskTemplate:1.0]
- Template validation in TaskSystem.validateTemplate() 
- Template parsing in TaskSystem constructor

Note: The new XML attributes (`ref` and `subtype`) and the `<cond>` element map to the corresponding types (i.e., TaskDefinition and FunctionCall) used in the Task System.

### Map Pattern Implementation
Refer to the XML schema for correct usage. For a complete example, please see the Task System documentation.
</file>
<file path="./system/README.md" project="">
# LLM interpreter system-level description

This system proposes a programming environment for LLMs.

```mermaid
graph TD
    subgraph "User Input & Compilation"
        NL[Natural Language Input] --> Compiler
        Compiler -->|Generate XML or DSL| AST[Task AST]
    end
    
    subgraph "Evaluation"
        AST --> Evaluator[Evaluator]
        Evaluator -->|Execute Atomic| Handler[Handler]
        Evaluator -->|Subtask / DSL| Evaluator
        Evaluator -->|Context Query| Memory[Memory System]
    end
    
    subgraph "Memory System"
        LongTerm[Global Metadata Index]
        Working[Working Memory]
        LongTerm -->|Associative Match| Working
    end
    
    subgraph "Runtime / Tools"
        LLM[LLM Session]
        External[Scripts & External Tools]
        Handler --> LLM
        Handler --> External
    end
```

## Table of Contents

## Documentation Guide

### Documentation Structure
This documentation is organized into several key areas:
- **System Level**: Architecture, patterns, and system-wide contracts in `/system`
- **Component Level**: Detailed documentation for each component in `/components`
- **Implementation Details**: Specific implementation guidance in `/components/*/impl`
- **Interface Definitions**: Public and internal interfaces in `/components/*/api`

### Canonical Sources and Cross-References
To reduce duplication and maintain consistency, this documentation follows a pattern of canonical sources and cross-references:

- **Canonical Sources**: Definitive documentation for each concept lives in a single location
- **Cross-References**: Other documents reference the canonical source rather than duplicating content

### Cross-Reference Syntax
The documentation uses a consistent cross-reference syntax:
- `[Type:Name:Version]` for type references (e.g., `[Type:TaskSystem:TaskResult:1.0]`)
- `[Pattern:Name:Version]` for pattern references (e.g., `[Pattern:DirectorEvaluator:1.1]`)
- `[ADR N: Title]` for architecture decision references (e.g., `[ADR 14: Operator Context Configuration]`)
- `[Component:Name:Version]` for component references (e.g., `[Component:Evaluator:1.0]`)
- `[Contract:Category:Name:Version]` for contract references (e.g., `[Contract:Tasks:TemplateSchema:1.0]`)
- `[Interface:Component:Name:Version]` for interface references (e.g., `[Interface:Memory:3.0]`)

### Finding Authoritative Sources
To find the canonical source for a concept:
1. Look for matching files in the pattern directories (`/system/architecture/patterns/`)
2. Check ADRs for decisions (`/system/architecture/decisions/`)
3. Consult component READMEs for component-specific concepts
4. Review contract documents for system-wide agreements

**Note on ADR References**: Architecture Decision Records (ADRs) document the decision-making process at a specific point in time. While they may be referenced for historical context or rationale, all critical functionality should be fully documented in the main documentation without requiring readers to consult ADRs. Documentation should be self-contained with ADRs serving as supplementary, not required, reading.
- [Key Capabilities](#key-capabilities)
- [Core Concepts & DSL Approach](#core-concepts--dsl-approach)
- [Architecture Overview](#architecture-overview)
- [Patterns & Execution Model](#patterns--execution-model)
- [Resource & Error Handling](#resource--error-handling)
- [Usage Workflow](#usage-workflow)
- [Development & Extension](#development--extension)
- [Examples](#examples)
- [References & Documentation Map](#references--documentation-map)

## Key Capabilities

- **Task Decomposition**
  - Convert a complex prompt into smaller, structured subtasks or DSL "functions."
- **DSL & XML Representation**
  - Represent tasks with a domain-specific language (Scheme-like or XML-based) that is easily parsed, versioned, and extended.
- **Resource-Aware Execution**
  - Limit turns and context windows to avoid runaway costs, with warnings at 80% usage.
- **Context Management**
  - Inherit, accumulate, or fetch fresh context using a three-dimensional model (inherit_context, accumulate_data, fresh_context).
- **Director-Evaluator Pattern**
  - Iterative refinement pattern that uses a "director" for output generation and an "evaluator" for feedback, optionally coupled with script execution steps.
- **Subtask Spawning**
  - Dynamically create subtasks if a parent task decides it needs to break the problem down further.
- **Error Taxonomy & Partial Results**
  - Structured error types (resource exhaustion, invalid output, subtask failure) with optional partial outputs for robust debugging and recovery.
- **Modular Components**
  - Compiler for translation, Evaluator for step execution, Memory System for metadata, Handler for LLM calls and script bridging.

## Core Concepts & DSL Approach

### Tasks as Code
A user's request can be viewed as a "program," with subtasks akin to function calls. The system supports two approaches:

1. **Implicit Variable Access** (Legacy):
   Tasks can access variables from their parent environment via `{{variable_name}}` syntax.

2. **Function-Based Templates** (Recommended):
   Templates explicitly declare their parameters, providing clearer scope boundaries and dependencies:

   ```xml
   <template name="analyze_data" params="dataset,config">
     <task>
       <description>Analyze {{dataset}} using {{config}}</description>
     </task>
   </template>
   ```

   These templates are called with positional arguments:
   ```xml
   <call template="analyze_data">
     <arg>weather_data</arg>
     <arg>standard_config</arg>
   </call>
   ```

   This approach offers cleaner variable scoping, explicit dependencies, and improved maintainability.

The system can also compile these instructions into a Scheme-like DSL:

```scheme
(define (process-data data-source)
  (sequential
    (task "load-data" data-source)
    (reduce 
      (lambda (chunk acc)
        (task "analyze-chunk" chunk acc))
      initial-value
      chunks)))
```

### DSL vs. XML

- **XML** is convenient for interoperability, partial validation, and structured storage.
- **DSL** is more expressive and compositional, letting you nest or map tasks in functional style.
- Both representations ultimately feed into the Evaluator, which orchestrates subtask calls.

### Lexical Environment and context management
Each DSL expression or XML subtask runs in an environment that may inherit context window data from its parent, in addition to lexically bound variables. 

## Architecture Overview

| Component | Responsibility | Docs |
|-----------|----------------|------|
| Compiler | Translate natural language or user input into a DSL/AST or XML. | [Compiler README](../components/compiler/README.md) |
| Evaluator | Walk the AST, manage subtask calls, handle resource usage. | [Evaluator README](../components/evaluator/README.md) |
| Task System | Provide operators, templates, and pattern frameworks. | [TaskSystem README](../components/task-system/README.md) |
| Memory System | Manage file metadata & associative context retrieval. | [Memory README](../components/memory/README.md) |
| Handler | LLM interface that exposes a unified tool system and manages resource tracking. | Handler Interface Docs |

### Flow
1. User Input → [Compiler] → AST (XML or DSL)
2. Evaluator interprets AST, possibly fetching context from [Memory].
3. Handler enforces resource limits (turn count, context window) and calls the LLM or external scripts.
4. Subtasks spawn recursively if the Evaluator returns "CONTINUATION" with a subtask request.

## Patterns & Execution Model

### 1. Director-Evaluator Pattern
A powerful approach for iterative refinement:

1. Director produces an initial output.
2. Evaluator checks correctness, possibly runs a validation script.
3. If improvements are needed, the Director is called again with the evaluator's feedback.

This can be done with either a static (predefined) or dynamic (continuation-based) approach.

### 2. Context Management

The system provides comprehensive context control through a three-dimensional model and explicit file selection:

#### Three-Dimensional Context Model
- **inherit_context**: Controls parent context inheritance with values "full" (complete inheritance), "none" (no inheritance), or "subset" (selective inheritance based on relevance).
- **accumulate_data**: Controls whether outputs from prior steps are accumulated (true/false).
- **accumulation_format**: When accumulating data, specifies whether to include "notes_only" or "full_output".
- **fresh_context**: Controls whether new context is fetched via associative matching ("enabled"/"disabled").

#### Explicit File Selection
In addition to the standard context model, tasks can explicitly specify files to include in their context:
```xml
<file_paths>
  <path>./src/main.py</path>
  <path>/absolute/path/file.txt</path>
</file_paths>
```
This feature operates orthogonally to the three-dimensional model, ensuring specified files are always included regardless of other settings.

Each operator type has sensible defaults while allowing explicit overrides through template XML.

### 3. Subtask Spawning
The system implements a standardized mechanism for dynamic task creation and composition through the subtask spawning protocol:

- A parent task returns with `status: "CONTINUATION"` and a `subtask_request` in its notes
- The subtask_request contains type, description, inputs, and optional template_hints
- The system validates the request, checks depth limits, and performs cycle detection
- The subtask executes with its own resource tracking and context management
- Results flow back to the parent task through direct parameter passing
- Error handling preserves partial results and detailed context for recovery

This mechanism enables dynamic task decomposition based on runtime discoveries while maintaining controlled depth management to prevent infinite recursion.

### 4. DSL Environment Model
- Lexical scoping for variables
- Evaluate or apply steps akin to a "metacircular interpreter"
- Potential for dynamic decomposition if tasks fail or are incomplete

## Resource & Error Handling

### Resource Exhaustion
- Handler raises `RESOURCE_EXHAUSTION` if turn count or context window is exceeded.
- The Evaluator can attempt decomposition or surface an error to the parent.

### Invalid Output & Parsing
- If an atomic task yields invalid XML or fails DSL constraints, we capture partial output in notes and either reparse or fail.

### Partial Results
- Operators like "sequential" can preserve prior step outputs even if a later step fails.
- The final error message includes partial outputs if the partial-results policy is enabled.

### Context-Generation Failures
- Typically signaled as a `TASK_FAILURE` with reason codes like "context_retrieval_failure" or "context_parsing_failure."
- The system can attempt fallback or re-try with simpler context if the policy allows.

## Usage Workflow

Below is a typical usage sequence:

1. User submits a high-level command or request.
2. Compiler produces an AST in XML or DSL form.
3. Evaluator loads the AST, checks resource availability, and sets up a Handler.
4. Evaluator executes tasks step by step. For each atomic step:
   - The Handler calls the LLM or runs a script, tracks tokens/turns, returns output.
   - If any step or subtask triggers re-parse or subtask spawning, the Evaluator orchestrates it.
5. Once done, the final TaskResult includes the content, status, and notes fields (which might hold partial results, warnings, or success scores).

## Development & Extension

### Defining New DSL Operators
- Add a new operator for your custom logic (e.g. "map," "filter," "director_evaluator_loop").
- Specify how it inherits context and whether it accumulates partial outputs.

### Authoring Task Templates in XML
- Use `<task type="atomic" ...>` or `<task type="sequential" ...>` with a `<context_management>` block.
- Provide `<inputs>` for data binding and an `<expected_output>` if relevant.

### Error Handling & Recovery
- Consider how partial results help your tasks recover from local failures.
- Possibly define a re-parse template to handle resource exhaustion.

### Resource Configuration
- Per-task or system-wide, set maxTurns, maxContextWindowFraction, etc. in the Handler config.
- Handler automatically enforces these limits.

## Examples

### 1. DSL Example (Scheme-style)
```scheme
(define (director-eval-pipeline initial-data)
  (sequential
    (task "director" initial-data)
    (task "script-execution" (lambda (dirOutput)
       (bash-run "analysis.sh" dirOutput)))
    (task "evaluator" "Evaluate script output" )))
```

### 2. XML Example: Director-Evaluator Loop
```xml
<task type="director_evaluator_loop">
  <description>Iterative code refinement</description>
  <max_iterations>3</max_iterations>
  <context_management>
    <inherit_context>none</inherit_context>
    <accumulate_data>true</accumulate_data>
    <accumulation_format>notes_only</accumulation_format>
    <fresh_context>enabled</fresh_context>
  </context_management>
  <director>
    <description>Generate solution for {{problem}}</description>
    <inputs>
      <input name="problem" from="user_query"/>
      <input name="feedback" from="evaluation_feedback"/>
      <input name="iteration" from="current_iteration"/>
    </inputs>
  </director>
  <evaluator>
    <description>Evaluate solution against requirements</description>
    <inputs>
      <input name="solution" from="director_result"/>
      <input name="requirements" from="user_query"/>
    </inputs>
  </evaluator>
  <script_execution>
    <command>./test_solution.sh</command>
    <timeout>300</timeout>
    <inputs>
      <input name="script_input" from="director_result"/>
    </inputs>
  </script_execution>
  <termination_condition>
    <condition>evaluation.success === true</condition>
  </termination_condition>
</task>
```

### 3. TypeScript Invocation
```typescript
const taskSystem = new TaskSystem({
  maxTurns: 8,
  maxContextWindowFraction: 0.75,
  systemPrompt: "System-level guidance"
});
const memorySystem = new MemorySystem();

const result = await taskSystem.executeTask(
  "Analyze the new code and see if it violates style guidelines.",
  memorySystem
);

if (result.status === "FAILED") {
  console.error("Task failed:", result.notes);
}
```

### 4. Function Template Example
```xml
<!-- Define a template -->
<template name="process_file" params="filepath,options">
  <task type="sequential">
    <description>Process file {{filepath}} with options {{options}}</description>
    <context_management>
      <inherit_context>none</inherit_context>
      <accumulate_data>true</accumulate_data>
      <accumulation_format>notes_only</accumulation_format>
    </context_management>
    <steps>
      <task>
        <description>Load and validate file {{filepath}}</description>
      </task>
      <task>
        <description>Apply transformations with {{options}}</description>
      </task>
    </steps>
  </task>
</template>

<!-- Call the template -->
<call template="process_file">
  <arg>data/sample.csv</arg>
  <arg>{"normalize": true, "filter_nulls": true}</arg>
</call>
```

```typescript
// Register a template
taskSystem.registerTemplate({
  name: "analyze_data",
  parameters: ["dataset", "config"],
  body: /* Task AST node */
});

// Execute a function call
const result = await taskSystem.executeCall({
  templateName: "analyze_data",
  arguments: [
    "weather_data.csv",
    {"method": "statistical", "outliers": "remove"}
  ]
});
```

## References & Documentation Map

### Architecture
- [Overview](./architecture/overview.md)
- [Patterns](./architecture/patterns/)
- [Decisions (ADRs)](./architecture/decisions/)

### Contracts
- [Interfaces](./contracts/interfaces.md)
- [Resources](./contracts/resources.md)
- [Protocols (XML Schema)](./contracts/protocols.md)

### Components
- [Task System](../components/task-system/README.md)
- [Evaluator](../components/evaluator/README.md)
- [Memory System](../components/memory/README.md)
- [Compiler](../components/compiler/README.md)
- Handler Tools
</file>
<file path="./system/architecture/overview.md" project="">
# Architecture Overview

## Problem Statement and Goals

### Atomic Task Matching

The system uses a uniform, heuristic approach for template matching:

- Only atomic tasks have templates; composite tasks are created by combining atomic tasks
- Template matching is performed heuristically by user-defined associative matching tasks with fixed I/O signatures
- Matching logic is uniform for MVP (no operator-specific differences or versioning)
- System always selects the highest-scoring candidate template
- Optional "disable context" flag allows tasks to run without inherited context
- Task results may include optional success score in "notes" field for future adaptive scoring

```mermaid
flowchart TD
    A[User Input] --> B[Create ContextGenerationInput]
    B --> C{Context Disabled?}
    C -->|No| D[Include inheritedContext]
    C -->|Yes| E[Omit inheritedContext]
    D --> F[Call getRelevantContextFor]
    E --> F
    F --> G[Score Candidates]
    G --> H[Select Highest Score]
```

This document provides a high‑level overview of the system architecture. Detailed technical discussions have been moved into canonical files in the sub‑folders:

- **Patterns:** Core patterns such as Director‑Evaluator, Error Handling, and Resource Management (see files under `system/architecture/patterns/`).
– **Decisions (ADRs):** Architecture Decision Records on topics such as context management and memory system design (see `system/architecture/decisions/`).
– **Q&A and Open Questions:** Clarifications and unresolved issues (see `system/architecture/qa/` and `system/architecture/questions.md`).

## Document Map

**This folder contains:**
 - `overview.md`: This high‑level summary and navigation index.
 - `patterns/`: Detailed technical descriptions of core patterns.
 - `decisions/`: Architecture Decision Records (ADRs) with rationale and scope.
 - `qa/`: Frequently asked questions and clarifications.
 - `questions.md`: A list of open and unresolved architecture questions.

For full technical details on any topic, please refer to the canonical file listed above.

### System Goals
1. Primary Goals
- Provide reliable task automation through structured decomposition and execution
- Ensure consistent task processing despite resource constraints
- Enable robust error recovery without human intervention
- Maintain system coherence across task boundaries

2. Quality Goals
- Predictable resource usage through explicit tracking and limits
- Consistent behavior through standardized protocols and interfaces
- Extensible task handling via template-based architecture
- Maintainable system through clear component boundaries

3. Operational Goals
- Handle varying task complexities through dynamic decomposition
- Support diverse task types through flexible template system
- Preserve critical context across task boundaries
- Manage resources efficiently within defined constraints

### System Constraints

#### Resource Constraints
- Fixed context window size
- Limited turn counts
- Synchronous operation only
- File access via Handler tools only

#### Operational Constraints  
- One Handler per task execution
- Immutable Handler configuration
- No persistent state maintenance
- Template immutability during execution

## Core Patterns

### Director-Evaluator Pattern [Pattern:DirectorEvaluator:1.1]

The system implements a standardized mechanism for iterative refinement that supports both static (predefined) and dynamic (continuation-based) variants. This pattern enables:

- Iterative improvement through feedback loops
- Optional integration with script execution
- Explicit termination conditions based on evaluation results

For the complete specification, including implementation details, integration points, and context management integration, see [Pattern:DirectorEvaluator:1.1] in `system/architecture/patterns/director-evaluator.md`.

The system follows a unified context management model. Task input values are dynamically substituted using the `{{...}}` syntax, allowing direct parameter passing between tasks. This explicit binding mechanism (using the `from` attribute) improves clarity and flexibility in task execution by making data dependencies clear and reducing reliance on environment variables.

Context is managed via a single `<context_management>` block that distinguishes between:
 - **Inheritance:** (using the new `inherit_context` enumeration)
 - **Accumulation:** (using the boolean `accumulate_data` and the `accumulation_format` setting)

### Error Handling [Pattern:Error:1.0]
Defines how errors propagate and recover across component boundaries.

See [Interface:ErrorHandling:1.0] in system/contracts/interfaces.md for complete specification.

### Resource Management [Pattern:ResourceManagement:1.0]
Defines resource usage tracking and lifecycle across components.

#### Core Principles
- Handler-based resource isolation
- Per-task resource tracking
- Context window management
- Memory system integration
- No cross-Handler resource sharing
- Read-only memory access

#### Component Responsibilities

##### Handler
- Owns turn counting per task execution
- Manages context window size
- Tracks token usage
- Enforces resource limits
- Ensures session termination

##### Task System
- Creates Handler instances
- Configures immutable resource limits
- Delegates resource tracking to Handler
- Manages template persistence

##### Memory System
- Maintains task context data
- Provides context management interface
- Maintains global file metadata index
- No file content storage

#### Resource Types and Protocols
- Turn Counter: Per-Handler atomic tracking with strict limits
- Context Window: Token-based size monitoring and enforcement
- Memory Resources: Short-term task context with clear boundaries
- Resource Release: Coordinated cleanup and state invalidation
- Error Handling: Resource exhaustion detection and preservation

See [Contract:Resources:1.0] in system/contracts/resources.md for complete specification.

### Task Execution [Pattern:TaskExecution:2.0]
Defines how tasks are structured, executed, and managed.

Key concepts:
- Template-based definition
- Handler-managed execution
- Resource-aware processing
- XML-based protocols

See [Contract:Tasks:2.0] in system/contracts/protocols.md for complete specification.

## Delegation Mechanisms

The system provides a unified tool interface with distinct implementation mechanisms:

### Template Substitution Delegation
- The Evaluator is solely responsible for all template variable substitution
- Handlers receive fully resolved content with no remaining template variables
- This separation ensures clean component boundaries and single responsibility

### Tool Interface
What the LLM sees and interacts with:
- Consistent tool-based invocation pattern for all operations
- Unified parameter schemas and error handling
- Standardized result format regardless of implementation

### Implementation Mechanisms

1. **Direct Tool Implementation**
   - Used for external interactions (files, APIs, scripts)
   - No continuation mechanism or complex context management
   - Executed directly by the Handler component

2. **Subtask Tool Implementation**
   - Involve Memory System for context management
   - Use SubtaskRequest structure and CONTINUATION status
   - Follow the context management model defined in ADR 14

See [Pattern:ToolInterface:1.0] for a detailed explanation of this unified approach.

## Component Architecture

The system consists of four core components working together to process, execute, and manage tasks:

### Handler [Component:Handler:1.0]
LLM interface and resource tracking component.
- Performs ALL file I/O operations (reading, writing, deletion)
- Supports multiple LLM providers with appropriate tool configurations
- Abstracts provider-specific implementation details while maintaining consistent capabilities
- Manages resource usage tracking (turns, tokens)
- Handles LLM interactions and session management
- Works with fully resolved content (no template variable substitution)
- Works with fully resolved content (no template variable substitution)

### Compiler [Component:Compiler:1.0]
Task parsing and transformation component.
- Translates natural language to XML/AST
- Validates against XML schema
- Handles task transformation
- Manages template validation

See [Contract:Integration:CompilerTask:1.0] for integration specification.

### Evaluator [Component:Evaluator:1.0]
Execution control component.
- Controls AST processing and execution
- Manages all lexical environments and variable scoping
- Performs all template variable substitution before Handler invocation
- Handles different substitution rules for function vs. standard templates
- Manages failure recovery
- Tracks resource usage
- Handles reparse requests

See [Contract:Integration:EvaluatorTask:1.0] for integration specification.

### Task System [Component:TaskSystem:1.0]
Task execution and management component.
- Coordinates task execution via Handlers
- Manages task templates and matching
- Interfaces with Memory System
- Processes XML input/output

See components/task-system/README.md for complete specification.

### Memory System [Component:Memory:3.0]
Metadata management component.
- Maintains global file metadata index (paths and descriptive strings)
- Provides metadata for associative matching without ranking or prioritization
- Supplies metadata for file-based lookup and partial matching
- Does NOT store file content, perform file operations, track resources, or rank matches
- NEVER performs file I/O operations - all file access is handled by Handler
- Follows read-only context model (no updateContext capability)

See [Contract:Integration:TaskMemory:3.0] for integration specification.

## Component Integration

### Core Integration Patterns
Components interact through defined contracts:

#### Compiler ↔ Task System
- Task parsing coordination
- Schema validation
- Template utilization

#### Evaluator ↔ Compiler
- AST execution feedback
- Reparse requests
- Resource usage updates

#### Task System ↔ Evaluator
- Execution coordination
- Resource allocation
- State management

#### Task System ↔ Memory System
- Context management
- Metadata index access
- Associative matching support

See system/contracts/interfaces.md for complete contract specifications.

### Resource Ownership
- Handlers own task resources
- Memory system owns context storage
- Task system coordinates allocation
- Clean resource release required

See system/contracts/resources.md for complete ownership model.

### System-Wide Protocols
- XML-based task definitions and protocols
- Standard error propagation
- Resource usage tracking
- Context management

---

### Function-Based Template Pattern [Pattern:FunctionTemplate:1.0]

The system implements a function-based template pattern that enforces clear boundaries between caller and callee contexts:

1. **Template Definition**
   - Templates explicitly declare their parameters using a `params` attribute
   - Each template has its own lexical scope containing only its parameters
   - Templates are registered in a central TaskLibrary during parsing

2. **Function Calling**
   - Function calls use positional arguments evaluated in the caller's context
   - Arguments can be literals, variable references, or nested expressions
   - A new environment is created for each function call with bindings for parameters
   - No implicit access to the caller's environment is allowed

3. **AST Representation**
   - TemplateNode represents function definitions
   - FunctionCallNode represents function invocations
   - ArgumentNode represents argument values

This pattern enables:
 - Clear data dependencies between components
 - Improved reasoning about variable scope
 - Better encapsulation of implementation details
 - Foundations for more advanced functional patterns

### Sequential Task Management [Pattern:SequentialTask:2.0]

The system maintains **explicit task history** for sequential operations. This design clarifies how multiple steps in a single task can share context in a controlled, trackable way, and how partial or final outputs are captured.

1. **Output Tracking**
   - The Evaluator maintains a list of all previous task outputs, keyed by step index or ID.
   - The lifecycle for this history is well-defined; it is preserved until the sequence finishes.
   - Storage must remain resource-aware to avoid memory limit issues. If output is large, the evaluator can store a summarized version or notes-only.

### Subtask Spawning Mechanism [Pattern:SubtaskSpawning:1.0]

The system implements a standardized subtask spawning mechanism that enables dynamic task creation and composition:

1. **Continuation Protocol**
   - A parent task returns with `status: "CONTINUATION"` and a `subtask_request` in its notes
   - The subtask_request contains type, description, inputs, and optional template_hints
   - The system validates the request structure before processing
   - Depth control prevents infinite recursion and detects cycles

2. **Request Structure**
   ```typescript
   interface SubtaskRequest {
     // Required fields
     type: TaskType;                      // Type of subtask to spawn
     description: string;                 // Description of the subtask
     inputs: Record<string, any>;         // Input parameters for the subtask
     
     // Optional fields
     template_hints?: string[];           // Hints for template selection
     context_management?: {               // Override default context settings
       inherit_context?: 'full' | 'none' | 'subset';
       accumulate_data?: boolean;
       accumulation_format?: 'notes_only' | 'full_output';
       fresh_context?: 'enabled' | 'disabled';
     };
     max_depth?: number;                  // Override default max nesting depth
     subtype?: string;                    // Optional subtype for atomic tasks
   }
   ```

3. **Data Flow**
   ```mermaid
   flowchart LR
       A[Parent LLM] -->|"tools.analyzeData({...})"| B[Handler]
       B -->|CONTINUATION with subtask_request| C[Task System]
       C -->|Execute subtask| D[Subtask LLM]
       D -->|Result| C
       C -->|Add as tool response| B
       B -->|Continue with tool result| A
   ```
   - Parent tasks make tool calls that require complex processing
   - Handler returns CONTINUATION status with subtask_request
   - Task System executes subtask independently
   - Subtask result is added as a tool response to parent's session
   - Parent continues execution with the tool result in conversation history
   - From the LLM's perspective, this appears as a normal tool call and response
   - No special resumption methods or complex continuation mechanisms required

4. **Context Integration**
   - Subtasks have default context management settings:
     | inherit_context | accumulate_data | accumulation_format | fresh_context |
     |-----------------|-----------------|---------------------|---------------|
     | subset          | false           | notes_only          | enabled       |
   - These defaults can be overridden through explicit configuration in the subtask_request
   - Context management follows the hybrid configuration approach with operator-specific defaults and explicit overrides

5. **Resource Protection**
   - Maximum nesting depth prevents infinite recursion (default: 5 levels)
   - Cycle detection prevents tasks from spawning themselves
   - Resource usage is tracked across the entire subtask chain
   - Partial results are preserved when subtasks fail

6. **Error Handling**
   - Subtask failures are wrapped in standardized error structures
   - Parent tasks receive detailed error information including:
     - Original subtask request
     - Specific error details from the subtask
     - Current nesting depth
     - Any partial results that were generated before failure
   - This enables robust recovery strategies at the parent task level

For historical context and decision rationale, see [ADR 11: Subtask Spawning Mechanism].

2. **Context Management**
   - The system implements a hybrid configuration approach with operator-specific defaults and explicit overrides:
     - **inherit_context**: Controls whether a subtask inherits "full" parent context, "none", or a "subset" based on relevance.
     - **accumulate_data**: Controls whether outputs from prior steps are accumulated.
     - **accumulation_format**: Specifies whether to store "notes_only" or "full_output" when accumulating data.
     - **fresh_context**: Controls whether new context is generated via associative matching.
   
   - **Constraint**: Fresh context generation (`fresh_context="enabled"`) is mutually exclusive with context inheritance (`inherit_context="full"` or `inherit_context="subset"`):
     - If a task inherits context (full or subset), it must not generate fresh context
     - If a task generates fresh context, it must not inherit context from its parent
     - This simplifies the system by preventing potential context duplication

   - **Subtype-Based Defaults**: Default context settings are determined by both operator type and subtype:
     - For atomic tasks with "standard" subtype: inherit_context="full", fresh_context="disabled"
     - For atomic tasks with "subtask" subtype: inherit_context="none", fresh_context="enabled"
     - This distinction allows different behavior for regular tasks vs. subtasks
   
   - Default settings for sequential tasks:
     | inherit_context | accumulate_data | accumulation_format | fresh_context |
     |-----------------|-----------------|---------------------|---------------|
     | full            | true            | notes_only          | disabled      |
   
   - These defaults can be overridden through an explicit XML structure:
   ```xml
   <context_management>
       <inherit_context>full|none|subset</inherit_context>
       <accumulate_data>true|false</accumulate_data>
       <accumulation_format>notes_only|full_output</accumulation_format>
       <fresh_context>enabled|disabled</fresh_context>
   </context_management>
   ```
   
   - When the context_management block is omitted, operator-specific defaults apply
   - When present, explicit settings override the defaults
   - This hybrid approach provides both consistency and flexibility

3. **Partial Failures**
   - If a step fails, all previous step outputs remain available in the final (error) output.
   - The final error output includes which step number or ID caused the failure.

4. **Resource Handling**
   - Maximum stored-history size is enforced by the system to prevent out-of-memory or context window exhaustion.
   - The evaluator must handle large output storage carefully, possibly discarding or summarizing to keep track usage in check.
   - Clear cleanup protocols ensure that once the sequence completes (successfully or in error), the stored step outputs are removed.

In summary, **SequentialTask** pattern addresses multi-step tasks with optional or partial data inheritance across steps, ensuring that both resource usage and error behavior remain consistent and predictable.

See system/contracts/protocols.md for protocol specifications.
</file>
<file path="./system/architecture/patterns/context-frames.md" project="">
# Context Frame Pattern [Pattern:ContextFrame:1.0]

## Related Documents
- Memory System ADR in [ADR:Memory:1.0]
- Memory component in [Component:Memory:3.0]
- Resource Management Pattern in [Pattern:ResourceManagement:1.0]

## Context Frame Operations

### Operator Default Settings

Each operator type and subtype combination has specific default context management settings:

| Operator Type (Subtype) | inherit_context | accumulate_data | accumulation_format | fresh_context |
|-------------------------|-----------------|-----------------|---------------------|---------------|
| atomic (standard)       | full            | false           | minimal             | disabled      |
| atomic (subtask)        | none            | false           | minimal             | enabled       |
| sequential              | full            | true            | minimal             | disabled      |
| reduce                  | none            | true            | minimal             | enabled       |
| script                  | full            | false           | minimal             | disabled      |
| director_evaluator_loop | none            | true            | minimal             | enabled       |

### Context Management Constraints

The following constraints apply to context management settings:

1. **Mutual Exclusivity**: `fresh_context="enabled"` cannot be combined with `inherit_context="full"` or `inherit_context="subset"`
   - If `inherit_context` is "full" or "subset", `fresh_context` must be "disabled"
   - If `fresh_context` is "enabled", `inherit_context` must be "none"

2. **Subtype-based Defaults**: Default context settings depend on both operator type and subtype
   - Atomic tasks with "standard" subtype default to `inherit_context="full"` and `fresh_context="disabled"`
   - Atomic tasks with "subtask" subtype default to `inherit_context="none"` and `fresh_context="enabled"`
   - For CONTINUATION-based subtasks, the "subtask" subtype is automatically applied

3. **Validation Errors**: Templates violating these constraints will fail validation with clear error messages

### File Paths as Orthogonal Context Control

In addition to the three-dimensional context model, the system supports direct file specification through the `file_paths` feature:

- **Purpose**: Explicitly include specific files in task context
- **Behavior**: Operates outside the standard three-dimensional model
- **Priority**: Files specified via `file_paths` are always included in context, regardless of other settings
- **Integration**: Works alongside other context management settings

```xml
<task type="atomic">
  <description>Task with specific file context</description>
  <context_management>
    <inherit_context>none</inherit_context>
    <fresh_context>disabled</fresh_context>
  </context_management>
  <file_paths>
    <path>./src/main.py</path>
    <path>/absolute/path/file.txt</path>
  </file_paths>
</task>
```

When `file_paths` is combined with other context settings:

- With `inherit_context="subset"`: Specified files become the subset (replacing associative matching-based subset selection)
- With `fresh_context="enabled"`: Specified files are forcibly included while associative matching still runs for additional context

### Common Context Management Patterns

The three-dimensional context management model can be configured to achieve various behaviors:

#### 1. Clear All Context (Fresh Start)
```xml
<context_management>
  <inherit_context>none</inherit_context>
  <accumulate_data>false</accumulate_data>
  <fresh_context>enabled</fresh_context>
</context_management>
```
This configuration:
- Ignores any parent/inherited context
- Doesn't accumulate previous results
- Generates entirely fresh context through associative matching

#### 2. Rebuild Context While Preserving History
```xml
<context_management>
  <inherit_context>none</inherit_context>
  <accumulate_data>true</accumulate_data>
  <accumulation_format>minimal</accumulation_format>
  <fresh_context>enabled</fresh_context>
</context_management>
```
This configuration:
- Ignores parent/inherited context
- Keeps accumulated outputs from previous steps
- Generates fresh context in addition to accumulated data
- Specifically, preserves essential metadata from previous steps

#### 3. Complete Context Preservation
```xml
<context_management>
  <inherit_context>full</inherit_context>
  <accumulate_data>true</accumulate_data>
  <accumulation_format>full</accumulation_format>
  <fresh_context>disabled</fresh_context>
</context_management>
```
This configuration:
- Preserves all parent context
- Keeps complete outputs from previous steps
- Doesn't generate additional context

- **Subtype Awareness**: Context defaults depend on both operator type and subtype
  - Standard atomic tasks inherit context but don't generate fresh context
  - Subtasks don't inherit context but do generate fresh context
  - This distinction avoids duplication while supporting different use cases

These defaults apply when no explicit context_management block is provided. When present, the explicit settings override the defaults:

```xml
<context_management>
    <inherit_context>full|none|subset</inherit_context>
    <accumulate_data>true|false</accumulate_data>
    <accumulation_format>notes_only|full_output</accumulation_format>
    <fresh_context>enabled|disabled</fresh_context>
</context_management>
```

Settings are merged during template loading, with explicit settings taking precedence over defaults.

### Context Inheritance Modes

The system supports three context inheritance modes:
- **full**: Complete inheritance of parent context
- **subset**: Selective inheritance of specific context elements
- **none**: No context inheritance

When the "disable context" flag is active, the inherited context is completely omitted from the ContextGenerationInput.

```mermaid
flowchart TD
    A[Task Start] --> B{Context Mode?}
    B -->|Full| C[Use Parent Context]
    B -->|Subset| D[Use Selected Context]
    B -->|None| E[No Inherited Context]
    C --> F[Execute Task]
    D --> F
    E --> F
```

```mermaid
flowchart TD
    A[Atomic Task Begins] --> B[Check "Disable Context" Flag]
    B -- Yes --> C[Omit inheritedContext in ContextGenerationInput]
    B -- No --> D[Include parent context]
    C --> E[Pass to MemorySystem.getRelevantContextFor]
    D --> E
    E --> F[Heuristic Matching & Candidate Scoring]
    F --> G[Select Highest-scoring Template]
```

## Formal Definitions

Example ContextGenerationInput:
```json
{
  "taskText": "analyze experimental data",
  "inheritedContext": "context string if available",
  "previousOutputs": "summarized outputs if any"
}
```

Example AssociativeMatchResult:
```json
{
  "context": "retrieved relevant context",
  "matches": [
    ["fileA.txt", "metadata info"],
    ["fileB.txt", null]
  ]
}
```

See also:
- [ADR:002-context-management]
- [ADR:005-context-handling]

### Frame Creation and Extension
```typescript
interface ContextFrame {
    // Based on Memory System design (see [ADR:Memory:1.0])
    bindings: Map<string, any>;    // Current variable bindings
    context: Map<string, any>;     // Working memory context
    
    extend(bindings: Map<string, any>): ContextFrame;
    cleanup(): void;
}

---
**Related Decisions:** For higher‑level context management decisions, see [decisions/002-context-management.md](../decisions/002-context-management.md) and [decisions/005-context-handling.md](../decisions/005-context-handling.md).
```

## EnvironmentFrame Interface

To support argument passing during task evaluation, we introduce a new interface:

```typescript
interface EnvironmentFrame {
    bindings: Map<string, any>;    // Arguments bound to this scope
    parent?: EnvironmentFrame;     // Parent scope for lookup chain
    context: Record<string, any>;    // Task execution context (separate from bindings)
}
```

This `EnvironmentFrame` is created at the start of each task execution (using the new `createFrame` method) and is chained to any existing parent frame. Its purpose is to maintain a clear separation between argument bindings and the overall task context.

### Frame Immutability
- No modification of existing frames
- New frames created through extension
- Clear task isolation boundaries
- Minimal required context principle

### Memory System Integration
- Associative memory system mediates between long-term and working memory
- Working memory instantiated from long-term storage using associative retrieval
- Context updates managed through frame extension
- Resource tracking delegated to Handler

## Implementation Examples

### Frame Creation
```typescript
// Example based on [Component:Memory:1.0] implementation
class ContextFrame implements IContextFrame {
    private bindings: Map<string, any>;
    private context: Map<string, any>;
    
    extend(newBindings: Map<string, any>): ContextFrame {
        const frame = new ContextFrame();
        frame.bindings = new Map([...this.bindings, ...newBindings]);
        frame.context = this.context;  // Shared context reference
        return frame;
    }
    
    cleanup(): void {
        // Resource cleanup handled by Handler
        this.bindings.clear();
        this.context = null;
    }
}
</file>
<file path="./system/architecture/patterns/director-evaluator.md" project="">
# Director‑Evaluator Pattern [Pattern:DirectorEvaluator:1.1]

**Canonical Reference:** This document is the authoritative description of the Director‑Evaluator Pattern. All extended descriptions in planning or misc files should refer here.

## Overview

The Director-Evaluator pattern is a specialized variant of the unified task‐subtask execution model, implemented in two ways:

1. **Dynamic Variant**: A parent task (the **Director**) produces an initial output that may require evaluation. When the Director's output returns with a `CONTINUATION` status and an `evaluation_request` object in its `notes`, the evaluation subtask is spawned dynamically.

2. **Static Variant**: A predefined task structure of type `director_evaluator_loop` that explicitly defines the Director, Evaluator, and optional script execution steps with clear iteration control.

**Note:** The `evaluation_request` object must include:
 - `type`: a string indicating the evaluation type,
 - `criteria`: a free-form string providing descriptive criteria used for dynamic evaluation template selection via associative matching,
 - `target`: a string representing the target (for example, the bash script command to run or the evaluation focus).

## Pattern Description

### Dynamic Variant

This pattern follows a three-phase flow:

1. **Parent Task (Director):**
   - Generates an initial output based on the user's instructions.
   - May return a result with `status: 'CONTINUATION'` and a populated `evaluation_request` in its `notes`.
   - Uses a `<context_management>` block with `inherit_context` set to `none` to start a new execution chain.

2. **Evaluation Trigger via Continuation:**
   - The evaluation step is triggered dynamically when the Director task returns a `CONTINUATION` status.
   - The embedded `evaluation_request` specifies both:
       - The **bash script** (or external callback mechanism) to execute, and
       - The **target** string that describes the aspects of the output requiring evaluation.
   - The Evaluator uses this information—with associative matching—to select an appropriate evaluation task template.

3. **Child Task (Evaluator):**
   - Is dynamically spawned by the Evaluator when it detects a `CONTINUATION` status along with an `evaluation_request` in the Director's output.
   - Uses a `<context_management>` block with `inherit_context` set to `subset` and `accumulate_data` enabled to incorporate only the relevant context.
   - Executes the evaluation subtask—which may include invoking the specified bash script via the Handler or Evaluator—and feeds its results back to the parent task.

### Static Variant (Director-Evaluator Loop)

The static variant uses a dedicated task type with a standardized structure:

```xml
<task type="director_evaluator_loop">
  <description>{{task_description}}</description>
  <max_iterations>5</max_iterations>
  <context_management>
    <inherit_context>none</inherit_context>
    <accumulate_data>true</accumulate_data>
    <accumulation_format>notes_only</accumulation_format>
    <fresh_context>enabled</fresh_context>
  </context_management>
  <director>
    <description>Generate solution for {{original_prompt}}</description>
    <inputs>
      <input name="original_prompt" from="user_query"/>
      <input name="feedback" from="evaluation_feedback"/>
      <input name="iteration" from="current_iteration"/>
    </inputs>
  </director>
  <evaluator>
    <description>Evaluate solution against {{original_prompt}}</description>
    <inputs>
      <input name="solution" from="director_result"/>
      <input name="original_prompt" from="user_query"/>
    </inputs>
  </evaluator>
  <script_execution>
    <!-- Optional script execution -->
    <command>{{script_path}}</command>
    <timeout>300</timeout>
    <inputs>
      <input name="script_input" from="director_result"/>
    </inputs>
  </script_execution>
  <termination_condition>
    <!-- Optional early termination -->
    <condition>evaluation.success === true</condition>
  </termination_condition>
</task>
```

### Parameter Passing

The Director-Evaluator loop uses direct parameter passing rather than environment variables:

```mermaid
flowchart LR
    A[Parent Task] -->|"TaskResult{status:CONTINUATION,\n notes:{subtask_request}}"| B[Task System]
    B -->|"Template Selection\n& Direct Input Passing"| C[Subtask]
    C -->|"TaskResult"| D[Task System]
    D -->|"Resume with\n{subtask_result:TaskResult}"| A
```

### Result Structure

All task results follow a consistent base structure with extensions for specific needs:

```typescript
// Base task result structure
interface TaskResult {
    content: string;
    status: "COMPLETE" | "CONTINUATION" | "WAITING" | "FAILED";
    notes: {
        [key: string]: any;
    };
}

// Specialized structure for evaluator feedback
interface EvaluationResult extends TaskResult {
    notes: {
        success: boolean;        // Whether the evaluation passed
        feedback: string;        // Human-readable feedback message
        details?: {              // Optional structured details
            metrics?: Record<string, number>; // Optional evaluation metrics
            violations?: string[];            // Specific validation failures
            suggestions?: string[];           // Suggested improvements
            [key: string]: any;               // Extension point
        };
        scriptOutput?: {         // Present when script execution is involved
            stdout: string;      // Standard output from script
            stderr: string;      // Standard error output from script
            exitCode: number;    // Exit code from script
        };
    };
}
```

### Example Workflow

Below is a conceptual example (in pseudocode) illustrating the updated director-evaluator flow. In this scenario, the Director task returns a result with `status: 'CONTINUATION'` and an embedded `evaluation_request`:

```typescript
// Example TaskResult returned by the Director task
const taskResult: TaskResult = {
    content: "Initial solution output...",
    status: 'CONTINUATION',
    notes: {
        evaluation_request: {
            type: "bash_script",
            criteria: ["validate", "log"],
            target: "run_analysis.sh"
        }
    }
};
```

Upon receiving this result, the Evaluator:
1. Uses the `evaluation_request` details (including the target string) to perform associative matching and select an appropriate evaluation task template.
2. Dynamically spawns an evaluation subtask (the Child Task) with a `<context_management>` block set to inherit a subset of context.
3. If necessary, invokes the specified bash script callback via the Handler or its own mechanism.
4. Feeds the evaluation results back to the Director, allowing the overall task to continue.

## Integration with the Unified Architecture

### Context Management Defaults

The Director-Evaluator pattern has specific default context management settings:

| Task Type | inherit_context | accumulate_data | accumulation_format | fresh_context |
|-----------|-----------------|-----------------|---------------------|---------------|
| director_evaluator_loop | none | true | notes_only | enabled |
| director (component) | full | false | notes_only | disabled |
| evaluator (component) | full | false | notes_only | disabled |

These defaults adhere to the mutual exclusivity constraint: when `inherit_context` is "full", `fresh_context` must be "disabled".

These defaults can be overridden through explicit configuration:

```xml
<task type="director_evaluator_loop">
  <description>Iterative refinement process</description>
  <context_management>
    <inherit_context>none</inherit_context>
    <accumulate_data>true</accumulate_data>
    <accumulation_format>full_output</accumulation_format>
    <fresh_context>enabled</fresh_context>
  </context_management>
  <!-- other elements -->
</task>
```

The Director-Evaluator pattern fully embraces the hybrid configuration approach and integrates with the three-dimensional context management model:

- **Inherited Context**: The parent task's context, controlled by `inherit_context` setting.
- **Accumulated Data**: The step-by-step outputs collected during sequential execution, controlled by `accumulate_data` setting.
- **Fresh Context**: New context generated via associative matching, controlled by `fresh_context` setting.

When the context_management block is omitted, the operator-specific defaults apply. When present, explicit settings override the defaults, providing both consistency and flexibility.

### Script Execution Integration

Script execution is implemented as a tool call handled by the Handler:

```xml
<script_execution>
  <command>{{script_path}}</command>
  <timeout>300</timeout>
  <inputs>
    <input name="script_input" from="director_result"/>
  </inputs>
</script_execution>
```

When a script execution step is included:
- The script receives the Director's output as input
- Script output (stdout, stderr, exit code) is captured
- Results are passed to the Evaluator for processing
- The Evaluator considers both the original output and script results
- As a tool call, script execution does not use the continuation mechanism

## Relationship to Subtask Spawning

The Director-Evaluator Loop and Subtask Spawning mechanism are complementary features:

### Integration Points

1. **Dynamic Director-Evaluator Implementation**
   - The dynamic variant of Director-Evaluator uses the subtask spawning mechanism
   - When a Director task returns with `CONTINUATION` status and an `evaluation_request`, it's using subtask spawning
   - The Evaluator component handles the subtask creation and execution
   - Results flow back to the parent task using the standard subtask result passing

2. **Context Management Alignment**
   - Both patterns follow the hybrid configuration approach from ADR 14
   - Director-Evaluator Loop uses `inherit_context: none` by default
   - Subtasks use `inherit_context: subset` by default
   - Both can be explicitly configured through their respective XML structures

### Pattern Selection Guide

**Director-Evaluator Loop** is optimal for:
- Iterative refinement processes requiring multiple feedback cycles
- Workflows needing external validation via scripts with standardized input/output
- Scenarios with well-defined evaluation criteria and termination conditions
- Cases requiring preservation of iteration history for auditing or debugging

**Subtask Spawning** is better for:
- Ad-hoc dynamic task creation based on runtime discoveries
- Complex task trees with varying subtypes and unpredictable branching
- Workflows requiring specialized template selection per subtask
- Situations where flexibility in execution path is more important than iteration structure

The patterns can be used together, with Director-Evaluator loops spawning subtasks when needed for specialized processing.

## Conclusion

The Director-Evaluator pattern provides a structured approach to iterative refinement, with both dynamic and static variants. It uses direct parameter passing for clean data flow and integrates fully with the unified context management model. This approach ensures flexibility and seamless integration within the overall task execution architecture.
</file>
<file path="./system/architecture/patterns/error-resources.md" project="">
# Error Condition Resource Pattern [Pattern:ErrorResource:1.0]

## Resource Handling During Errors

From task-system/spec/behaviors.md and impl/resource-management.md:

### Required Actions
On error detection:
- Stop resource usage
- Complete resource accounting
- Begin cleanup process
- Surface error with metrics

### Cleanup Requirements
From task-system/impl/resource-management.md:
- Handler cleanup
- Resource release
- Memory cleanup
- Metric collection

## Implementation

```typescript
interface ErrorResourceMetrics {
    // From task-system/spec/types.md
    turns: {
        used: number;
        limit: number;
    };
    context: {
        used: number;
        limit: number;
        peakUsage: number;
    };
}
```</file>
<file path="./system/architecture/patterns/errors.md" project="">
# Error Handling and Recovery Pattern [Pattern:Error:1.0]

**Intended Focus:** This document covers overall error detection, classification, and recovery strategies. For resource‑related cleanup details, see the "Resource Cleanup" subsection below. (For low‑level resource metrics, refer to [Handler Resource Tracking](../decisions/001-memory-system.md) and related docs.)

## 1. Pattern Definition

### 1.1 Purpose
Error handling pattern for the task execution system, focusing on three key concerns:
- Resource exhaustion detection and recovery
- Invalid output handling
- Progress failure management

### 1.2 Context
This pattern is used by:
- [Component:TaskSystem:1.0] for error detection
- [Component:Evaluator:1.0] for error recovery
- [Component:Handler:1.0] for resource monitoring

### 1.3 Core Elements
- Error Type System: See [Type:TaskSystem:TaskError:1.0]
- Recovery Protocols: See [Protocol:Tasks:Reparse:1.0]
- Resource Contracts: See [Contract:Resources:1.0]

## 2. Error Categories

### 2.1 Resource Exhaustion
Handled by [Protocol:Tasks:Reparse:1.0]

Resource exhaustion occurs when a task exceeds allocated system resources:
- **Type**: 'RESOURCE_EXHAUSTION'
- **Resources**: 'turns' | 'context' | 'output'
- **Metrics**: Contains usage and limit values
- **Recovery**: Attempt task decomposition

### 2.2 Task Failure
Related to [Contract:Tasks:TemplateSchema:1.0]

Task failures now include a standardized `reason` field for more specific categorization:

```typescript
type TaskFailureReason = 
  | 'context_retrieval_failure'    // Failure to retrieve context data
  | 'context_matching_failure'     // Failure in associative matching algorithm 
  | 'context_parsing_failure'      // Failure to parse or process retrieved context
  | 'xml_validation_failure'       // Output doesn't conform to expected XML schema
  | 'output_format_failure'        // Output doesn't meet format requirements
  | 'execution_timeout'            // Task execution exceeded time limits
  | 'execution_halted'             // Task execution was deliberately terminated
  | 'subtask_failure'              // A subtask failed, causing parent task failure
  | 'input_validation_failure'     // Input data didn't meet requirements
  | 'unexpected_error';            // Catch-all for truly unexpected errors
```

Task failures also include a structured details object that may contain error-specific information such as:
- partial_context: Any partial context that was retrieved before failure
- context_metrics: Metrics related to context retrieval
- violations: Specific validation rule violations
- partialResults: Results from steps that completed before failure
- failedStep: Index of the step that failed in a sequential task

### 2.3 Failure to Make Progress
Progress failures occur when a task cannot advance despite resources being available:
- **Type**: 'TASK_FAILURE'
- **Reason**: 'execution_halted'
- **Indicators**: Multiple rounds with no state change
- **Recovery**: Alternative approach or termination

### 2.4 Partial Results Handling

The system maintains a standardized approach for preserving partial results when multi-step operations fail:

### Error Output Structure

#### Atomic Tasks
```typescript
// Successful atomic task
{
  content: "Complete task output",
  status: "COMPLETE", 
  notes: {
    dataUsage: "Resource usage statistics",
    successScore: 0.95
  }
}

// Failed atomic task
{
  content: "Partial output generated before failure",  // Partial content in content field
  status: "FAILED",
  notes: {
    dataUsage: "Resource usage statistics",
    executionStage: "validation",
    completionPercentage: 60
  }
}
```
Task status (`COMPLETE` vs `FAILED`) indicates whether content is complete or partial.

#### Sequential Tasks
```typescript
{
  type: 'TASK_FAILURE',
  reason: 'subtask_failure',
  message: 'Sequential task "Process Dataset" failed at step 3',
  details: {
    failedStep: 2,
    totalSteps: 5,
    partialResults: [
      { 
        stepIndex: 0, 
        content: "Data loaded successfully: 1000 records",
        notes: { 
          recordCount: 1000, 
          status: "completed"
        }
      },
      { 
        stepIndex: 1, 
        content: "Data transformed to required format",
        notes: { 
          transformType: "normalization", 
          status: "completed"
        }
      }
    ]
  }
}
```

#### Reduce Tasks
```typescript
{
  type: 'TASK_FAILURE',
  reason: 'subtask_failure',
  message: 'Reduce task "Aggregate metrics" failed processing input 2',
  details: {
    failedInputIndex: 2,
    totalInputs: 5,
    processedInputs: [0, 1],
    currentAccumulator: { totalCount: 1500, averageValue: 42.3 },
    partialResults: [
      { 
        inputIndex: 0, 
        content: "Processed metrics for server 1",
        notes: { 
          status: "completed",
          serverName: "server-01",
          metricsCount: 250
        }
      },
      { 
        inputIndex: 1, 
        content: "Processed metrics for server 2",
        notes: { 
          status: "completed",
          serverName: "server-02",
          metricsCount: 180
        }
      }
    ]
  }
}
```

#### Storage Format Control
The format of preserved partial results depends on the task's `accumulation_format` setting:
- `notes_only`: Only the notes field is preserved (default for memory efficiency)
- `full_output`: Both content and notes fields are preserved (with size limits)

```typescript
// With accumulation_format="notes_only"
partialResults: [
  { 
    stepIndex: 0, 
    notes: { 
      recordCount: 1000,
      status: "completed"
      // Other essential metadata
    }
  }
]

// With accumulation_format="full_output"
partialResults: [
  { 
    stepIndex: 0,
    content: "Complete step output text",
    notes: { 
      recordCount: 1000,
      status: "completed"
    }
  }
]
```

Each operator type has specific default settings for context management. For sequential tasks, the default `accumulation_format` is `minimal`. These defaults apply when the `context_management` block is omitted. When present, explicit settings override the defaults, following the hybrid configuration approach.

#### Size Management
To prevent memory issues:
- Individual step outputs are kept reasonably sized
- When accumulated data becomes too large, older results may be summarized or truncated
- The system indicates when truncation has occurred

### 2.5 Output Format Validation

When a task specifies an output format using `<output_format type="json" schema="...">`, validation failures result in:

```typescript
{
  type: 'TASK_FAILURE',
  reason: 'output_format_failure',
  message: 'Expected output of type "array" but got "object"',
  details: {
    expectedType: "array",
    actualType: "object",
    originalOutput: "..." // The original output
  }
}
```

This error occurs when:
1. The task specifies an output format with `type="json"`
2. The output is successfully parsed as JSON
3. The parsed content doesn't match the specified schema type

The original output is preserved in the `partialOutput` field to allow for potential recovery or manual parsing. The error includes both the expected and actual types to aid in debugging and recovery.

## 3. Recovery Process

### 3.1 Detection Phase
See [Interface:Handler:ResourceMonitoring:1.0]

Error detection now includes identifying:
- Context retrieval failures
- Context matching failures
- Context parsing failures
- Output format validation failures

```typescript
function handleTaskError(error: TaskError) {
  if (error.type === 'RESOURCE_EXHAUSTION') {
    // Handle resource exhaustion based on resource type
    handleResourceExhaustion(error);
  } else if (error.type === 'TASK_FAILURE') {
    // Handle task failure based on reason
    if (error.reason.startsWith('context_')) {
      // Handle context-related failures
      handleContextFailure(error);
    } else if (error.reason === 'subtask_failure') {
      // Handle subtask failures, potentially using partial results
      handleSubtaskFailure(error);
    } else if (error.reason === 'output_format_failure') {
      // Handle output validation failures
      handleOutputFormatFailure(error);
    } else {
      // Handle other failures
      handleGeneralFailure(error);
    }
  }
}
```

### 3.2 Planning Phase
See [Component:Evaluator:1.0] for error handling.

Based on error type and reason, the system will surface appropriate error information:
- **Resource Exhaustion**: Complete resource metrics and context
- **Context Failures**: Context-related error details
- **Validation Errors**: Validation failure specifics
- **Subtask Failures**: Error details including partial execution data

### 3.3 Execution Phase
See [Protocol:Tasks:Reparse:1.0] for execution details.

Error handling involves:
- Preparing complete error context
- Including relevant partial execution data
- Surfacing errors through standard error flow
- Providing detailed diagnostics

- **Associative Matching Failures:** If an associative matching task encounters an error—such as insufficient context or partial output—it will automatically trigger a retry. These errors will include any partial output and, if available, an optional success score (recorded in the task's `notes` field) to support future adaptive behavior.

#### Error Recovery Flow for Associative Matching

```mermaid
flowchart TD
    A[Associative Matching Task Initiated] --> B{Error Detected?}
    B -- Yes --> C[Trigger Automatic Retry]
    C --> D[Capture Partial Output & Success Score]
    D --> E[Reattempt Associative Matching]
    B -- No --> F[Proceed with Normal Execution]
```

### Subtask Failure Handling

When a subtask fails, the system provides a standardized error structure that preserves context and enables recovery:

```typescript
{
  type: 'TASK_FAILURE',
  reason: 'subtask_failure',
  message: 'Subtask "Process complex data" failed',
  details: {
    subtaskRequest: {
      type: 'atomic',
      description: 'Process complex data',
      inputs: { /* original inputs */ }
    },
    subtaskError: {
      type: 'TASK_FAILURE',
      reason: 'execution_halted',
      message: 'Failed to process data format'
    },
    nestingDepth: 2,
    partialOutput: "Partial processing results before failure"
  }
}
```

This standardized structure provides several benefits:
1. Complete error context preservation
2. Clear indication of which subtask failed
3. Access to the original subtask request for potential retry
4. Preservation of partial results for recovery

Example of parent task handling subtask failures:
```typescript
try {
  const result = await taskSystem.executeTask(complexTask);
} catch (error) {
  if (error.type === 'TASK_FAILURE' && error.reason === 'subtask_failure') {
    console.log(`Subtask failed: ${error.details.subtaskRequest.description}`);
    
    // Access the original subtask request for potential retry
    const modifiedRequest = {
      ...error.details.subtaskRequest,
      description: `Retry: ${error.details.subtaskRequest.description} with simplified approach`
    };
    
    // Use partial results if available
    if (error.details.partialOutput) {
      console.log(`Using partial output: ${error.details.partialOutput}`);
    }
    
    // Attempt recovery with modified request
    const recoveryResult = await taskSystem.executeTask(modifiedRequest);
  }
}
```

### 3.4 Validation Phase
Recovery validation includes:
- Verifying resource usage of recovery approach
- Ensuring progress is made
- Validating output structure and format
- Limiting recovery depth to prevent infinite loops

## 4. Pattern Examples
See components/task-system/impl/examples.md for concrete examples.

### Context Retrieval Failure Example
```typescript
// Memory System's file index is corrupted or unavailable
const error = {
  type: 'TASK_FAILURE',
  reason: 'context_retrieval_failure',
  message: 'Failed to access memory system index',
  details: { error: 'Index corruption detected' }
};

// Recovery involves alternative context strategy
const recovery = await evaluator.recoverFromContextFailure(error);
```

### Sequential Task Failure Example
```typescript
try {
  const result = await taskSystem.executeTask(
    "process data in multiple steps",
    memorySystem
  );
} catch (error) {
  if (error.type === 'TASK_FAILURE' && error.reason === 'subtask_failure') {
    console.log(`Failed at step ${error.details.failedStep} of ${error.details.totalSteps}`);
    
    // Access partial results from completed steps
    error.details.partialResults.forEach(result => {
      console.log(`Step ${result.stepIndex} output: ${result.output}`);
    });
    
    // Potentially use partial results for recovery
    const recoveryResult = await evaluator.recoverWithPartialResults(error);
  }
}
```

## 5. Known Limitations
- **Recovery Depth**: Limited to prevent infinite loops
- **Partial Result Size**: May be truncated for very large outputs
- **Stateful Recovery**: Not supported across sessions
- **Complex Dependencies**: Recovery may not work for deeply nested failures

## 6. Related Patterns
- [Pattern:ResourceManagement:1.0]
- [Pattern:TaskExecution:2.0]
</file>
<file path="./system/architecture/patterns/resource-warnings.md" project="">
<!-- This file has been consolidated into resource-management.md. Refer to the "Warning Thresholds" subsection there. -->
</file>
<file path="./system/architecture/patterns/tool-interface.md" project="">
# Unified Tool Interface Pattern [Pattern:ToolInterface:1.0]

## Purpose

Define a consistent tool-based delegation model for LLMs that unifies direct operations (file I/O, scripts) and complex operations (subtasks, evaluations) under a common interface while maintaining separate implementation mechanisms.

## Pattern Description

The Unified Tool Interface pattern separates the LLM-facing interface from the underlying implementation mechanisms:

1. **Interface Layer**: What the LLM sees and interacts with
   - Consistent tool-based invocation pattern
   - Standardized parameter schemas
   - Unified error handling

2. **Implementation Layer**: How tools are executed
   - **Direct Tools**: Synchronous execution via Handler
   - **Subtask Tools**: Asynchronous execution via CONTINUATION mechanism

```mermaid
flowchart TD
    A[LLM] -->|Calls| B[Unified Tool Interface]
    B -->|Direct Operations| C[Handler Tools]
    B -->|Complex Operations| D[Subtask Mechanism]
    C -->|Synchronous| E[File Systems/Scripts]
    D -->|Asynchronous| F[Template Selection]
    F -->|CONTINUATION| G[Child LLM Tasks]
```

## Interface vs. Implementation

### What the LLM Sees

From the LLM's perspective, all tools share a common invocation pattern:

```python
# Direct tool example
result = tools.readFile("path/to/file")

# Subtask tool example
result = tools.analyzeData({
  "data": fileContent,
  "method": "statistical" 
})
```

### How It's Implemented

Behind the interface, tools operate differently:

1. **Direct Tools**:
   - Executed synchronously by the Handler
   - No complex context management
   - Predictable resource usage
   - Examples: file operations, script execution, API calls

2. **Subtask Tools**:
   - Implemented via CONTINUATION mechanism
   - Managed by Task System and Evaluator
   - Full context management capabilities
   - Depth tracking and cycle detection
   - Examples: data analysis, creative generation, complex reasoning

## Implementation Details

### Handler Responsibilities

The Handler component exposes both types of tools through a unified registry:

```typescript
interface ToolRegistry {
  registerDirectTool(name: string, handler: Function): void;
  registerSubtaskTool(name: string, templateHints: string[]): void;
  
  // Called by LLM through tool interface
  invokeTool(name: string, params: any): Promise<any>;
}
```

When a tool is invoked:
1. Handler determines tool type (direct or subtask)
2. For direct tools: executes immediately and returns result
3. For subtask tools: creates CONTINUATION with SubtaskRequest and yields

## Subtask Results as Tool Responses

When a subtask tool is called, the system implements a streamlined approach that preserves session continuity:

```mermaid
flowchart TD
    A[Parent LLM] -->|"tools.analyzeData({...})"| B[Handler]
    B -->|CONTINUATION with subtask_request| C[Task System]
    C -->|Execute subtask| D[Subtask LLM]
    D -->|Result| C
    C -->|Add as tool response| B
    B -->|Continue with tool result| A
```

### Key Benefits
1. **Session Preservation**: The parent Handler session is maintained throughout execution
2. **Natural Conversation Flow**: From the LLM's perspective, this is just a tool call and response
3. **No Special Methods**: No need for `resumeTask()` or similar special continuation methods
4. **Simplified Implementation**: Clean component boundaries with clear responsibilities

### Example Flow
1. Parent LLM calls a subtask tool (e.g., `tools.analyzeData({...})`)
2. Handler recognizes this as a subtask tool and returns CONTINUATION with subtask_request
3. Task System executes the subtask as a separate LLM interaction
4. Task System adds the subtask result to the parent's session as a tool response
5. Parent LLM continues execution with the tool result in its conversation history

This approach eliminates the need for the LLM to understand continuation concepts - it simply sees its tool call and the corresponding response.

### Task System Integration

For subtask tools, the Task System:
1. Receives CONTINUATION status with subtask_request
2. Selects appropriate template using associative matching
3. Executes subtask with context management
4. Returns result to parent task

## Usage Examples

### Direct Tool Example

```typescript
// LLM invocation
const fileContent = tools.readFile("data.csv");

// Implementation (synchronous via Handler)
async function readFile(path: string): Promise<string> {
  return await fs.readFile(path, 'utf8');
}
```

### Subtask Tool Example

```typescript
// LLM invocation
const analysis = tools.analyzeData({
  "data": fileContent,
  "method": "statistical"
});

// Implementation (asynchronous via CONTINUATION)
async function analyzeData(params: any): Promise<any> {
  return {
    status: "CONTINUATION",
    notes: {
      subtask_request: {
        type: "atomic",
        description: `Analyze data using ${params.method} method`,
        inputs: { data: params.data, method: params.method },
        template_hints: ["data_analysis", "statistics"]
      }
    }
  };
}
```

## Selection Criteria

Use **Direct Tools** when:
- Operations are deterministic with predictable outputs
- No complex reasoning is required
- Response format is simple and structured
- Resource usage is predictable

Use **Subtask Tools** when:
- Complex reasoning or creativity is required
- Dynamic template selection would be beneficial
- Operation might involve multiple steps or iterations
- Context management needs are sophisticated

## Integration with Other Patterns

This pattern complements:
- [Pattern:DirectorEvaluator:1.1] - Can use both mechanism types
- [Pattern:SubtaskSpawning:1.0] - Provides tool interface for spawning
- [Pattern:ContextFrame:1.0] - Works with context management model
- [Pattern:Error:1.0] - Unified error handling across mechanisms

## Constraints

1. **Naming Consistency**: Tool names must be unique across both direct and subtask tools
2. **Parameter Validation**: All tools must validate parameters regardless of implementation
3. **Error Propagation**: Error handling must be consistent across both mechanisms
4. **Resource Tracking**: Both mechanisms must track resource usage appropriately

### User Input Request Tool

The system provides a standardized tool for requesting user input:

```typescript
// Standard tool for requesting user input
const USER_INPUT_TOOL: ToolDefinition = {
  name: "requestUserInput",
  description: "Request input from the user when additional information is needed",
  parameters: {
    type: "object",
    properties: {
      prompt: {
        type: "string",
        description: "The question or prompt to show to the user"
      }
    },
    required: ["prompt"]
  }
};

// LLM usage
const userAnswer = tools.requestUserInput({
  prompt: "What file would you like to analyze?"
});

// Handler implementation
class Handler implements IHandler {
  constructor(config: HandlerConfig) {
    // Register standard tools
    this.registerDirectTool(USER_INPUT_TOOL.name, this.handleUserInputRequest.bind(this));
  }
  
  private async handleUserInputRequest(params: {prompt: string}): Promise<{userInput: string}> {
    if (!this.onRequestInput) {
      throw new Error("No input request handler registered");
    }
    
    const userInput = await this.onRequestInput(params.prompt);
    this.session.addUserMessage(userInput);
    
    return { userInput };
  }
}
```

This standardized approach allows LLMs to consistently request user input across different providers while maintaining proper conversation tracking and resource management.

## Implementation Guidance

1. Register direct tools during Handler initialization
2. Define subtask tools in the TaskLibrary with template hints
3. Implement invocation routing in the Handler
4. Ensure consistent error handling in both paths
5. Provide clear documentation of available tools to the LLM
</file>
<file path="./system/architecture/patterns/resource-management.md" project="">
# Resource Management Pattern [Pattern:ResourceManagement:1.0]

## Purpose & Constraints
This document defines the resource management strategy for the task execution system. It covers:
 - Memory Hierarchy
 - Resource Tracking (turn counts, context window, token usage)
 - Warning Thresholds (e.g. 80% limits)
 - Cleanup Procedures

**Note:** Warning signals are purely informative; hard limits trigger termination.

## Core Components

### Memory Hierarchy
1. **Long‑term Memory:** Stores data and procedures; accessed via the Memory System (read‑only during task execution).
2. **Working Memory:** The active computation space (task‑specific context) managed through Environment objects; cleared after task completion.
3. **Context Frames:** Capture complete execution environments (bindings and working memory) using a minimal‑context extension pattern.

### Resource Tracking

The Handler is responsible for:
 - Tracking turn counts and enforcing limits.
 - Monitoring context window usage (tokens) with warning thresholds (e.g. 80%) and hard limits.
 - Reporting resource metrics for error handling.

### Resource Tracking

#### Handler Responsibility
- One Handler per task execution
- Creates a HandlerSession to manage conversation state
- Tracks resource usage through session:
  * Turn counts (incremented for assistant messages)
  * Message history (all user and assistant messages)
  * Context window size (including all conversation history)
  * Token usage and peak usage statistics
- Enforces resource limits at session level
- Manages clean termination
- Structures complete interaction payload via HandlerPayload

#### Session Management
- Maintains complete conversation history
- Tracks message timestamps and roles
- Manages turn counting (assistant messages only)
- Constructs provider-agnostic payloads
- Provides resource metrics for monitoring
- Enforces limits based on configuration

### Context Management

#### Window Management
- Token-based calculation
- Explicit size monitoring
- Fraction-based limits
- No content optimization
- Warning thresholds at 80%
- Hard limits with error handling

#### Context Preservation

- The nested Environment model ensures that each child task gets its own context.
- Proper chaining of environments prevents context leakage.
- InheritedContext and any built-in objects (like TaskLibrary) persist unchanged in the chain.
- Context frames capture environments
- Minimal required context per task
- Associative memory mediation
- Clean extension mechanism

## Resource Configuration

### Default Configuration
- Base resource limits
- Warning thresholds
- Default turn counts
- Context window limits

### Per-Task Overrides
- Task-specific limits
- Custom thresholds
- Resource constraints
- Performance targets

## Script Execution Resource Management

- Script tasks (type="script") are executed by the Handler.
- The Handler captures stdout, stderr, and exitCode for script tasks.
- A non-zero exitCode is treated as a TASK_FAILURE error.

Example interface:
```typescript
interface ScriptTaskResult {
    stdout: string;
    stderr: string;
    exitCode: number;
}
```

## Interactions

### With Memory System [Component:Memory:3.0]
- Maintains global file metadata index
- Provides associative matching services
- Manages read-only context retrieval
- No file content operations (delegated to Handler)

### With Task System [Component:TaskSystem:1.0]
- Creates/manages Handlers
- Enforces resource limits
- Manages task execution
- Handles resource errors

### With Handler [Component:Handler:1.0]
- Tracks resource usage
- Enforces limits
- Manages session lifecycle
- Reports resource metrics

## Implementation Requirements

### Resource Tracking
```typescript
// Resource metrics definition moved to spec/types.md
// See [Type:ResourceMetrics:1.0] for the complete interface
```

### Handler Configuration
```typescript
interface HandlerConfig {
    maxTurns: number;
    maxContextWindowFraction: number;
    warningThreshold: number;
    defaultModel?: string;
    systemPrompt: string;
}
```

## Error Handling

### Resource Exhaustion
- Immediate task termination
- Clean resource release
- Error surfacing with metrics
- No automatic retry

### Context Overflow
- Token limit enforcement
- Warning at threshold
- Clean termination
- Context metrics reported

### Progress Failure
- Resource accounting completion
- State cleanup
- Error propagation
- Recovery guidance

### Sequential Task Resources

For **sequential** tasks, the Evaluator maintains a step-by-step record of partial results and history, often called `SequentialHistory`. This history is **not** tracked against the Handler's context window limits. Instead:

- **Evaluator** owns the entire sequential history for multi-step tasks.
- **Handler** continues to track standard resource usage (turns, tokens).
- Because the sequential history is purely textual or metadata that the Evaluator stores separately, it does not consume the Handler's context window.
- If the task is configured to accumulate data (`<accumulate_data>true</accumulate_data>`), the Evaluator may pass prior step outputs into `MemorySystem.getRelevantContextFor()`.
- The Evaluator maintains a SequentialHistory for sequential (type="sequential") tasks.
- The SequentialHistory (holding step outputs and metadata) is not counted against the Handler’s context window limits.
- Accumulated outputs are retained for the entire sequential task execution loop and then discarded (or archived) once the sequence completes.

Example interface:
```typescript
interface SequentialHistory {
    outputs: TaskOutput[];
    metadata: { startTime: Date; currentStep: number; resourceUsage: ResourceMetrics; };
}
interface TaskOutput {
    stepId: string;
    output: string;
    notes: string;
    timestamp: Date;
}
```
- This separation ensures Handler resource metrics stay consistent and the Evaluator can keep any relevant partial outputs for as long as needed.
- Once the sequential task completes (success or failure), the Evaluator discards (or archives) the sequential history to free memory.

This design ensures a clean boundary between higher-level multi-step results (owned by the Evaluator) and resource usage constraints (handled by the Handler).

## Related Patterns
- [Pattern:Error:1.0] - Error handling strategy
- [Pattern:TaskExecution:1.0] - Task execution flow
</file>
<file path="./system/architecture/qa/resource-management.md" project="">
# Resource Management Q&A

## Memory Types & Organization

Q: What types of memory need to be managed?
A: The system has a hierarchical memory design:
- Long-term memory for data and procedures
- Working memory for active computations
- Context frames that capture execution environments (including working memory)
- Variable bindings within environments

Q: How is context organized and managed?
A: Through Environment objects that combine:
- Variable bindings (local scope)
- Working memory context
- Context frames are passed and extended through task execution

Q: How is minimal required context determined?
A: Context selection is handled at the LLM + prompt level, not by the system architecture. This allows for more intelligent and flexible context selection based on task requirements.

## Resource Ownership & Isolation

Q: Who owns and tracks resources?
A: Per existing documentation:
- Handler-based resource tracking
- One Handler per task execution
- No cross-Handler resource pooling
- Per-session resource isolation

Q: How are resources allocated and released?
A: As documented:
- Clean resource release on completion
- Turn limit passed during Handler initialization
- Default resource limits from config
- Per-task limit overrides possible

Q: How are memory operation failures handled?
A: Through a simple retry mechanism:
- Failed operations should be retried once
- If retry fails, an informative error is surfaced
- No complex recovery mechanisms in MVP

## Context Window Management

Q: How is the context window managed?
A: Through explicit tracking:
- Token-based calculation
- Window size monitoring
- Fraction-based limits
- No content optimization
- Warning at 80% threshold
- Error at limit reached

Q: How does context preservation work between tasks?
A: Through the Environment system:
- Context frames capture complete execution environments
- Each task receives minimal required context
- Associative memory system mediates between long-term and working memory
- Environment.extend() creates new contexts with additional bindings
- No context merging in MVP - extension patterns only

## Resource Metrics & Limits

Q: What resource metrics are tracked?
A: Currently documented:
- Turn counts
- Context window size
- Token usage
- Peak usage statistics

Q: How are resource limits handled?
A: As specified:
- Default limits from config
- Per-task overrides possible
- Warning thresholds (80%) are purely informative
- Hard limits with error handling
- Clean termination on limit violation
</file>
<file path="./system/architecture/decisions/completed/014-operator-ctx-config.md" project="">
# ADR 14: Operator Context Configuration [Summary]

## Status
Completed - Full version moved to completed_plans directory

## Context
Operators needed a standardized way to configure context management with sensible defaults for different operator types.

## Decision
Implemented a hybrid configuration approach where:
- Each operator type has specific default settings
- Explicit settings in `context_management` blocks override defaults
- Sequential tasks default to `notes_only` accumulation format
- Context inheritance and accumulation behaviors are configurable

## Consequences
- Reduced boilerplate in task definitions
- Improved memory efficiency with appropriate defaults
- Maintained flexibility through explicit overrides
- Standardized context management across operator types
</file>
<file path="./system/architecture/decisions/completed/011-subtask-spawning.md" project="">
# ADR 11: Subtask Spawning Mechanism (Summary)

## Status
Accepted

## Context
The system needed a standardized mechanism for dynamic task creation and composition. Previous implementations of continuation-based task spawning were inconsistent across components, with varying approaches to context management, parameter passing, and error handling.

## Decision
Implement a standardized subtask spawning mechanism with the following key features:

1. **Standardized Request Structure**: A well-defined `SubtaskRequest` interface with required fields (type, description, inputs) and optional fields (template_hints, context_management, max_depth, subtype).

2. **Direct Parameter Passing**: All data flow uses direct parameter passing rather than environment variables, ensuring clear dependencies and improved testability.

3. **Context Management Integration**: Default settings with optional overrides following the hybrid approach from ADR 14:
   - `inherit_context: subset` (default)
   - `accumulate_data: false` (default)
   - `accumulation_format: notes_only` (default)
   - `fresh_context: enabled` (default)

4. **Depth Control**: Maximum nesting depth (default: 5) and cycle detection to prevent infinite recursion.

5. **Error Handling**: Standardized error structure that preserves the complete error context, allowing for potential recovery strategies.

6. **Resource Protection**: Resource tracking across the entire subtask chain to prevent exhaustion.

## Consequences
- Consistent subtask spawning behavior across all components
- Clear data flow with explicit dependencies
- Controlled resource usage with protection against infinite recursion
- Improved error handling with context preservation
- Flexible context management with sensible defaults

The full ADR has been moved to ../../../../completed_plans/ for space efficiency.
</file>
<file path="./system/architecture/decisions/completed/009-partial-results.md" project="">
# ADR 9: Partial Results Policy (Summary)

## Status
Accepted and Implemented

## Context
When multi-step operations fail partway through execution, the system needs a standardized approach for preserving and exposing partial results. This is particularly important for sequential and reduce tasks where significant work may have been completed before failure.

## Decision
We will implement a standardized approach for preserving partial results across different task types:

1. **Atomic Tasks**: Store partial content in `notes.partialOutput`
2. **Sequential Tasks**: Store step-by-step outputs in `details.partialResults` with metadata
3. **Reduce Tasks**: Store processed input results and current accumulator state

The format of preserved partial results will be controlled by the task's `accumulation_format` setting:
- `notes_only`: Only summary information (default for memory efficiency)
- `full_output`: Complete outputs (with size limits)

## Consequences
- Improved debugging and recovery capabilities
- Consistent error structures across task types
- Memory usage management through format control
- Ability to resume or recover from partial execution

## Implementation Notes
- All partial results storage is ephemeral (in-memory only)
- Size limits prevent memory issues with large partial outputs
- Recovery is limited to simple retries in the MVP
- Error structures are consistent with the error taxonomy in ADR 8

## Related Documents
- Full ADR: ../../../../completed_plans/adr9-partial-results.md
- Error Handling: ../patterns/errors.md
</file>
<file path="./system/architecture/decisions/completed/010-evaluator-director.md" project="">
# ADR 10: Evaluator-to-Director Feedback Flow [Summary]

## Status
Accepted

## Context
The system needed a standardized approach for feedback flow between Director and Evaluator components, replacing ad-hoc environment variable usage with direct parameter passing.

## Decision
1. Replace environment variable references (e.g., `last_evaluator_output`) with direct parameter passing
2. Add a dedicated `director_evaluator_loop` task type
3. Update XML schema and type definitions
4. Standardize the Director-Evaluator pattern with both dynamic and static variants

## Consequences
### Positive
- Clearer data flow between components
- Improved testability with explicit dependencies
- Better support for iterative refinement processes
- Consistent XML schema for both variants

### Negative
- Required updates to existing documentation and code
- Slightly more verbose XML structure

## Implementation
The implementation included:
- Adding `director_evaluator_loop` task type to TaskType enum
- Creating DirectorEvaluatorLoopTask interface
- Standardizing EvaluationResult structure
- Updating XML schema with new elements
- Removing environment variable references

See the full ADR for complete implementation details.
</file>
<file path="./system/architecture/decisions/completed/013-json-output.md" project="">
# ADR 13: JSON-based Output Standardization (Summary)

## Status
Accepted and Implemented

## Context
The system needed a standardized approach for handling structured outputs from tasks. Previously, tasks returned outputs in arbitrary text formats, leading to type ambiguity, structure inconsistency, parsing burden, error-prone data exchange, and poor composability.

## Decision
We implemented a JSON-based output standardization mechanism with the following key features:

1. Added an `<output_format>` XML element to specify structured output requirements:
   ```xml
   <output_format type="json" schema="string[]" />
   ```

2. Added a `returns` attribute to function templates for type validation:
   ```xml
   <template name="get_file_info" params="filepath" returns="object">
   ```

3. Enhanced TaskResult interface with a `parsedContent` property to store parsed JSON data.

4. Added output format validation with specific error handling for format violations.

5. Implemented automatic JSON detection and parsing for tasks with `type="json"`.

## Consequences
- Tasks can now specify structured output formats with validation
- Function templates can declare return types
- Consumers can access parsed JSON data directly via the TaskResult interface
- Type validation ensures outputs match specified schemas
- Error handling provides clear feedback for format violations

## Implementation
The implementation includes:
- XML schema updates for `<output_format>` and `returns` attributes
- TaskResult interface enhancements
- parseTaskOutput method in TaskSystem interface
- Output validation logic
- Error handling for format violations

## References
Full ADR: ../../../../completed_plans/013-json-output.md
</file>
<file path="./system/architecture/decisions/completed/012-function-based-templates.md" project="">
# ADR 12: Function-Based Template Model

## Status
Accepted

## Context
The system needed a cleaner approach to variable scoping and parameter passing between tasks. The previous model allowed implicit access to parent environment variables through `{{variable_name}}` syntax, which created unclear dependencies and made reasoning about scope difficult.

## Decision
Implement a function-based template model with explicit parameter declarations:

1. **Template Definition**
   - Templates explicitly declare parameters using a `params` attribute
   - Each template has its own lexical scope containing only its parameters
   - Templates are registered in a central TaskLibrary

2. **Function Calling**
   - Function calls use positional arguments evaluated in the caller's context
   - Arguments can be literals, variable references, or nested expressions
   - A new environment is created for each function call with bindings for parameters
   - No implicit access to the caller's environment is allowed

3. **XML Representation**
   ```xml
   <!-- Template definition -->
   <template name="analyze_data" params="dataset,config">
     <task>
       <description>Analyze {{dataset}} using {{config}}</description>
     </task>
   </template>

   <!-- Function call -->
   <call template="analyze_data">
     <arg>weather_data</arg>
     <arg>standard_config</arg>
   </call>
   ```

## Consequences
- **Positive**
  - Clear data dependencies between components
  - Improved reasoning about variable scope
  - Better encapsulation of implementation details
  - Foundation for more advanced functional patterns
  - Explicit parameter documentation

- **Negative**
  - Slightly more verbose syntax
  - Migration effort for existing templates
  - Additional parsing complexity

## Implementation
The implementation includes:
- New AST node types: TemplateNode, FunctionCallNode, ArgumentNode
- Environment extension mechanism for creating child scopes
- Parameter binding during function calls
- Argument resolution strategy for variable references vs. literals

The full ADR with implementation details has been moved outside the docs/ tree to save space.
</file>
<file path="./system/architecture/decisions/15-notes.md" project="">
# ADR 15: Notes Field Standardization

## Status
Proposed

## Context
The documentation inconsistently uses "notes" and "summary information" when referring to task outputs, creating ambiguity about:
- What's generated by tasks
- What's preserved during accumulation
- Component responsibilities

Example inconsistencies:
```typescript
// In TaskResult
notes: {
    dataUsage: string;
    successScore?: number;
    partialOutput?: string;
    [key: string]: any;
};

// In SequentialHistory.TaskOutput
interface TaskOutput {
    stepId: string;
    output: string;  // The main content
    notes: any;      // Described as "Additional metadata or partial information"
    timestamp: Date;
}
```

Documentation describing `accumulation_format` options inconsistently refers to "summary information" vs "notes":
```
- `notes_only`: Only summary information is preserved (default)
- `full_output`: Complete output (with reasonable size limits)
```

## Decision

1. **Terminology Standardization**
   - Use "notes" consistently throughout documentation
   - Eliminate ambiguous term "summary information" entirely

2. **Component Responsibilities**
   - LLM/Task: Generates notes content during task execution
   - Evaluator: Stores and manages notes without modification
   - No separate summary generation mechanism needed

3. **Accumulation Format Behavior**
   ```typescript
   // Clarified behavior:
   enum AccumulationFormat {
     NOTES_ONLY,  // Preserves only the complete notes field
     FULL_OUTPUT  // Preserves both content and notes fields
   }
   ```

4. **Sequential History Management**
   ```typescript
   // When accumulation_format is "notes_only":
   sequentialHistory.push({
     stepId: generateStepId(),
     output: null,  // Not preserved
     notes: taskResult.notes,  // Preserved intact
     timestamp: new Date()
   });
   
   // When accumulation_format is "full_output":
   sequentialHistory.push({
     stepId: generateStepId(),
     output: taskResult.content,  // Preserved
     notes: taskResult.notes,     // Preserved
     timestamp: new Date()
   });
   ```

## Consequences

### Positive
- Simplified Evaluator implementation (no summary generation needed)
- Clearer component responsibilities
- Consistent terminology across documentation
- Reduced ambiguity in implementation

### Negative
- Requires documentation updates across multiple components
- Potential confusion for developers familiar with previous terminology

## Implementation

```mermaid
flowchart TD
    A[Task Execution] --> B[TaskResult]
    B --> C{accumulation_format?}
    C -->|notes_only| D[Store notes field only]
    C -->|full_output| E[Store both content and notes]
    D --> F[Sequential History]
    E --> F
    F --> G[Next Task Input]
```

Required documentation updates:
1. `components/task-system/spec/types.md`: Update TaskResult and TaskOutput interfaces
2. `components/task-system/spec/behaviors.md`: Clarify accumulation_format behavior
3. `components/task-system/impl/*.md`: Update implementation examples
4. `system/architecture/patterns/*.md`: Update pattern descriptions 
5. `system/contracts/*.md`: Update contract specifications

Type definition updates:
```typescript
// Updated accumulation_format documentation
/**
 * Controls what information is preserved during sequential task execution
 * - notes_only: Only the notes field is preserved (default)
 * - full_output: Both content and notes fields are preserved
 */
type AccumulationFormat = 'notes_only' | 'full_output';
```
</file>
<file path="./system/architecture/decisions/removed.md" project="">
ADRs 002 and 005 were removed to save space, can be provided if needed
</file>
<file path="./system/architecture/decisions/specs/memory.md" project="">
# Memory System 3.0 Consolidation Specification
> Ingest the information from this file, implement the Low-Level Tasks, and generate the code that will satisfy the High and Mid-Level Objectives.

## High-Level Objective

- Standardize Memory System references to version 3.0 throughout the codebase and documentation

## Mid-Level Objectives

- Update all documentation to consistently reference Memory System 3.0
- Ensure clear definition of interface boundaries between Memory System and Handler tools
- Standardize type definitions across all components
- Remove or update references to deprecated methods (particularly `updateContext`)

## Implementation Notes
- Focus on documentation updates rather than code changes
- Metadata format should remain unspecified to provide implementation flexibility
- Take a light approach to validation, focusing on basic type checking
- Maintain the read-only context model established in ADR-003
- Ensure clear separation between metadata management (Memory System) and file operations (Handler tools)

## Context

### Beginning context
- Documentation contains inconsistent references to Memory System 2.0 and 3.0
- Some documents reference deprecated methods like `updateContext` and `getContext`
- Unclear boundaries between Memory System and Handler responsibilities in some documents
- Inconsistent definitions of core types across components

### Ending context  
- All documentation consistently references Memory System 3.0
- Clear boundary definition between Memory System (metadata) and Handler (file operations)
- Standardized type definitions across all components
- No references to deprecated methods
- Updated architecture documentation reflecting the read-only context model

## Low-Level Tasks
> Ordered from start to finish

1. Create consolidated Memory System 3.0 reference document
```aider
Create a new reference document components/memory/REFERENCE.md that:
- Defines the canonical Memory System 3.0 interface
- Lists all standard type definitions (GlobalIndex, FileMetadata, FileMatch, etc.)
- Clearly states the read-only context model
- Defines the responsibility boundary between Memory System and Handler tools
- Documents the removal of updateContext capability
- Serves as the single source of truth for interface definitions
```

2. Update Task System documentation with consistent references
```aider
Update the following Task System documents to reference Memory System 3.0 consistently:
- components/task-system/spec/interfaces.md
- components/task-system/spec/behaviors.md
- components/task-system/impl/design.md

Remove any references to:
- updateContext method
- getContext method
- Memory System 2.0
- Any context persistence capabilities

Update type imports and interface references to match the canonical definitions.
```

3. Update architecture documentation with consistent references
```aider
Update the following architecture documents:
- system/architecture/overview.md
- system/architecture/decisions/001-memory-system-qa.md
- system/architecture/decisions/needs_update/001-memory-system.md

Ensure all references use Memory System 3.0 terminology.
Clarify that Memory System maintains a read-only approach to context.
Update any diagrams or flowcharts to reflect current architecture.
```

4. Update system contracts with consistent references
```aider
Update the following system contract documents:
- system/contracts/interfaces.md
- system/contracts/resources.md

Ensure [Contract:Integration:TaskMemory:3.0] is consistently referenced.
Verify interface definitions match the canonical reference.
Update any examples to reflect current architecture.
```

5. Create migration guidance document
```aider
Create a new document system/architecture/migrations/memory-system-2-to-3.md that:
- Summarizes changes from Memory System 2.0 to 3.0
- Provides guidance for updating code that used updateContext
- Explains the new read-only context model
- References ADR-003 for the architectural decision rationale
- Includes examples of proper Memory System 3.0 usage
```
</file>
<file path="./system/architecture/decisions/16-output-structure-simplification.md" project="">
# ADR 16: Output Structure Simplification

## Status
Accepted

## Context
The current system has multiple fields for task output:
- `content`: Primary output when task completes
- `partialOutput`/`incompleteContent`: Output from incomplete atomic tasks
- `notes`: Task metadata
- `partialResults`: Results from sequential task steps

This creates unnecessary complexity and ambiguity.

## Decision
We will simplify the output structure:

1. **Atomic Tasks**: Use only `content` and `notes` fields
   - `content`: Contains all output (complete or partial)
   - `notes`: Contains only metadata, never content
   - Task status indicates whether content is complete
   
2. **Sequential/Reduce Tasks**: Maintain existing structure with proper separation
   - Each step/input result contains content and metadata
   - Clear separation between task-level and step-level concerns

## Consequences

### Positive
- Simpler interface with fewer fields
- Clear separation between content and metadata
- Status code provides necessary completion information
- Consistent approach across task types
- Easier error handling and recovery

### Negative
- Requires updates to existing code checking for partialOutput
- Slightly less explicit about content completeness

## Implementation
For atomic tasks, the completion status is determined by the `status` field:
- `COMPLETE`: Content is final and complete
- `FAILED`: Content may be partial or incomplete
- `CONTINUATION`: Content is intermediate

This approach maintains the essential information while simplifying the structure.
</file>
<file path="./system/architecture/decisions/001-memory-system-qa.md" project="">
# Memory System Architecture Q&A

## Related Documents
- Memory component specification in [Component:Memory:3.0]
- Handler interface in [Interface:Handler:ResourceMonitoring:1.0]
- Memory Task Example in components/task-system/impl/examples.md
- Context Frame Pattern in [Pattern:ContextFrame:1.0]
- Resource Management Pattern in [Pattern:ResourceManagement:1.0]

## Memory and Context Concepts

Q: What exactly constitutes working memory?
A: The Memory System only manages two types of data:
1. Short-term task data context generated by associative matching
2. Global file metadata index for bootstrapping associative matching

The Handler manages all other task-related data including chat history and prompt handling.

Q: How is "context" different from "working memory"?
A: "Context" specifically refers to data context, while working memory is broader and includes chat history, system prompts, and templates. See [Pattern:ContextFrame:1.0] for details.

Q: What exactly does associative matching return?
A: Associative matching has two specific return values:
1. An unstructured data context section
2. A list of tuples containing absolute file paths and optional index strings for those files

## Component Responsibilities 

Q: Should the memory system track context window size?
A: No, this is delegated to the Handler (see [Interface:Handler:ResourceMonitoring:1.0]). The memory system stores content but doesn't track usage limits.

Q: Is the memory system responsible for managing system prompts and templates?
A: No, the Memory System does not handle prompts or templates at all. These are managed by the Task System and Handler.

Q: Who handles chat history?
A: The Handler manages all chat history and interaction state. The Memory System does not store any chat history.

Q: Who handles file content access?
A: File content access is handled by the LLM using Handler tools, not by the Memory System. The Memory System only provides the file paths and metadata needed to identify relevant files.

## Global Index

Q: What is the format of file metadata in the global index?
A: The metadata is simply stored as strings, with file paths as keys in a map structure. There is no predefined structure for the metadata content.

Q: What operations are supported on the global index?
A: The global index only supports two operations:
1. Getting the complete index
2. Bulk updates to the index
Individual updates, filtering, and querying are intentionally not supported to maintain simplicity.

## Context Management

Q: How can tasks get their data context?
A: Two ways (see operator specifications in components/task-system/spec/operators.md):
1. Inherit from parent/predecessor task
2. Generate fresh via associative matching using the global file metadata index as bootstrap. The associative matching task returns both context and a list of potentially relevant files, which can then be accessed using Handler tools.

Q: How does associative matching work when there's too much content?
A: When the full contents of all long-term storage files don't fit in the context window (times some factor), the associative matching task uses the existing global index to bootstrap the matching process. The input to the associative matching task includes all file paths in long-term storage.

Q: Should context inheritance vs. generation be a template parameter?
A: No, it should be part of the operator XML syntax in task library entries (see [Contract:Tasks:TemplateSchema:1.0]). It's not a substitutable parameter like task inputs.

Q: Can prior task output influence context generation?
A: Yes, the notes section of prior task output can be passed to associative matching for child/successor tasks. See Memory Task Example in components/task-system/impl/examples.md.

## Interface Design

Q: How should context source be controlled in the XML?
A: Through an optional inherit_context attribute on task elements, with defaults that can vary by operator type. See operator specifications in components/task-system/spec/operators.md.

## Implementation Questions

Q: Do operator defaults need to be standardized?
A: Not immediately. Different operators can have different context inheritance defaults based on their use cases. See operator specifications.

Q: How should context be handled in map/reduce operations?
A: The inherit_context attribute can be part of inner_task and reduction_task elements, following the same pattern as other operators. See operator specifications.

## Future Considerations

Q: When would file pre-extraction be considered?
A: File pre-extraction and concatenation is explicitly deferred for later consideration. The current design relies solely on tool-based file access via the Handler. Any performance optimizations through pre-extraction would only be considered after evaluating the effectiveness of the tool-based approach.
</file>
<file path="./system/architecture/decisions/15-notes-field-standardization.md" project="">
# ADR 15: Notes Field Standardization

## Status
Accepted

## Context
The task system currently has an artificial distinction between `notes` and `partialOutput` fields in the TaskResult interface. This creates unnecessary complexity and inconsistency across the codebase:

1. Some documents state partial results are not preserved, while others describe preservation mechanisms
2. Sequential task results have separate `output` and `notes` fields for each step
3. The `accumulation_format` currently has values `notes_only` and `full_output` which are not intuitive
4. Error handling structures have inconsistent approaches to storing partial results

## Decision
We will simplify the task output structure by eliminating the artificial distinction between `notes` and `partialOutput` fields:

1. Use `notes` as the universal container for all task metadata, including partial results
2. Modify accumulation format values from `notes_only`/`full_output` to `minimal`/`full`
3. Update all error handling structures to store partial results directly in the notes field
4. Standardize terminology around `notes` across all documentation

### Simplified TaskResult Interface
```typescript
export interface TaskResult {
    content: string;
    status: ReturnStatus;
    criteria?: string;
    parsedContent?: any;
    notes: {
        dataUsage?: string;
        successScore?: number;
        // Partial results are directly included in notes
        [key: string]: any;
    };
}
```

### Simplified TaskOutput Interface
```typescript
export interface TaskOutput {
    stepId: string;  // or step index
    notes: any;      // Task notes field (always preserved, may include partial results)
    timestamp: Date;
}
```

### Simplified Accumulation Format
```typescript
// Simplified behavior:
enum AccumulationFormat {
    MINIMAL,  // Preserves only success/failure status and critical metadata
    FULL      // Preserves all notes content including partial results
}
```

## Consequences
### Positive
- Reduced complexity in the task output structure
- More intuitive naming for accumulation formats
- Consistent approach to partial results across the system
- Simplified error handling structures
- Better alignment with how LLMs naturally structure output

### Negative
- Requires updates to existing documentation and code
- May require migration for existing implementations

## Implementation
1. Update TaskResult interface to remove partialOutput field
2. Update TaskOutput interface to remove output field
3. Update accumulation format values in XML schema
4. Update operator default settings to use minimal/full values
5. Update error handling documentation to reflect the simplified approach
6. Ensure backward compatibility where possible

## Related Documents
- [Pattern:Error:1.0]
- [Contract:Tasks:TemplateSchema:1.0]
- [Component:TaskSystem:1.0]
</file>
<file path="./system/architecture/decisions/006-context-types.md" project="">
<!-- This file is currently under construction.
It will cover decisions on the precise data types for context representation.
For now, refer to the context management ADRs (002 and 005).
-->
</file>
<file path="./system/architecture/questions.md" project="">
# Open Architecture Questions

This document now lists only *unresolved* questions. Many items previously listed have been answered in the ADRs or clarified in the patterns. For the MVP, the matching logic is uniform for atomic tasks, and no operator‑specific matching or versioning considerations are applied.

## Unresolved Questions

1. **Context Generation Failures:** When associative matching fails, should partial outputs be preserved for re‑try? (See ADRs 002 and 005 for related discussion.)

2. **Subtask Spawning Mechanism:** How exactly should subtasks be created dynamically (beyond the Director‑Evaluator pattern)? This remains an open design area.

For additional background, see also [decisions/005-context-handling.md](decisions/005-context-handling.md) and [questions.md](questions.md) in previous revisions.
</file>
<file path="./components/evaluator/README.md" project="">
# Evaluator Component

## Overview

The **Evaluator** is the unified task-execution component of the system. It is responsible for:

1. **Controlling AST processing and execution**  
2. **Managing failure recovery** via standard task return statuses (`COMPLETE`, `CONTINUATION`, `FAILED`)
3. **Tracking resource usage** (in coordination with Handlers)
4. **Handling reparse/decomposition requests** when tasks fail (e.g. due to resource exhaustion or invalid output)

### References in Existing Documentation

- **System-Level References**  
  - *System README (`system/README.md`)* and *Architecture Overview (`system/architecture/overview.md`)* list the Evaluator as a core component, describing it as the manager for AST execution, resource tracking, and reparse/error handling.  
  - *Contracts & Interfaces (`system/contracts/interfaces.md`)* references "[Contract:Integration:EvaluatorTask:1.0]," tying the Evaluator to tasks and describing the need for an integration interface (though that interface is not yet fully elaborated).  
  - The *"Metacircular Evaluator"* concept is mentioned in `misc/textonly.tex.md`, demonstrating an evaluator that calls LLM operations as part of `apply-proc` and `eval-task`. This underscores that the Evaluator runs tasks by leveraging LLM-based primitives (for decomposition, atomic calls, or re-checking).  

- **Error Handling**  
  - The Evaluator is mentioned repeatedly (e.g., `misc/errorspec.md`, `system/architecture/patterns/errors.md`) as the component receiving error signals from tasks or sub-operations. It manages or coordinates the "control flow" when resource exhaustion or invalid outputs appear.  
  - Errors of type `RESOURCE_EXHAUSTION` or `TASK_FAILURE` can cause the Evaluator to request "reparse" or "decomposition."  

- **Implementation Plan**  
  - *Phase 2: Expanded Context Management* mentions extending environment usage so that sub-tasks may inherit or manage context. The Evaluator is implicitly involved in ensuring tasks have the right environment or partial results.  
  - *Phase 3: Task Execution Enhancements* explicitly names the Evaluator as a place to add "summary output or additional logging" for advanced debugging. The same phase also suggests new flags like `rebuild_memory` or `clear_memory` that the Evaluator would honor when building or discarding context.  

In many existing code examples (both TypeScript-like and Scheme-like), the system calls an `eval` or `apply` function that effectively belongs to the Evaluator domain. When direct execution fails, a decomposition or reparse step is triggered, also under the Evaluator's responsibility.

## 3.1 Lexical Environment Model

The Environment class implements lexical scoping for DSL variables through nested environments. This is strictly for variable binding and lookup - completely separate from template matching or context management:

- Maintains variable bindings at each scope level via `bindings` map
- Supports variable lookup through parent scopes via `outer` reference
- Creates child scopes with additional bindings via `extend` method
- Resolves variables through lexical chain with `find` method

```typescript
// Example Environment implementation
class Env implements Environment {
    constructor(public bindings: Record<string, any> = {}, public outer?: Environment) {}
    find(varName: string): any {
        return (varName in this.bindings)
            ? this.bindings[varName]
            : this.outer ? this.outer.find(varName) : throw new Error(`Variable ${varName} not found`);
    }
    extend(bindings: Record<string, any>): Environment {
        return new Env(bindings, this);
    }
}
```

## Nested Environment Model for Function Templates

Function calls create new environments with parameter bindings:

```typescript
// Function call evaluation
function evaluateFunctionCall(call: FunctionCallNode, env: Environment): Promise<any> {
  // 1. Lookup the template in the TaskLibrary
  const template = env.find("taskLibrary").get(call.templateName);
  
  // 2. Evaluate all arguments in the caller's environment
  const argValues = await Promise.all(
    call.arguments.map(arg => evaluateArgument(arg, env))
  );
  
  // 3. Create a new environment with parameter bindings
  const funcEnv = env.extend({});
  for (let i = 0; i < template.parameters.length; i++) {
    funcEnv.bindings[template.parameters[i]] = argValues[i];
  }
  
  // 4. Evaluate the template body in the new environment
  return evaluateTask(template.body, funcEnv);
}
```

This ensures proper variable scoping where templates can only access their explicitly declared parameters, not the caller's entire environment.

## Responsibilities and Role

1. **AST Execution Controller**  

## Context and Template Matching

## Template Substitution

The Evaluator is solely responsible for resolving all template variables before passing tasks to the Handler. This template substitution phase occurs after task selection but before execution:

```typescript
// Template substitution in Evaluator
function resolveTemplateVariables(task: Task, env: Environment): Task {
  // Create a copy to avoid modifying the original
  const resolvedTask = {...task};
  
  // Apply appropriate substitution rules based on task type
  if (task.isFunctionTemplate && task.parameters) {
    // For function templates, create isolated environment with only parameters
    const funcEnv = new Environment({});
    for (const param of task.parameters) {
      funcEnv.bindings[param] = env.find(param);
    }
    resolvedTask.taskPrompt = substituteVariables(task.taskPrompt, funcEnv);
  } else {
    // For standard templates, use the full environment
    resolvedTask.taskPrompt = substituteVariables(task.taskPrompt, env);
  }
  
  return resolvedTask;
}
```

The Evaluator ensures that all placeholder substitutions (e.g., `{{variable_name}}`) are completed before dispatching to the Handler, ensuring all execution happens with fully resolved inputs. This includes resolving variables in both direct templates and function templates, with different resolution rules for each type. Associative matching tasks operate on the final, substituted task description.

Furthermore, the Evaluator extracts an optional success score from the task result's `notes` field. This score, if present, is intended to support future adaptive matching and error-handling strategies.

For more details on context handling and the disable context option implemented for atomic tasks, see [ADR 002 - Context Management](../../system/architecture/decisions/002-context-management.md) and [ADR 005 - Context Handling](../../system/architecture/decisions/005-context-handling.md).

#### Evaluator Coordination Diagram

```mermaid
flowchart TD
    A[Task Submission]
    B[Placeholder Substitution Completed]
    C[Prepare ContextGenerationInput]
    D[Invoke MemorySystem.getRelevantContextFor]
    E[Receive AssociativeMatchResult]
    F[Compute Matching Scores]
    G[Select Highest-Scoring Template]
    H[Extract Optional Success Score from Notes]
    I[Pass Template for Execution]
    A --> B
    B --> C
    C --> D
    D --> E
    E --> F
    F --> G
    G --> H
    H --> I
```

1. **AST Execution Controller**  
   - Orchestrates the step-by-step or operator-by-operator execution of tasks represented as an AST.
   - Calls out to the Handler for LLM-specific interactions and resource tracking (e.g. turn counts, context window checks).
   - Interacts with the Compiler when re-parsing or decomposition is required.

2. **Failure Recovery**  
   - Detects or receives error signals when tasks fail or exceed resources.  
   - Initiates "reparse" tasks or alternative decomposition approaches if the system's policies allow.  
   - Surfaces errors back to the Task System or parent contexts (e.g., "resource exhaustion," "invalid output").  

3. **Resource Usage Coordination**  
   - Not purely "owns" resource tracking (that's part of the Handler), but integrates with it. The Evaluator is aware of usage or limit errors and decides whether to attempt decomposition or fail outright.  

4. **Context and Environment Handling**  
   - In multi-step or operator-based tasks (sequential, reduce, etc.), the Evaluator ensures the proper propagation of parameters and context. Every new task or function call execution uses direct parameter passing between tasks rather than relying on environment variables. The Evaluator leverages Memory System 3.0 for associative context retrieval (following its read-only context model) but does not manage file content directly.

5. **Integration with Task System**  
   - The Task System may call the Evaluator with a structured or partially structured task. The Evaluator then "executes" it by walking its representation (e.g., an AST or an XML-based operator chain).  
   - On error or partial success, the Evaluator can signal the Task System to orchestrate higher-level recovery or store partial results.  

## 3.3 FunctionCall AST Node for DSL Function Calling

The FunctionCall node is used to invoke a task functionally by looking up its definition in the TaskLibrary. When a FunctionCall is evaluated, the Evaluator:
- Uses env.find("taskLibrary") to retrieve the registry;
- Looks up the task by its funcName;
- Creates a child environment (e.g., new Env({}, env));
- Binds the parameters from task_def.metadata to the evaluated arguments;
- And finally calls taskDef.astNode.eval(newEnv) to get the result.

```typescript
// Example FunctionCall evaluation
class FunctionCallNode implements FunctionCall {
    constructor(public funcName: string, public args: ASTNode[]) {}
    async eval(env: Environment): Promise<any> {
        const taskLibrary = env.find("taskLibrary")["taskLibrary"];
        const taskDef = taskLibrary.getTask(this.funcName);
        const funcEnv = new Env({}, env);
        const parameters: string[] = taskDef.metadata?.parameters || [];
        for (let i = 0; i < parameters.length && i < this.args.length; i++) {
            funcEnv.bindings[parameters[i]] = await this.args[i].eval(env);
        }
        return await taskDef.astNode.eval(funcEnv);
    }
}
```

## Function Call Processing

Function calls use direct parameter passing with lexical isolation:

1. **Template Lookup**: Retrieve template by name from TaskLibrary
2. **Argument Resolution**: For each argument in the caller's environment:
   - For string values: Try variable lookup first, fallback to literal value
   - For AST nodes: Recursively evaluate in caller's environment
3. **Fresh Environment Creation**: Create new environment with parameter bindings
   - Parameters explicitly bound to evaluated argument values
   - No implicit access to caller's variables
4. **Isolated Execution**: Execute template in this clean environment

This ensures templates can only access explicitly passed parameters, maintaining clear boundaries between caller and template scopes.

This process maintains clean scope boundaries, preventing unintended variable access.

### Argument Resolution Strategy

For string arguments, a two-step resolution occurs:
```typescript
function resolveArgument(arg: string, env: Environment): any {
  // First try to find it as a variable in the environment
  try {
    return env.find(arg);
  } catch (e) {
    // If not found as a variable, treat as a literal
    return arg;
  }
}
```
This allows for passing both variable references and literal values as function arguments.

## Metacircular Approach

Documentation (especially in `misc/textonly.tex.md`) sometimes refers to the system's evaluator as a "metacircular evaluator," meaning:
> The interpreter (Evaluator) uses LLM-based operations as its basic building blocks, while the LLM also uses the DSL or AST from the evaluator for self-decomposition tasks.

In practice, this means:  
- The Evaluator calls an LLM to run "atomic" tasks or to do "decomposition."  
- The LLM might generate or refine structured XML tasks that, in turn, the Evaluator must interpret again.  
- This cycle repeats until the tasks can be successfully executed without exceeding resource or output constraints.

Because of this, the Evaluator is partially "self-hosting": it leverages the same LLM to break down tasks that can't be executed directly.  

---

## Potential Future Enhancements

The existing plan outlines several optional or future features that involve the Evaluator:

1. **Advanced Debug Logging** (Phase 3 in the Implementation Plan)  
   - Collecting or storing extensive logs in `notes.debugLogs` or similar.  
   - Exposing partial steps or re-try decisions for advanced debugging.  

2. **Multi-Step or "Continuation" Protocol**  
   - The Evaluator might support tasks that require multiple interactions or "continuation steps" without losing context.  
   - This could involve storing partial states or sub-results in the environment and continuing in a new iteration.  

3. **Agent Features** (Phase 4 in some documents)  
   - The Evaluator could handle conversation-like tasks with a "REPL" approach, or coordinate multiple LLM backends.  
   - This is out of scope for the MVP, but recognized as an extension point.

---

## Known Open Questions

1. **Partial Results**  
   - Some references (e.g., "Phase 2: Expanded Context Management") mention partial-result handling if sub-tasks fail mid-operator. It is not yet finalized how the Evaluator will pass partial data up or whether to discard it.  

2. **Context Generation Errors**  
   - The error taxonomy may or may not include a dedicated "CONTEXT_GENERATION_FAILURE." Currently, the Evaluator might treat it as a generic `TASK_FAILURE` or trigger reparse.  

3. **Inheritance on Map/Reduce**  
   - It is hinted that "inherit_context" might become relevant for parallel or reduce operators. The Evaluator's role in distributing or discarding environment data for sub-tasks is still being discussed.

---

## Summary

The Evaluator coordinates the execution of tasks—represented in AST or XML-based form—by calling LLM operations, handling resource usage signals, managing sub-task context, and recovering from errors. It serves as the system's "control loop" for deciding whether tasks can be executed directly or require alternative approaches (like decomposition).  

*For further details:*  
- **System-Level Descriptions:** See `system/architecture/overview.md`  
- **Error Patterns & Recovery:** See `system/architecture/patterns/errors.md`, `misc/errorspec.md`  
- **Metacircular Evaluator Examples:** See the "Evaluator" sketches in `misc/textonly.tex.md`  
- **Future Expansions:** Refer to Implementation Plan phases in `implementation.md` (root-level or system docs).

## Context Management Implementation

The Evaluator manages all dimensions of the context management model:

### Standard Three-Dimensional Model
1. **Inherited Context**: The parent task's context, controlled by `inherit_context` setting ("full", "none", or "subset").
2. **Accumulated Data**: The step-by-step outputs collected during sequential execution, controlled by `accumulate_data` setting.
3. **Fresh Context**: New context generated via associative matching, controlled by `fresh_context` setting.

### Explicit File Inclusion
In addition to the standard model, the Evaluator supports explicit file inclusion through the `file_paths` feature:
- Files specified via `file_paths` are always included in context
- This operates orthogonally to the three-dimensional model
- File retrieval is delegated to Handler tools
- File content is formatted with XML tags indicating source paths

These dimensions are configured through the standardized context management XML structure:
```xml
<context_management>
    <inherit_context>full|none|subset</inherit_context>
    <accumulate_data>true|false</accumulate_data>
    <accumulation_format>notes_only|full_output</accumulation_format>
    <fresh_context>enabled|disabled</fresh_context>
</context_management>
```

When contexts are needed, the Evaluator decides which dimensions to include based on these settings.

## Associative Matching Invocation

When executing a sequential task step with `<inherit_context>none</inherit_context>` but `<accumulate_data>true</accumulate_data>` and `<fresh_context>enabled</fresh_context>`, the Evaluator:
1. Calls `MemorySystem.getRelevantContextFor()` with prior steps' partial results
2. Merges the returned `AssociativeMatchResult` into the next step's environment
3. Maintains complete separation from the Handler's resource management

### Evaluator Responsibilities for Associative Matching

* **Initiation**: The Evaluator is the *sole* caller of `MemorySystem.getRelevantContextFor()`.
* **Sequential History**: It retrieves partial outputs from `SequentialHistory` (the step-by-step data structure it maintains).
* **Context Merging**: If the step is configured for accumulation, the Evaluator incorporates the match results into the upcoming step's environment.
* **Error Handling**: Any failure to retrieve context (e.g., a memory system error) is handled through the existing `TASK_FAILURE` or resource-related error flow. No new error category is introduced.
* **No Handler Involvement**: The Handler does not participate in the retrieval or assembly of this context data, beyond tracking resource usage at a high level.

This design ensures that only the Evaluator initiates associative matching, preventing confusion about which component is responsible for cross-step data retrieval. The Memory System remains a service that simply provides matches upon request.

---

---

## Sequential Task History

When evaluating sequential tasks, the Evaluator implements the Sequential Task Management pattern [Pattern:SequentialTask:2.0] as defined in the system architecture. This includes:

- Maintaining explicit task history for each sequential operation
- Preserving step outputs until task completion or failure
- Implementing resource-aware storage with potential summarization
- Including partial results in error responses for failed sequences

The Evaluator is responsible for tracking this history independent of the Handler's resource management and implementing the appropriate accumulation behavior based on the task's context_management configuration.

For the complete specification of the Sequential Task Management pattern, including output tracking, preservation policies, and resource considerations, see `system/architecture/overview.md`.
## Subtask Spawning Implementation

The Evaluator implements the subtask tool mechanism as defined in [Pattern:ToolInterface:1.0], using the CONTINUATION status internally. From the LLM's perspective, these appear as tools but are implemented using the subtask spawning protocol.

Key responsibilities of the Evaluator in this pattern:
- Handling CONTINUATION requests from subtask tool calls
- Managing context according to the specified configuration
- Coordinating script execution when required
- Passing evaluation results back to the Director

When creating subtasks with explicit file paths:
```typescript
// The file_paths field takes precedence over associative matching
subtask_request = {
  type: "atomic",
  description: "Analyze specific modules",
  inputs: { /* parameters */ },
  context_management: { inherit_context: "subset" },
  file_paths: ["/src/main.py", "/src/utils.py"]
}
```
The Evaluator ensures these files are fetched and included in the subtask's context before execution.

## Tool Interface Integration

When the LLM invokes a subtask-based tool:
1. The Handler transforms this into a CONTINUATION with SubtaskRequest
2. The Evaluator receives and processes this request
3. Template selection occurs via associative matching
4. Execution follows the subtask spawning protocol
5. Results are returned to the parent task
</file>
<file path="./components/memory/README.md" project="">
# Memory System Component [Version 3.0]

## Overview

The Memory System provides context management and associative matching services for task execution. It maintains a global metadata index to support context retrieval while delegating actual file operations to Handler tools. The system focuses purely on metadata management and context retrieval - it does not store file content, perform file operations, track resources, or rank matches.

## Core Interface

The Memory System provides a standardized interface as defined in [Interface:Memory:3.0]. This interface supports:

- Global metadata index management through bulk operations
- Context retrieval through associative matching
- Clear separation between metadata management and file operations

**IMPORTANT: The Memory System NEVER performs file I/O operations (reading, writing, deletion). All file operations are exclusively handled by Handler tools.**

**NOTE: As of version 3.0, the Memory System follows a read-only context model. The updateContext method has been removed, and all context must be managed through the appropriate context management mechanisms in the Task System.**

The system focuses purely on metadata management and context retrieval - it does not store file content, perform file operations, track resources, or rank matches.

For the complete interface specification, method signatures, and type definitions, see `components/memory/api/interfaces.md`.

## Usage

The Memory System is typically used for two main purposes: managing the global metadata index and retrieving context for task execution. Here's a typical usage pattern:

```typescript
// Initialize and update index
const memory = new MemorySystem();
await memory.updateGlobalIndex(new Map([
    ['data.txt', 'Experimental results from 2024-02'],
    ['config.json', 'System configuration parameters']
]));

// Request context for task execution
const result = await memory.getRelevantContextFor({
    taskText: "Analyze recent experimental results",
    inheritedContext: "Previous analysis focused on temperature variance",
    previousOutputs: "Initial data validation complete"
});
```

## Integration Points

The Memory System integrates with other components through clear boundaries. It provides context and metadata to the Task System and Evaluator while relying on Handler tools for any actual file operations. The system maintains a read-only approach to context management, with updates happening through bulk operations on the metadata index.

All file operations go through Handler tools rather than the Memory System itself. This separation ensures clean resource management and clear responsibility boundaries. Error handling follows a simple pattern - all failures map to standard TASK_FAILURE errors with clear messages.

## Resource Management

The system takes a conservative approach to resource management. Index updates are bulk operations that replace the entire index, avoiding complexities of partial updates or hierarchical organization. Context is never preserved between tasks, and all updates happen through clean extensions rather than merging.

## Implementation Guidelines

When implementing against the Memory System, keep these principles in mind:

```typescript
// Context generation requires structured input
const input: ContextGenerationInput = {
    taskText: "Required task description",
    inheritedContext: "Optional parent context",
    previousOutputs: "Optional step outputs"
};

// Index management uses absolute paths
const index: GlobalIndex = new Map([
    ['/absolute/path/to/file', 'Metadata string']
]);
```

Use descriptive but concise metadata strings, maintain consistent formats, and handle all error cases explicitly. Always access file content through Handler tools rather than trying to store it in the Memory System.

## Version History

The current Version 3.0 removed the updateContext capability and enforces a read-only context model with simplified state management. This follows from architecture decisions documented in [ADR:Memory:1.0] and interface specifications in [Interface:Memory:3.0].

For detailed implementation specifications and patterns, refer to [Contract:Integration:TaskMemory:3.0] and [Pattern:ContextFrame:1.0].
</file>
<file path="./components/memory/api/interfaces.md" project="">
# Memory System Interfaces [Interface:Memory:3.0]

/**
 * Memory System Interfaces [Interface:Memory:3.0]
 *
 * IMPORTANT: The Memory System manages ONLY metadata about files (paths and descriptive strings).
 * It does NOT perform any file I/O operations - all file reading, writing, and deletion
 * is handled exclusively by Handler tools.
 * 
 * NOTE: As of version 3.0, the Memory System follows a read-only context model.
 * The updateContext method has been removed to enforce better architectural boundaries.
 */

## Overview
The Memory System provides two core capabilities:
1. Global file metadata index maintenance for associative matching

The system does not handle file content storage or retrieval.

## Core Types

/**
 * Represents metadata associated with a file
 * Stored as an unstructured string for flexibility
 */
type FileMetadata = string;

/**
 * Global index mapping file paths to their metadata
 * - Keys are absolute file paths
 * - Values are unstructured metadata strings
 * 
 * NOTE: The Memory System is responsible only for providing file metadata
 * (including file paths and unstructured metadata strings). All file I/O 
 * operations (reading, writing, deletion) are delegated to Handler tools.
 * The index serves as a bootstrap mechanism for associative matching when
 * full content scanning is not feasible.
 */
type GlobalIndex = Map<string, FileMetadata>;
```

A mapping of file paths to their associated metadata strings. The Memory System is responsible only for providing file metadata (including file paths and unstructured metadata strings). All file I/O operations (reading, writing, deletion) are delegated to Handler tools. The index serves as a bootstrap mechanism for associative matching when full content scanning is not feasible.

Key characteristics:
- Keys are absolute file paths
- Values are unstructured metadata strings
- No hierarchical organization
- Updated in bulk operations only

### FileMatch
```typescript
type FileMatch = [string, string | undefined];
```

Represents a relevant file match from associative matching:
- First element: Absolute file path
- Second element: Optional metadata string providing context for this specific match

### AssociativeMatchResult
```typescript
interface AssociativeMatchResult {
    context: string;      // Unstructured data context
    matches: FileMatch[]; // Relevant file matches
}
```

The complete result of associative matching, containing:
- Unstructured context data relevant to the current task
- List of potentially relevant files with optional per-file context

## Interface Methods


### Index Management

#### getGlobalIndex()
```typescript
getGlobalIndex(): Promise<GlobalIndex>
```
Retrieves the complete global file metadata index. The GlobalIndex should now support the needs of the TaskLibrary and nested environment model. Any changes to the GlobalIndex structure must allow associative matching in the context of nested task environments. Keys remain absolute file paths and values remain unstructured metadata but remain useful for task context extraction.

#### updateGlobalIndex(index: GlobalIndex)
```typescript
updateGlobalIndex(index: GlobalIndex): Promise<void>
```
Performs a bulk update of the global file metadata index. Replaces the entire existing index.


/**
 * Retrieve context using associative matching.
 * 
 * The Memory System does NOT perform ranking or prioritization of matches.
 * It only provides associative matching based on the input structure.
 *
 * @param input - The ContextGenerationInput containing task context
 * @returns A promise resolving to an AssociativeMatchResult object containing:
 *   - `context`: Unstructured text data relevant to the query
 *   - `matches`: An unordered list of file paths (and optional metadata) relevant to the query
 * @throws {INVALID_INPUT} If the input structure is malformed or missing required fields
 */
/**
 * Retrieve context using associative matching.
 * 
 * The Memory System does NOT perform ranking or prioritization of matches.
 * It only provides associative matching based on the input structure.
 * It does NOT read file contents - it only returns file paths and metadata.
 *
 * @param input - The ContextGenerationInput containing task context
 * @returns A promise resolving to an AssociativeMatchResult object containing:
 *   - `context`: Unstructured text data relevant to the query
 *   - `matches`: An unordered list of file paths (and optional metadata) relevant to the query
 * @throws {INVALID_INPUT} If the input structure is malformed or missing required fields
 */
declare function getRelevantContextFor(input: ContextGenerationInput): Promise<AssociativeMatchResult>;

// Note: updateContext method has been removed in version 3.0
// Context updates must happen through appropriate Task System mechanisms

## Integration Points
- Handler: Uses file paths from AssociativeMatchResult to read files via tools
- Associative Matching: Uses GlobalIndex as basis for context generation in tasks
</file>
<file path="./components/task-system/impl/examples.md" project="">
# Task System Implementation Examples

> **Important Note:** The examples in this document demonstrate that templates and function-based templates can define any task type (atomic, sequential, reduce, etc.). While all task types can be defined as templates, only atomic task templates participate in the template matching process used for task selection based on natural language descriptions. This distinction is important for understanding how tasks, templates, and functions interact in the system.

> **Note:** For a detailed description of the Director‑Evaluator pattern, refer to [system/architecture/patterns/director-evaluator.md](../system/architecture/patterns/director-evaluator.md).

## Basic Task Execution
```typescript
// Initialize TaskSystem
const taskSystem = new TaskSystem({
  maxTurns: 10,
  maxContextWindowFraction: 0.8,
  systemPrompt: "Default system prompt"
});

// Register handlers
taskSystem.onError((error) => {
  console.error('Task error:', error);
});

taskSystem.onWarning((warning) => {
  console.warn('XML validation warning:', warning.message);
});

// Execute task
const result = await taskSystem.executeTask(
  "analyze data",
  memorySystem
);

// Check XML parsing status
if (!result.outputs.some(output => output.wasXMLParsed)) {
  console.warn("XML parsing failed, using fallback string output");
}
```

## Task Management
```typescript
const taskDef: TaskDefinition = {
  name: "process_data",
  type: "atomic",
  provider: "anthropic",
  model: "claude-3-sonnet",
  body: {
    type: "atomic",
    content: `<task>
      <description>Process data using specific format</description>
      <inputs>
        <input name="raw_data">
          <description>Load and validate input data</description>
          <expected_output>
            Validated data in standard format:
            - Field validations complete
            - Type conversions applied
            - Missing values handled
          </expected_output>
        </input>
      </inputs>
      <expected_output>
        Processed data meeting format requirements:
        - Correct structure
        - Valid field types
        - Complete required fields
      </expected_output>
    </task>`
  },
  metadata: {
    isManualXML: true,
    disableReparsing: true
  }
};

// Register the task
await taskSystem.registerTask(taskDef);

// Validate task
const validation = taskSystem.validateTask(taskDef);

if (!validation.valid) {
  console.warn('Task validation warnings:', validation.warnings);
}

// Find matching tasks
const matches = await taskSystem.findMatchingTasks(
  "analyze peak patterns",
  memorySystem
);

console.log('Found matching tasks:', 
  matches.map(m => ({
    score: m.score,
    type: m.task.type,
    name: m.task.name
  }))
);
```

## Resource Management
```typescript
// Configure with resource limits
const taskSystem = new TaskSystem({
  maxTurns: 5,
  maxContextWindowFraction: 0.5,
  systemPrompt: "Resource-constrained execution"
});

try {
  const result = await taskSystem.executeTask(
    "process large dataset",
    memorySystem
  );
} catch (error) {
  if (error.type === 'RESOURCE_EXHAUSTION') {
    console.log('Resource limit exceeded:', error.resource);
    console.log('Usage metrics:', error.metrics);
  }
}
```

## Output Format Examples

### Task with JSON Output Format
```xml
<task type="atomic">
  <description>List files in directory</description>
  <output_format type="json" schema="string[]" />
</task>
```

### Template with Return Type
```xml
<template name="get_file_info" params="filepath" returns="object">
  <task>
    <description>Get metadata for {{filepath}}</description>
    <output_format type="json" schema="object" />
  </task>
</template>
```

### Function Call with JSON Result
```xml
<task type="sequential">
  <steps>
    <task>
      <description>Get directory listing</description>
    </task>
    <call template="get_file_info">
      <arg>result[0]</arg>
    </call>
  </steps>
</task>
```

### TypeScript Example
```typescript
// Execute task with JSON output format
const result = await taskSystem.executeTask(
  "<task><description>List files</description><output_format type='json' schema='string[]'/></task>",
  memorySystem
);

// Check if content was parsed as JSON
if (result.parsedContent) {
  // Use the parsed content as a typed value
  const files: string[] = result.parsedContent;
  console.log(`Found ${files.length} files`);
  
  // Process the files array
  const textFiles = files.filter(file => file.endsWith('.txt'));
  console.log(`Text files: ${textFiles.join(', ')}`);
} else {
  // Fall back to string content
  console.log(`Raw output: ${result.content}`);
}
```

## Task Definition and Function Calling

### Basic Function-Style Task Definition
```xml
<task name="validate_input" type="atomic" parameters="data,rules">
  <description>Validate {{data}} against {{rules}}</description>
  <context_management>
    <inherit_context>none</inherit_context>
    <fresh_context>enabled</fresh_context>
  </context_management>
</task>
```

### Context Management Mutual Exclusivity Examples

```xml
<!-- Valid: inherit_context="none" and fresh_context="enabled" -->
<task type="atomic">
  <description>Process data with fresh context only</description>
  <context_management>
    <inherit_context>none</inherit_context>
    <accumulate_data>false</accumulate_data>
    <fresh_context>enabled</fresh_context>
  </context_management>
</task>

<!-- Valid: inherit_context="full" and fresh_context="disabled" -->
<task type="atomic">
  <description>Process data with inherited context only</description>
  <context_management>
    <inherit_context>full</inherit_context>
    <accumulate_data>false</accumulate_data>
    <fresh_context>disabled</fresh_context>
  </context_management>
</task>

<!-- Invalid: inherit_context="full" and fresh_context="enabled" -->
<!-- This would cause validation failure -->
<task type="atomic">
  <description>Invalid combination</description>
  <context_management>
    <inherit_context>full</inherit_context>
    <accumulate_data>false</accumulate_data>
    <fresh_context>enabled</fresh_context>
  </context_management>
</task>
```

### Function Call with Variable Arguments
/**
 * Function Call XML Syntax
 * 
 * Arguments are evaluated in the caller's environment before being passed to the task:
 */
```xml
<call task="validate_input">
  <arg>user_input</arg>        <!-- Resolved as variable if possible -->
  <arg>validation_schema</arg>  <!-- Or used as literal if no variable matches -->
</call>
```

### Function-Style Task with Return Type
```xml
<task name="extract_metrics" type="atomic" parameters="log_data" returns="object">
  <description>Extract performance metrics from {{log_data}}</description>
  <output_format type="json" schema="object" />
</task>
```

### Complex Function Composition
```xml
<task type="sequential">
  <steps>
    <task>
      <description>Load input data</description>
    </task>
    <call task="validate_input">
      <arg>loaded_data</arg>
      <arg>{"required": ["name", "email"], "format": {"email": "email"}}</arg>
    </call>
    <call task="process_validated_data">
      <arg>validation_result</arg>
      <arg>processing_options</arg>
    </call>
  </steps>
</task>
```

/**
 * Function-Style Task and Call Example
 */
```typescript
// 1. Register function-style task with explicit parameters
await taskSystem.registerTask({
  name: "analyze_data",
  type: "atomic",
  parameters: ["dataset", "config"],  // Explicitly declared parameters
  body: {
    type: "atomic",
    content: "Analyze {{dataset}} using {{config}}",
  },
  returns: "object",
  provider: "anthropic"
});

// 2. Setup caller environment with variables
const callerEnv = new Environment({
  data_file: "sensor_readings.csv",
  analysis_options: {method: "statistical", outliers: "remove"}
});

// 3. Execute call with arguments evaluated in caller's environment
const result = await taskSystem.executeCall({
  taskName: "analyze_data",  // Changed from templateName to taskName
  arguments: [
    "data_file",         // Resolved to "sensor_readings.csv" from callerEnv
    "analysis_options"   // Resolved to the object from callerEnv
  ]
}, callerEnv);

// 4. Example with mixed variable/literal arguments
const mixedResult = await taskSystem.executeCall({
  taskName: "analyze_data",  // Changed from templateName to taskName
  arguments: [
    "data_file",                     // Variable lookup
    {method: "custom", limit: 100}   // Direct literal (no lookup)
  ]
}, callerEnv);
```

## Global Index Example
```typescript
// Minimal memory object focusing on file metadata
const memory = {
  getGlobalIndex() {
    return new Map([
      ['data.txt', 'metadata'],
      ['config.json', 'metadata'],
      ['history.log', 'metadata']
    ]);
  },
  updateGlobalIndex(index) {}
};

// Now we can pass this memory object to the taskSystem
const result = await taskSystem.executeTask("analyze recent changes", memory);
```

## Specialized Task Types

### Reparse Task Example
```typescript
const reparseTemplate: TaskTemplate = {
  taskPrompt: `<task type="reparse">
    <description>Decompose large task into smaller units</description>
    <failed_task>
      <error type="RESOURCE_EXHAUSTION">
        <resource>context</resource>
        <message>Context window limit exceeded</message>
      </error>
      <original_prompt>Process entire codebase</original_prompt>
    </failed_task>
  </task>`,
  systemPrompt: "Decomposition specialist",
  model: "claude-3-sonnet",
  isManualXML: true
};

try {
  const result = await taskSystem.executeTask(
    reparseTemplate.taskPrompt,
    memorySystem,
    "reparse"
  );
} catch (error) {
  console.error('Reparse failed:', error);
}
```

### Memory Task Example
```typescript
const memoryTemplate: TaskTemplate = {
  taskPrompt: `<task type="associative_memory">
    <description>Find relevant context for implementation</description>
    <query>error handling patterns</query>
    <constraints>
      <max_results>3</max_results>
      <relevance_threshold>0.8</relevance_threshold>
    </constraints>
  </task>`,
  systemPrompt: "Context retrieval specialist",
  model: "claude-3-sonnet"
};

const memoryResult = await taskSystem.executeTask(
  memoryTemplate.taskPrompt,
  memorySystem,
  "associative_memory"
);

console.log('Retrieved context:', memoryResult.content);
```

---

// Example of error handling with simplified structure
try {
  const result = await taskSystem.executeTask(taskDefinition, memorySystem);
  // Success case: content is complete
  console.log("Task completed successfully:", result.content);
} catch (error) {
  if (error.type === 'RESOURCE_EXHAUSTION') {
    console.log(`Resource limit exceeded: ${error.resource}`);
  } else if (error.type === 'TASK_FAILURE') {
    // Access partial content directly from content field
    console.log(`Partial output before failure: ${error.content}`);
    console.log(`Execution stage: ${error.notes.executionStage || "unknown"}`);
    console.log(`Completion: ${error.notes.completionPercentage || 0}%`);
  }
}

<!-- Example: Sequential Task with Default Context Management -->
<task type="sequential">
    <description>Process and analyze data</description>
    <!-- No context_management block - using defaults:
         inherit_context: full
         accumulate_data: true
         accumulation_format: notes_only
         fresh_context: enabled -->
    <steps>
        <task>
            <description>Load dataset</description>
            <inputs>
                <input name="data_file" from="csv_file_path"/>
            </inputs>
        </task>
        <task>
            <description>Filter invalid rows</description>
        </task>
    </steps>
</task>

<!-- Example: Sequential Task with Custom Context Management -->
<task type="sequential">
    <description>Process and analyze data</description>
    <context_management>
        <inherit_context>none</inherit_context>
        <accumulate_data>true</accumulate_data>
        <accumulation_format>full_output</accumulation_format>
        <fresh_context>enabled</fresh_context>
    </context_management>
    <steps>
        <task>
            <description>Load dataset</description>
            <inputs>
                <input name="data_file" from="csv_file_path"/>
            </inputs>
        </task>
        <task>
            <description>Filter invalid rows</description>
        </task>
    </steps>
</task>

<!-- Example: Static Director-Evaluator Loop with Script Execution -->
<task type="director_evaluator_loop">
  <description>Process and evaluate code</description>
  <max_iterations>3</max_iterations>
  <context_management>
    <inherit_context>none</inherit_context>
    <accumulate_data>true</accumulate_data>
    <accumulation_format>notes_only</accumulation_format>
    <fresh_context>enabled</fresh_context>
  </context_management>
  <director>
    <description>Generate Python code to solve problem: {{problem_statement}}</description>
    <inputs>
      <input name="problem_statement" from="user_query"/>
      <input name="feedback" from="evaluation_feedback"/>
      <input name="iteration" from="current_iteration"/>
    </inputs>
  </director>
  <script_execution>
    <command>python3 -c "{{script_input}}"</command>
    <timeout>5</timeout>
    <inputs>
      <input name="script_input" from="director_result"/>
    </inputs>
  </script_execution>
  <evaluator>
    <description>Evaluate code quality and execution results</description>
    <inputs>
      <input name="code" from="director_result"/>
      <input name="execution_output" from="script_output"/>
      <input name="execution_errors" from="script_errors"/>
      <input name="exit_code" from="script_exit_code"/>
    </inputs>
  </evaluator>
  <termination_condition>
    <condition>evaluation.success === true || iteration >= 3</condition>
  </termination_condition>
</task>

## Subtask Spawning Examples

### Basic Subtask Request Example

```typescript
// Parent task implementation
async function processComplexTask(input: string): Promise<TaskResult> {
  // Determine if we need to spawn a subtask
  if (isComplexInput(input)) {
    // Return a continuation with subtask request
    return {
      content: "Need to process complex input with a specialized subtask",
      status: "CONTINUATION",
      notes: {
        subtask_request: {
          type: "atomic",
          description: "Process complex data structure",
          inputs: {
            data: input,
            format: "json",
            validation_rules: { required: ["id", "name"] }
          },
          template_hints: ["data_processor", "validator"],
          context_management: {
            inherit_context: "subset",
            fresh_context: "enabled"
          }
        }
      }
    };
  }
  
  // Process simple input directly
  return {
    content: `Processed: ${input}`,
    status: "COMPLETE",
    notes: { dataUsage: "Simple processing completed" }
  };
}

// Example of subtask tool execution
async function executeWithSubtasks() {
  // Create a Handler with session preservation
  const handler = new Handler(handlerConfig);
  
  // Register subtask tool
  handler.registerSubtaskTool("analyzeData", ["data_analysis", "statistical"]);
  
  // Execute the task (internal handling of continuations)
  const result = await taskSystem.executeTask("process complex data", memorySystem);
  
  // TaskSystem automatically:
  // 1. Detects CONTINUATION status
  // 2. Executes the subtask
  // 3. Adds result as tool response to parent's session
  // 4. Continues parent execution
  
  console.log("Final result:", result.content);
}

// Example of what happens internally in taskSystem.executeTask
async function internalTaskSystemFlow(task, context) {
  // Get or create a Handler
  const handler = this.getHandlerForTask(task);
  
  // Execute the initial prompt
  const result = await handler.executePrompt(task.taskPrompt);
  
  // Check for continuation
  if (result.status === "CONTINUATION" && result.notes?.subtask_request) {
    // Execute the subtask
    const subtaskResult = await this.executeSubtask(result.notes.subtask_request);
    
    // Add subtask result as a tool response to parent's session
    handler.addToolResponse(
      this.getToolNameFromRequest(result.notes.subtask_request),
      subtaskResult.content
    );
    
    // Continue parent execution
    return handler.executePrompt("Continue based on the tool results.");
  }
  
  return result;
}

### Example with Explicit File Paths

```typescript
// Subtask with explicit files
return {
  status: "CONTINUATION",
  notes: {
    subtask_request: {
      type: "atomic",
      description: "Analyze code modules",
      inputs: { analysis_depth: "detailed" },
      context_management: { inherit_context: "subset" },
      file_paths: ["/src/main.py", "/src/utils.py"]
    }
  }
};
```
```

### Error Handling Example

```typescript
// Simplified error handling example
try {
  const result = await taskSystem.executeTask("analyze complex document");
  console.log("Analysis complete:", result.content);
} catch (error) {
  // Standard error types without complex partial results
  if (error.type === 'RESOURCE_EXHAUSTION') {
    console.log(`Resource limit exceeded: ${error.resource}`);
  } else if (error.type === 'TASK_FAILURE') {
    console.log(`Task failed: ${error.message}`);
    
    // If this was a subtask failure, the error contains the original request
    if (error.reason === 'subtask_failure') {
      console.log(`Failed subtask: ${error.details.subtaskRequest.description}`);
    }
  }
}
```

### Context Integration Example

```xml
<!-- Parent task that spawns a subtask -->
<task type="atomic">
  <description>Analyze code repository structure</description>
  <context_management>
    <inherit_context>full</inherit_context>
    <fresh_context>enabled</fresh_context>
  </context_management>
  
  <!-- This task will return CONTINUATION status with a subtask_request -->
</task>

<!-- Example of how the subtask request would be structured -->
<!-- This is not XML that would be written directly, but represents
     the structure that would be in the subtask_request -->
<task type="atomic">
  <description>Analyze specific module dependencies</description>
  <context_management>
    <inherit_context>subset</inherit_context>
    <fresh_context>enabled</fresh_context>
  </context_management>
  <inputs>
    <input name="module_path" from="parent_analysis.target_module"/>
    <input name="depth" from="parent_analysis.analysis_depth"/>
  </inputs>
</task>
```

```typescript
// TypeScript implementation showing context flow
async function executeWithContextIntegration() {
  // Execute parent task
  const parentResult = await taskSystem.executeTask(
    "<task><description>Analyze code repository structure</description></task>",
    memorySystem
  );
  
  // Check for continuation with subtask request
  if (parentResult.status === "CONTINUATION" && 
      parentResult.notes.subtask_request) {
    
    const subtaskRequest = parentResult.notes.subtask_request;
    
    // Context management settings from request (or defaults)
    const contextSettings = subtaskRequest.context_management || {
      inherit_context: "subset",
      fresh_context: "enabled"
    };
    
    // Get appropriate context based on settings
    let subtaskContext;
    if (contextSettings.inherit_context === "full") {
      subtaskContext = parentContext;
    } else if (contextSettings.inherit_context === "subset") {
      // Get relevant subset via associative matching
      subtaskContext = await memorySystem.getRelevantContextFor({
        taskText: subtaskRequest.description,
        inheritedContext: parentContext
      });
    } else {
      // No inherited context
      subtaskContext = null;
    }
    
    // Execute subtask with appropriate context
    const subtaskResult = await taskSystem.executeSubtask(
      subtaskRequest,
      subtaskContext
    );
    
    // Resume parent task with subtask result
    const finalResult = await taskSystem.resumeTask(
      parentResult,
      { subtask_result: subtaskResult }
    );
    
    console.log("Final analysis:", finalResult.content);
  }
}
```

<!-- Example: Context Management Patterns -->

<!-- Pattern 1: Clear All Context (Fresh Start) -->
<task type="atomic">
    <description>Start with completely fresh context</description>
    <context_management>
        <inherit_context>none</inherit_context>
        <accumulate_data>false</accumulate_data>
        <fresh_context>enabled</fresh_context>
    </context_management>
</task>

## Unified Tool Interface Examples

### Direct vs. Subtask Tool Examples

```typescript
// Example of Handler registering both tool types
handler.registerDirectTool("readFile", async (path) => {
  return await fs.readFile(path, 'utf8');
});

handler.registerSubtaskTool("analyzeData", ["data_analysis", "statistical"]);

// LLM usage looks similar for both
const fileContent = tools.readFile("data.csv");
const analysis = tools.analyzeData({ 
  data: fileContent,
  method: "statistical"
});

// But implementations differ:
// Direct tool implementation (in Handler)
async function executeReadFile(path) {
  return await fs.readFile(path, 'utf8');
}

// Subtask tool implementation (via CONTINUATION)
async function executeAnalyzeData(params) {
  return {
    status: "CONTINUATION",
    notes: {
      subtask_request: {
        type: "atomic",
        description: `Analyze data using ${params.method}`,
        inputs: params,
        template_hints: ["data_analysis"]
      }
    }
  };
}
```

<!-- Pattern 2: Rebuild Context While Preserving History -->
<task type="sequential">
    <description>Rebuild context while keeping step history</description>
    <context_management>
        <inherit_context>none</inherit_context>
        <accumulate_data>true</accumulate_data>
        <accumulation_format>notes_only</accumulation_format>
        <fresh_context>enabled</fresh_context>
    </context_management>
    <steps>
        <task>
            <description>First step</description>
        </task>
        <task>
            <description>Second step with fresh context but notes from first step</description>
        </task>
    </steps>
</task>

<!-- Pattern 3: Complete Context Preservation -->
<task type="sequential">
    <description>Preserve all context</description>
    <context_management>
        <inherit_context>full</inherit_context>
        <accumulate_data>true</accumulate_data>
        <accumulation_format>full_output</accumulation_format>
        <fresh_context>disabled</fresh_context>
    </context_management>
    <steps>
        <task>
            <description>First step</description>
        </task>
        <task>
            <description>Second step with all context preserved</description>
        </task>
    </steps>
</task>

```typescript
// Example of context pattern usage in TypeScript

// Pattern 1: Clear All Context (Fresh Start)
const clearContextResult = await taskSystem.executeTask(
  "<task type='atomic'><description>Fresh context task</description><context_management><inherit_context>none</inherit_context><accumulate_data>false</accumulate_data><fresh_context>enabled</fresh_context></context_management></task>",
  memorySystem
);

// Pattern 2: Rebuild Context While Preserving History
const rebuildWithHistoryResult = await taskSystem.executeTask(
  "<task type='sequential'><description>Sequential with history</description><context_management><inherit_context>none</inherit_context><accumulate_data>true</accumulate_data><fresh_context>enabled</fresh_context></context_management></task>",
  memorySystem
);

// Pattern 3: Complete Context Preservation
const preserveAllContextResult = await taskSystem.executeTask(
  "<task type='sequential'><description>Preserve all context</description><context_management><inherit_context>full</inherit_context><accumulate_data>true</accumulate_data><fresh_context>disabled</fresh_context></context_management></task>",
  memorySystem
);
```

## File Paths Examples

### Atomic Task with File Paths

```xml
<!-- Example: Atomic Task with File Paths -->
<task type="atomic">
    <description>Analyze the main source files</description>
    <context_management>
        <inherit_context>none</inherit_context>
        <fresh_context>disabled</fresh_context>
    </context_management>
    <file_paths>
        <path>./src/main.py</path>
        <path>./src/utils.py</path>
    </file_paths>
</task>
```

### TypeScript Usage Example

```typescript
// Execute atomic task with specific file paths
const result = await taskSystem.executeTask(
  "<task type='atomic'><description>Analyze source files</description><file_paths><path>./src/main.py</path><path>./src/utils.py</path></file_paths></task>",
  memorySystem
);

// Create subtask request with file paths
return {
  status: "CONTINUATION",
  notes: {
    subtask_request: {
      type: "atomic",
      description: "Analyze specific modules",
      inputs: { level: "detailed" },
      context_management: { inherit_context: "subset" },
      file_paths: ["./src/main.py", "./src/utils.py"]
    }
  }
};
```

### Combining File Paths with Context Management

```xml
<!-- Example: Combining File Paths with fresh_context -->
<task type="atomic">
    <description>Analyze code with base context and specific files</description>
    <context_management>
        <inherit_context>none</inherit_context>
        <fresh_context>enabled</fresh_context>
    </context_management>
    <file_paths>
        <path>./src/main.py</path>
        <path>./src/utils.py</path>
    </file_paths>
</task>
```

This combination:
1. Doesn't inherit any parent context
2. Explicitly includes the specified files
3. Uses associative matching to find additional relevant context
</file>
<file path="./components/task-system/impl/xml-processing.md" project="">
# XML Processing Implementation

> **Overview and References:** This document focuses on the Task System's XML processing implementation. For the complete XML schema definition and template guidelines, please refer to [system/contracts/protocols.md](../system/contracts/protocols.md).

## Schema Validation

### Core Schema Requirements
- Based on [Contract:Tasks:TemplateSchema:1.0]
- Required elements:
  * `instructions` (maps to taskPrompt)
  * `system` (maps to systemPrompt)
  * `model` (maps to model)
- Optional elements:
  * `inputs` with named input definitions
  * `manual_xml` flag
  * `disable_reparsing` flag

### Validation Rules
- All required fields must be present
- Input names must be unique
- Boolean fields must be "true" or "false"
- Model must be a valid LLM identifier

## Template Processing

### Template Validation
- Schema conformance checking
- Required field validation
- Type checking for known fields
- Warning generation for non-critical issues

### Manual XML Tasks
- Direct structure usage without reparsing
- Schema validation still applies
- Support for disable_reparsing flag
- No automatic restructuring

### Function Template Processing

#### Template Parsing
- When a `<template>` element is encountered, it's parsed into a TemplateNode
- The `name` attribute becomes the template name
- The `params` attribute is split into individual parameter names
- The body task is parsed recursively
- The template is automatically registered in the TaskLibrary

#### Function Call Parsing
- When a `<call>` element is encountered, it's parsed into a FunctionCallNode
- The `template` element value becomes the templateName
- Each `<arg>` child element is parsed into an ArgumentNode
- String values in arguments are evaluated to check if they represent variables

#### Template Validation
- Template names must be unique within the TaskLibrary
- Parameter lists must use valid identifiers
- Templates must have a valid body task
- Function calls must reference existing templates
- Argument counts must match parameter counts

## Output Processing

### XML Generation
```xml
<!-- Example Output Structure -->
<task type="sequential">
    <description>Task description</description>
    <steps>
        <task>
            <description>Step description</description>
            <inputs>
                <input name="input_name">
                    <task>
                        <description>Input task</description>
                    </task>
                </input>
            </inputs>
        </task>
    </steps>
</task>
```

### Output Validation
- Structure validation against schema
- Required field presence checking
- Type validation for known fields
- XML well-formedness checking

### Output Format Validation
- Format specification via `<output_format>` element
- JSON detection process:
  * Attempts to parse content as JSON
  * Validates parsed content against schema attribute
  * Returns original content if parsing fails
- Type validation against schema attribute:
  * "object" - Validates as JavaScript object
  * "array" or "[]" - Validates as array
  * "string[]" - Validates as array of strings
  * "number" - Validates as numeric value
  * "boolean" - Validates as boolean value
- Error handling for format violations:
  * Generates TASK_FAILURE with reason "output_format_failure"
  * Includes expected vs actual type information
  * Preserves original output in error details
- Template return type validation:
  * Function templates can specify return types
  * Return types are validated against actual output
  * Type mismatches generate validation errors

Example XML showing proper usage:
```xml
<task>
  <description>Get repository statistics</description>
  <output_format type="json" schema="object" />
</task>

<!-- With template return type -->
<template name="get_stats" params="repo_path" returns="object">
  <task>
    <description>Get statistics for {{repo_path}}</description>
    <output_format type="json" schema="object" />
  </task>
</template>
```

### Fallback Behavior
- Return unstructured string on parse failure
- Collect and surface warnings
- Maintain original content
- Include parsing error details

## Error Handling

### Validation Errors
```typescript
interface XMLError {
  type: 'XML_PARSE_ERROR' | 'VALIDATION_ERROR';
  message: string;
  location?: string;
  violations?: string[];
}
```

### Recovery Strategies
- Attempt partial content recovery
- Generate fallback string output
- Preserve original content
- Surface all validation issues

## Integration Points

### Template Management
- Load and validate schemas
- Process template definitions
- Handle manual XML flags
- Track template versions

### Task Execution
- Validate output structure
- Handle parsing failures
- Surface warnings appropriately
- Maintain execution context
</file>
<file path="./components/task-system/impl/resource-management.md" project="">
# Resource Management Implementation

> **Further Reading:** For an architectural overview of resource management principles, see [system/architecture/patterns/resource-management.md](../system/architecture/patterns/resource-management.md).

## Core Principles
- No resource usage prediction
- No task decomposition optimization
- Handler-based resource tracking
- Clear resource ownership boundaries

## Turn Counter Management

### Implementation Details
- Per-Handler turn tracking
- Atomic increment operations
- Strict limit enforcement
- No cross-Handler pooling

### Turn Management Rules
- Turn tracking owned by Handler instance
- Turn limit passed during Handler initialization
- Interactive sessions count against turn limits

### Usage Tracking
```typescript
// Using canonical ResourceMetrics definition from spec/types.md
// See [Type:ResourceMetrics:1.0]

class TurnCounter {
  private metrics: ResourceMetrics['turns'];
  
  increment(): void {
    if (this.metrics.used >= this.metrics.limit) {
      throw new ResourceExhaustionError('turns');
    }
    this.metrics.used++;
    this.metrics.lastTurnAt = new Date();
  }
}
```

### Configuration
- Default turn limits from config
- Per-task limit overrides
- Warning at 80% threshold
- Error at limit reached

## Context Window Management

### Size Tracking
- Token-based calculation
- Window size monitoring
- Fraction-based limits
- No content optimization

### Implementation
```typescript
// Using canonical ResourceMetrics definition from spec/types.md
// See [Type:ResourceMetrics:1.0]

class ContextManager {
  private metrics: ResourceMetrics['context'];
  
  addContent(content: string): void {
    const tokens = this.countTokens(content);
    if (this.metrics.used + tokens > this.metrics.limit) {
      throw new ResourceExhaustionError('context');
    }
    this.metrics.used += tokens;
    this.metrics.peakUsage = Math.max(this.metrics.peakUsage, this.metrics.used);
  }
}
```

### Monitoring
- Continuous token tracking
- Peak usage recording
- Threshold warnings
- Limit enforcement

## Resource Cleanup

### Handler Termination
- Clean session shutdown
- Resource accounting completion
- Metric collection
- No state preservation

### Memory Management
- Context cleanup
- Reference clearing
- Memory release
- State invalidation

## Integration Points

### Handler Integration
- Resource initialization
- Usage monitoring
- Limit enforcement
- Cleanup coordination

/**
 * Implementation of session-based resource management
 */
class HandlerSession {
  private systemPrompt: string;
  private messages: Message[] = [];
  private turnCounter: TurnCounter;
  private contextManager: ContextManager;
  private config: HandlerConfig;
  
  constructor(config: HandlerConfig) {
    this.config = config;
    this.systemPrompt = config.systemPrompt;
    this.turnCounter = new TurnCounter({
      limit: config.maxTurns,
      used: 0,
      lastTurnAt: new Date()
    });
    this.contextManager = new ContextManager({
      limit: Math.floor(config.maxContextWindowFraction * this.getModelMaxTokens(config.defaultModel)),
      used: 0,
      peakUsage: 0
    });
  }
  
  addUserMessage(content: string): void {
    this.messages.push({ 
      role: "user", 
      content, 
      timestamp: new Date() 
    });
    this.contextManager.addContent(content);
    // No turn increment for user messages
  }
  
  addAssistantMessage(content: string): void {
    this.messages.push({ 
      role: "assistant", 
      content, 
      timestamp: new Date() 
    });
    this.contextManager.addContent(content);
    this.turnCounter.increment(); // Increment turn counter for assistant responses
  }
  
  /**
   * Constructs a payload for the LLM using fully resolved content
   * @param task The resolved task template with all variables already substituted
   * @returns A complete HandlerPayload ready for LLM submission
   */
  constructPayload(task: TaskTemplate): HandlerPayload {
    // Note: All template variables should already be resolved by the Evaluator
    return {
      systemPrompt: this.systemPrompt,
      messages: [...this.messages, { 
        role: "user", 
        content: task.taskPrompt, // Already fully resolved
        timestamp: new Date()
      }],
      context: this.contextManager.getCurrentContext(),
      tools: this.getAvailableTools(),
      metadata: {
        model: this.config.defaultModel,
        resourceUsage: this.getResourceMetrics()
      }
    };
  }
  
  // resolveTemplatePlaceholders method has been removed
  // Template substitution is now an Evaluator responsibility
  
  getResourceMetrics(): ResourceMetrics {
    return {
      turns: this.turnCounter.getMetrics(),
      context: this.contextManager.getMetrics()
    };
  }
  
  private getModelMaxTokens(model: string): number {
    // Return model-specific token limits
    // Implementation details omitted
    const modelTokenLimits = {
      "claude-3-opus": 200000,
      "claude-3-sonnet": 180000,
      "claude-3-haiku": 150000,
      "gpt-4": 128000,
      "gpt-4-turbo": 128000,
      "gpt-3.5-turbo": 16000
    };
    
    return modelTokenLimits[model] || 100000; // Default fallback
  }
  
  private getAvailableTools(): ToolDefinition[] {
    // Return registered tools
    // Implementation details omitted
    return []; // Placeholder - actual implementation would return registered tools
  }
}

### Memory System
- Read-only metadata access
- Global index management
- Associative matching services
- No file operations or content storage
- Clear interface boundaries with Handler tools

### Error Handling
- Resource exhaustion detection
- Clean termination
- Metric reporting
- State cleanup
</file>
<file path="./components/task-system/impl/design.md" project="">
# Implementation Design

## Terminology and References

 - **Handler** and **Evaluator** definitions are standardized in [spec/types.md](../spec/types.md).
 - XML schema definitions are available in [system/contracts/protocols.md](../system/contracts/protocols.md).
 - For detailed resource tracking implementation (including turn counter and context window monitoring), see [resource-management.md](./resource-management.md).
 - For XML processing details (parsing, validation, and fallback behavior), refer to [xml-processing.md](./xml-processing.md).

## Handler Implementation

### Session Management Strategy
- Handler creates a HandlerSession for each task execution
- Session maintains complete conversation state and message history
- Provider-agnostic HandlerPayload structure for LLM interactions
- Clean session termination on completion
- Tool-based approach for user input requests
  
### Resource Tracking Implementation
- Turn counter integrated with HandlerSession
- Turns incremented only for assistant messages
- Context window tracks all messages and context
- Token usage monitored across full conversation
- Resource metrics available via session.getResourceMetrics()
- Limits enforced during session operations

### Payload Construction
- HandlerPayload provides unified structure for LLM requests
- Includes: systemPrompt, messages, context, tools, metadata
- Provider-specific adapters transform to appropriate format
- Session constructs payload via constructPayload() method
- Full conversation history included in structured format

### Error Propagation Design
- Standard error type system
- Immediate propagation on detection
- Clean resource release
- No retry attempt handling
- Complete error context preservation

### Interactive Session Support
- Input detection capabilities
- Agent-controlled input requests
- Resource tracking during interaction
- Input timeout handling
- Cancellation support

## Template Management
### Storage Implementation
- XML file-based storage
- Disk-based persistence
- Directory organization by type
- Template versioning support
- Schema validation enforcement
  
### Validation Implementation
- Basic XML structure validation
- Schema conformance checking
- Warning generation for issues
- Template field validation
- Model availability checking
  
### Matching Algorithm Design
- Scoring based on prompt results
- Top-N candidate selection
- Separate matching for human input vs AST
- Score normalization
- Clear ordering requirements
  
### XML Processing Details
- Lenient parsing with fallback
- Warning collection
- Graceful degradation
- Partial parsing support
- Clear error locations

## Context Management Implementation

### Hybrid Configuration Approach

The Task System implements a hybrid configuration approach with operator-specific defaults and explicit overrides:

```typescript
// Default context management settings by operator type and subtype
const DEFAULT_CONTEXT_SETTINGS = {
  atomic: {
    standard: {
      inheritContext: 'full',
      accumulateData: false,
      accumulationFormat: 'notes_only',
      freshContext: 'disabled'
    },
    subtask: {
      inheritContext: 'none',
      accumulateData: false,
      accumulationFormat: 'notes_only',
      freshContext: 'enabled'
    },
    // Default to standard if no subtype specified
    default: {
      inheritContext: 'full',
      accumulateData: false,
      accumulationFormat: 'notes_only',
      freshContext: 'disabled'
    }
  },
  sequential: {
    inheritContext: 'full',
    accumulateData: true,
    accumulationFormat: 'notes_only',
    freshContext: 'disabled'
  },
  reduce: {
    inheritContext: 'none',
    accumulateData: true,
    accumulationFormat: 'notes_only',
    freshContext: 'enabled'
  },
  script: {
    inheritContext: 'full',
    accumulateData: false,
    accumulationFormat: 'notes_only',
    freshContext: 'disabled'
  },
  director_evaluator_loop: {
    inheritContext: 'none',
    accumulateData: true,
    accumulationFormat: 'notes_only',
    freshContext: 'enabled'
  }
};

// File Path Processing
function processFilePaths(task: TaskDefinition, context: TaskContext): Promise<TaskContext> {
  if (!task.file_paths || task.file_paths.length === 0) {
    return Promise.resolve(context);
  }
  
  return fetchAndAddFiles(task.file_paths, context);
}

async function fetchAndAddFiles(filePaths: string[], context: TaskContext): Promise<TaskContext> {
  const fileContents: Array<{path: string, content: string}> = [];
  const warnings: string[] = [];
  
  // Process each file path
  for (const path of filePaths) {
    try {
      // Resolve relative paths to absolute
      const absolutePath = resolvePath(path);
      
      // Fetch file content using Handler tools
      const content = await handler.tools.readFile(absolutePath);
      
      fileContents.push({ path: absolutePath, content });
    } catch (error) {
      warnings.push(`Warning: Failed to read file ${path}: ${error.message}`);
    }
  }
  
  // Format file contents with XML tags
  const formattedContent = fileContents.map(fc => 
    `<file path="${fc.path}">\n${fc.content}\n</file>`
  ).join('\n\n');
  
  // Add to context
  const enhancedContext = {
    ...context,
    fileContent: (context.fileContent || '') + formattedContent
  };
  
  // Store warnings
  if (warnings.length > 0) {
    enhancedContext.warnings = (enhancedContext.warnings || []).concat(warnings);
  }
  
  return enhancedContext;
}

function resolvePath(path: string): string {
  if (path.startsWith('/')) {
    // Already absolute
    return path;
  }
  
  // Resolve relative to repo root
  return joinPath(REPO_ROOT, path);
}

// Template processing with merged settings
function processTemplate(template) {
  const operatorType = template.type;
  
  // Get defaults based on type and subtype
  let defaults;
  if (operatorType === 'atomic' && template.subtype) {
    defaults = DEFAULT_CONTEXT_SETTINGS[operatorType][template.subtype] || 
               DEFAULT_CONTEXT_SETTINGS[operatorType].default;
  } else {
    defaults = DEFAULT_CONTEXT_SETTINGS[operatorType];
  }
  
  // The Evaluator stores the TaskResult's notes field (and optionally content) based on the task's accumulation_format setting.
  // No additional summary generation is performed - the notes field as generated by the LLM is preserved directly.
  
  // Special case: For subtask requests via CONTINUATION, use subtask defaults
  if (operatorType === 'atomic' && template.source === 'continuation') {
    defaults = DEFAULT_CONTEXT_SETTINGS.atomic.subtask;
  }
  
  // If context_management is present, merge with defaults
  if (template.contextManagement) {
    const mergedSettings = {
      ...defaults,
      ...template.contextManagement
    };
    
    // Validate mutual exclusivity constraint
    if ((mergedSettings.inheritContext === 'full' || mergedSettings.inheritContext === 'subset') 
        && mergedSettings.freshContext === 'enabled') {
      throw new Error('Invalid context management configuration: fresh_context="enabled" cannot be combined with inherit_context="full" or inherit_context="subset"');
    }
    
    return mergedSettings;
  }
  
  // Otherwise use defaults
  return defaults;
}

// Context Constraint Validation
function validateContextSettings(settings) {
  // Check for mutual exclusivity violation
  if ((settings.inheritContext === 'full' || settings.inheritContext === 'subset') 
      && settings.freshContext === 'enabled') {
    return {
      valid: false,
      error: 'Context constraint violation: fresh_context="enabled" cannot be combined with inherit_context="full" or inherit_context="subset"'
    };
  }
  
  // Check for empty context warning
  if (settings.inheritContext === 'none' && !settings.accumulateData && settings.freshContext === 'disabled') {
    return {
      valid: true,
      warning: 'Warning: Task will execute with minimal context (no inheritance, no accumulation, no fresh context)'
    };
  }
  
  return { valid: true };
}

async function getContextForTask(task: TaskDefinition, parentContext: ExecutionContext): Promise<ExecutionContext> {
  // Start with base context according to inherit_context setting
  let context = getBaseContext(task.context_management?.inherit_context, parentContext);
  
  // If file_paths is specified, fetch those files
  if (task.file_paths && task.file_paths.length > 0) {
    context = await fetchAndAddFiles(task.file_paths, context);
  }
  
  // If fresh_context is enabled, still do associative matching
  if (task.context_management?.fresh_context === 'enabled') {
    const freshContext = await memorySystem.getRelevantContextFor({
      taskText: task.description,
      inheritedContext: context.inherit_context === 'none' ? undefined : context
    });
    
    // Combine fresh context with existing context
    context = combineContexts(context, freshContext);
  }
  
  return context;
}
```

During task execution, the final merged configuration is passed to the Evaluator, which applies the settings accordingly.

## Task Template Matching

Template matching is a selection process that occurs before and separate from execution:

- **Selection Process**: Matches natural language task descriptions to appropriate atomic task templates
- **Scoring Mechanism**: Uses associative matching to compute similarity scores
- **Context Awareness**: May use task context to improve matching accuracy
- **No Execution Connection**: Completely separate from execution environment or variable binding
- **Scope Limitation**: Applies only to atomic task templates, not composite task templates

Template matching exclusively answers "which atomic task template should handle this task?" and has no role in variable resolution or execution. While the TaskLibrary can store templates for any task type (atomic, sequential, reduce, etc.), only atomic task templates participate in the template matching process.

For further details on context handling and related design decisions, see [ADR 002 - Context Management](../../system/architecture/decisions/002-context-management.md), [ADR 005 - Context Handling](../../system/architecture/decisions/005-context-handling.md), and [ADR 14 - Operator Context Configuration](../../system/architecture/decisions/14-operator-ctx-config.md).

#### Matching Call Chain

```mermaid
flowchart TD
    A[User provides task description]
    B[Construct ContextGenerationInput]
    C{Disable Context?}
    C -- Yes --> D[Omit inheritedContext]
    C -- No --> E[Include parent context]
    D & E --> F[Call MemorySystem.getRelevantContextFor]
    F --> G[Receive AssociativeMatchResult]
    G --> H[Compute similarity scores for each candidate]
    H --> I[Select highest-scoring atomic task template]
    I --> J[Return template for execution]
```

### Context Constraint Validation

The system enforces a mutual exclusivity constraint between fresh context generation and context inheritance:

```typescript
function validateContextSettings(settings) {
  // Check for mutual exclusivity violation
  if ((settings.inheritContext === 'full' || settings.inheritContext === 'subset') 
      && settings.freshContext === 'enabled') {
    return {
      valid: false,
      error: 'Context constraint violation: fresh_context="enabled" cannot be combined with inherit_context="full" or inherit_context="subset"'
    };
  }
  
  // Check for empty context warning
  if (settings.inheritContext === 'none' && !settings.accumulateData && settings.freshContext === 'disabled') {
    return {
      valid: true,
      warning: 'Warning: Task will execute with minimal context (no inheritance, no accumulation, no fresh context)'
    };
  }
  
  return { valid: true };
}
```

## Resource Management

The Task System enforces resource limits via a per‑Handler turn counter and context window monitoring. For the complete low‑level implementation (including code examples and configuration details), please refer to [resource-management.md](./resource-management.md).
  
### Context Window Management
- Token counting approach
- Size limit enforcement
- No optimization strategy
- Window usage monitoring
- Clear limit boundaries
  
### Limit Enforcement Strategy
- Immediate termination on violation
- Resource exhaustion error generation
- Clean session cleanup
- Resource usage reporting
- Clear violation metrics
  
### Error Detection Mechanisms
- Resource limit monitoring, progress tracking, output and XML structure validation, and input validation.

## Variable and Parameter Management

The system maintains clear separation between two distinct mechanisms:

### 1. Lexical Environment
- **Purpose**: Maintains DSL variable bindings and scoping
- **Structure**: Chain of environments with parent references
- **Operations**: Variable lookups, environment extension
- **Scope**: Isolated between function calls

### 2. Direct Parameter Passing
- **Purpose**: Transfers data between caller and template
- **Mechanism**: Arguments evaluated in caller's scope, bound to parameters in template's scope
- **Execution**: Templates operate only on their parameters
- **Isolation**: No implicit access between scopes

This clean separation prevents unexpected variable leakage and ensures predictable execution.

## Template Substitution Process

The system implements a standardized template substitution process in the Evaluator component, which resolves `{{variable_name}}` placeholders before dispatching tasks to the Handler:

### 1. Function-Based Templates
- For templates with declared parameters (`<template name="example" params="param1,param2">`):
- Substitution is limited to only the declared parameters
- The template has no access to other variables in the parent scope
- Example: `<description>Process {{param1}} with {{param2}}</description>`

### 2. Standard Templates
- For templates without explicit parameter declarations:
- Substitution uses variables from the current lexical environment
- Variables are resolved through the Environment.find() method
- Example: `<description>Process {{data_file}} with {{options}}</description>`

### Implementation in Evaluator

The Evaluator has a formal substitution phase in its execution pipeline:

```typescript
// In Evaluator component
async function executeTask(task: Task, environment: Environment): Promise<TaskResult> {
  try {
    // Process and resolve all template variables
    const resolvedTask = resolveTemplateVariables(task, environment);
    
    // Execute task using Handler with fully resolved content
    const result = await handler.executePrompt(
      resolvedTask.systemPrompt,
      resolvedTask.taskPrompt
    );
    
    // Process and return results
    return processResult(result, task);
  } catch (error) {
    if (error.message.includes('Variable resolution error')) {
      return createTaskFailure(
        'template_resolution_failure',
        error.message,
        { task: task.description }
      );
    }
    throw error;
  }
}

// Template variable resolution
function resolveTemplateVariables(task: Task, env: Environment): Task {
  const resolvedTask = {...task};
  
  if (task.isFunctionTemplate && task.parameters) {
    // For function templates, create isolated environment with only parameters
    const funcEnv = new Environment({});
    for (const param of task.parameters) {
      funcEnv.bindings[param] = env.find(param);
    }
    resolvedTask.taskPrompt = substituteVariables(task.taskPrompt, funcEnv);
  } else {
    // For standard templates, use the full environment
    resolvedTask.taskPrompt = substituteVariables(task.taskPrompt, env);
  }
  
  return resolvedTask;
}
```

The Evaluator handles variable resolution before passing fully resolved content to the Handler. This ensures all placeholders are substituted prior to LLM execution, with appropriate error handling for missing variables.

For Director-Evaluator loops, parameters are passed explicitly:
```typescript
// Director-Evaluator Loop with Evaluator handling template substitution
async function executeDirectorEvaluatorLoop(task, inputs) {
  // In the Evaluator component:
  
  // 1. Prepare director inputs
  const directorInputs = {
    ...inputs,
    feedback: previousEvaluation?.feedback,
    current_iteration: currentIteration
  };
  
  // 2. Resolve all template variables in the director task
  const resolvedDirectorTask = resolveTemplateVariables(task.director, directorInputs);
  
  // 3. Execute director with fully resolved content
  const directorOutput = await handler.executePrompt(
    resolvedDirectorTask.systemPrompt,
    resolvedDirectorTask.taskPrompt
  );
  
  // 4. Prepare evaluator inputs
  const evaluatorInputs = {
    solution: directorOutput.content,
    original_prompt: inputs.original_prompt
  };
  
  // 5. Resolve all template variables in the evaluator task
  const resolvedEvaluatorTask = resolveTemplateVariables(task.evaluator, evaluatorInputs);
  
  // 6. Execute evaluator with fully resolved content
  const evaluationResult = await handler.executePrompt(
    resolvedEvaluatorTask.systemPrompt,
    resolvedEvaluatorTask.taskPrompt
  );
  
  // Resume loop with new parameters
  return continueExecution(task, {
    ...inputs,
    director_result: directorOutput,
    evaluation_result: evaluationResult
  });
}
```

## Subtask Spawning Implementation

The Task System implements a standardized subtask spawning mechanism that enables dynamic task creation and composition.

### Request Structure

```typescript
interface SubtaskRequest {
  // Required fields
  type: TaskType;                      // Type of subtask to spawn
  description: string;                 // Description of the subtask
  inputs: Record<string, any>;         // Input parameters for the subtask
  
  // Optional fields
  template_hints?: string[];           // Hints for template selection
  context_management?: {               // Override default context settings
    inherit_context?: 'full' | 'none' | 'subset';
    accumulate_data?: boolean;
    accumulation_format?: 'notes_only' | 'full_output';
    fresh_context?: 'enabled' | 'disabled';
  };
  max_depth?: number;                  // Override default max nesting depth
  subtype?: string;                    // Optional subtype for atomic tasks
}
```

### Execution Flow

The subtask spawning process follows four main steps:

1. **Validation**
   - Validates the SubtaskRequest structure
   - Checks nesting depth against maximum allowed
   - Performs cycle detection to prevent recursive spawning
   - Validates input parameters

2. **Template Matching**
   - Uses the description and template_hints for associative matching
   - Selects the highest-scoring template that matches the request
   - Falls back to default templates if no specific match is found

3. **Subtask Creation**
   - Creates a new execution environment with direct parameter passing
   - Applies context management settings (defaults or overrides)
   - Prepares resource tracking linked to the parent task

4. **Execution and Result Handling**
   - Executes the subtask with appropriate resource limits
   - Passes the complete TaskResult back to the parent task
   - Handles errors with standardized error structures
   - Ensures proper cleanup of resources

### Depth Control Implementation

To prevent infinite recursion and resource exhaustion, the system implements depth control:

```typescript
async function executeTaskWithDepthControl(
  request: SubtaskRequest, 
  parentContext: ExecutionContext,
  currentDepth: number = 0
): Promise<TaskResult> {
  // Check maximum nesting depth
  const maxDepth = request.max_depth ?? DEFAULT_MAX_NESTING_DEPTH;
  if (currentDepth >= maxDepth) {
    throw new Error({
      type: 'TASK_FAILURE',
      reason: 'execution_halted',
      message: `Maximum nesting depth (${maxDepth}) exceeded`
    });
  }
  
  // Perform cycle detection
  if (detectCycle(request, parentContext.executionPath)) {
    throw new Error({
      type: 'TASK_FAILURE',
      reason: 'execution_halted',
      message: 'Cycle detected in subtask spawning'
    });
  }
  
  // Get context based on settings, with support for explicit file paths
  const contextSettings = getContextSettings(request);
  if (contextSettings.inheritContext === 'subset' && 
      request.file_paths && request.file_paths.length > 0) {
    // Use explicit file paths when provided
    parentContext.context = await getContextForFiles(request.file_paths, parentContext);
  }
  
  // Execute subtask with incremented depth
  try {
    return await executeTask(request, {
      ...parentContext,
      nestingDepth: currentDepth + 1,
      executionPath: [...parentContext.executionPath, getTaskSignature(request)]
    });
  } catch (error) {
    // Wrap error in standardized subtask failure structure
    throw {
      type: 'TASK_FAILURE',
      reason: 'subtask_failure',
      message: `Subtask "${request.description}" failed`,
      details: {
        subtaskRequest: request,
        subtaskError: error,
        nestingDepth: currentDepth + 1,
        partialOutput: error.details?.partialOutput
      }
    };
  }
}
```

### Context Management Integration

Subtasks use a hybrid configuration approach with default settings that can be overridden:

```typescript
// Default context management settings for subtasks
const DEFAULT_SUBTASK_CONTEXT_SETTINGS = {
  inherit_context: 'subset',
  accumulate_data: false,
  accumulation_format: 'notes_only',
  fresh_context: 'enabled'
};

/**
 * Retrieves context for specific file paths
 * @param filePaths Array of file paths to include in context
 * @param parentContext Parent execution context
 * @returns Context containing only the specified files
 */
async function getContextForFiles(
  filePaths: string[], 
  parentContext: ExecutionContext
): Promise<ExecutionContext> {
  // Filter the parent context to only include the specified files
  // This implementation assumes the memory system can retrieve specific files
  const fileContexts = await Promise.all(
    filePaths.map(path => memorySystem.getFileContext(path))
  );
  
  // Combine the file contexts into a single context
  return {
    ...parentContext,
    files: fileContexts.filter(Boolean) // Remove any null/undefined entries
  };
}

function getContextSettings(request: SubtaskRequest): ContextSettings {
  // If context_management is provided in the request, merge with defaults
  if (request.context_management) {
    return {
      ...DEFAULT_SUBTASK_CONTEXT_SETTINGS,
      ...request.context_management
    };
  }
  
  // Otherwise use defaults
  return DEFAULT_SUBTASK_CONTEXT_SETTINGS;
}
```

### Parent-Child Communication

When a parent task spawns a subtask, the communication follows this pattern:

1. Parent task returns a CONTINUATION status with subtask_request in notes
2. Task System validates the request and spawns the subtask
3. Subtask executes with its own resource tracking
4. Subtask result is passed back to the parent task when it resumes
5. Parent task continues execution with the subtask result available as a parameter

This approach ensures clear data flow and explicit dependencies between parent and child tasks.

For historical context and decision rationale behind this implementation approach, see [ADR 11: Subtask Spawning Mechanism].

### Script Execution Implementation
The system now supports executing external scripts as part of a static director-evaluator workflow. When a script_execution element is specified:

1. The script receives the Director's output as direct input
2. Script execution captures stdout, stderr, and exit code
3. These outputs are passed as direct parameters to the Evaluator
4. No environment variables are used in this data flow

This design ensures that the director's output flows seamlessly through the script execution step before final evaluation, using explicit parameter passing throughout.

## Integration Points
### Memory System Interaction

#### Clear Responsibility Boundaries
- **Memory System** (read-only metadata):
  - Maintains metadata index (file paths and descriptions)
  - Provides associative matching based on metadata
  - Never performs file I/O operations
  - Does not store or parse file contents
  - Follows read-only context model (no updateContext capability)

- **Handler** (all file operations):
  - Reads/writes files when needed
  - For Anthropic models: Configures computer use tools (optional)
  - For other models: Uses appropriate file access mechanisms
  - Handles all file system interaction
  
### Compiler Integration
- Task parsing services
- XML validation
- Schema conformance
- Error surfacing
- Validation feedback
  
### Evaluator Support
- Error surfacing
- Reparse template support
- No retry management
- State preservation
- Recovery guidance
  
### LLM Session Management
- Handler encapsulation
- Resource tracking
- Model selection support
- Clean termination
- Session isolation
</file>
<file path="./components/task-system/spec/requirements.md" project="">
# Task System Requirements

This document outlines the high-level functional and non‑functional requirements for the Task System.

## Key Requirements Summary

1. **XML-based Task Templates:**  
   - All task templates must conform to [Contract:Tasks:TemplateSchema:1.0].  
   - Both LLM‑generated and manual XML structures are supported, with options to disable reparsing.

2. **Handler and Resource Management:**  
   - One Handler is created per task execution with immutable configuration and strict resource tracking.
   - Resource limits (turn counts, context window) are enforced by the Handler. For details, see [Pattern:ResourceManagement:1.0].

3. **Task Matching:**  
   - The system matches natural language inputs and AST nodes to candidate task templates (up to 5 candidates) using numeric scoring.

4. **Memory Integration:**  
   - The Task System uses a read‑only Memory System interface for context retrieval. See [Interface:Memory:3.0].

5. **Script Execution Support:**  
   - XML schema extensions support specifying external command details for script tasks. Clear input/output contracts must be defined.

For detailed operational behavior, see the Behaviors document. Additional system‑level details are available in the central contracts and architecture documents.

## See Also

 - [Task System Behaviors](behaviors.md)
 - [Task System Interfaces](interfaces.md)
 - System-level contracts: [Contract:Tasks:TemplateSchema:1.0] and [Pattern:ResourceManagement:1.0]
</file>
<file path="./components/task-system/spec/behaviors.md" project="">
# Task System Behaviors

This document describes the runtime behaviors of the Task System.

## Overview

The Task System is responsible for managing LLM task execution, including:
 - Template matching and management,
 - Lifecycle handling for Handlers,
 - XML processing with graceful degradation, and
 - Error detection and recovery.

## Key Behavior Highlights

1. **Template Management:**  
   - Task templates are stored and validated against the XML schema defined in [Contract:Tasks:TemplateSchema:1.0].  
   - The system matches both natural language inputs and AST nodes to candidate templates (up to 5 candidates), using numeric scoring.

2. **Template Variable Substitution:**
   - The Evaluator is solely responsible for all template variable substitution.
   - This includes resolving all {{variable_name}} placeholders before passing tasks to Handlers.
   - Different resolution rules apply for function templates vs. standard templates.
   - Variable resolution errors are detected early and handled at the Evaluator level.

3. **Handler Lifecycle:**  
   - A new Handler is created for each task execution with an immutable configuration.
   - The Handler enforces resource limits (turn counts, context window limits) as described in [Pattern:ResourceManagement:1.0].
   - Handlers receive fully resolved content with no remaining template variables.

4. **XML Processing:**  
   - Basic structural validation is performed, with warnings generated for non‑critical issues.
   - In cases of partial XML parsing failure, the original content is preserved and error details are included in the task notes.

5. **Error Handling:**  
   - Errors such as RESOURCE_EXHAUSTION, TASK_FAILURE, and template resolution failures are surfaced according to the rules defined in [Pattern:Error:1.0].
   - The Evaluator delegates error recovery (e.g., reparse tasks) and includes partial outputs when available.
   - Template variable resolution errors are handled specifically with `template_resolution_failure` reason codes.

## See Also

 - [Task System Requirements](requirements.md)
 - [Task System Interfaces](interfaces.md)
 - System-level error handling: [Pattern:Error:1.0]

### Interactive Sessions
- Handler implements a standardized tool-based approach for user input requests
- The system registers a `requestUserInput` tool during Handler initialization
- Input Flow:
  ```
  LLM -> Calls requestUserInput tool -> Handler detects tool call ->
  onRequestInput called with prompt -> User input returned ->
  Handler adds user message to session -> Conversation continues
  ```
- Sessions track all conversation turns and message history
- User messages are added to the session but don't increment turn counters
- Assistant messages increment turn counters and are tracked for resource limits
- Context window includes full conversation history managed by HandlerSession
- HandlerPayload includes all context, messages, and available tools

### XML Processing
- Basic structural validation only
- Warning generation for validation issues
- Lenient output parsing with fallback
- Graceful handling of partially valid XML
- Support for manual XML task flag

## Task Execution

### Standard Task Execution
- Process tasks using appropriate templates
- Return both output and notes sections in TaskResult structure
- Surface errors for task failure or resource exhaustion
- Support specialized execution paths for reparsing and memory tasks
- Implement lenient XML output parsing with fallback to single string
- Generate warnings for malformed XML without blocking execution
- Include 'data usage' section in task notes as specified by system prompt

### Output Format Handling
- Format declaration via `<output_format>` element with required `type` attribute
- Supported format types:
  * "json" - Structured JSON data
  * "text" - Plain text (default)
- Optional `schema` attribute for type validation:
  * "object" - JSON object
  * "array" or "[]" - JSON array
  * "string[]" - Array of strings
  * "number" - Numeric value
  * "boolean" - Boolean value
- Automatic JSON detection:
  * Attempts to parse content as JSON when type="json"
  * Adds parsed content to TaskResult as parsedContent property
  * Falls back to original string content if parsing fails
- Type validation process:
  * Validates parsed content against schema attribute
  * Generates error if type mismatch occurs
  * Preserves original content in error details
- Template return type validation:
  * Function templates can specify return types via returns attribute
  * Return types are validated against actual output
  * Type mismatches generate validation errors

Example error structure for output_format_failure:
```typescript
{
  type: 'TASK_FAILURE',
  reason: 'output_format_failure',
  message: 'Expected output of type "array" but got "object"',
  details: {
    expectedType: "array",
    actualType: "object",
    content: "..." // The original output is now in content field
  }
}
```

## Output Structure

The Task System uses a simplified output structure:

### Atomic Tasks
- `content`: Contains all output (complete or partial)
- `notes`: Contains only metadata (never content)
- `status` indicates completion:
  * `COMPLETE`: Content is final
  * `FAILED`: Content may be partial
  * `CONTINUATION`: Content is intermediate

### Sequential Tasks
- Structure preserves step outputs with proper separation:
  ```typescript
  partialResults: [
    {
      stepIndex: number,
      content: string,      // Step output
      metadata: {           // Step metadata
        status: string,
        [key: string]: any
      }
    }
  ]
  ```

### Reduce Tasks
- Structure preserves processed inputs with proper separation:
  ```typescript
  processedResults: [
    {
      inputIndex: number,
      content: string,      // Processing output
      metadata: {           // Processing metadata
        status: string,
        [key: string]: any
      }
    }
  ]
  ```

This structure maintains clear separation between content and metadata at all levels.

### Task Template Matching

#### Human Input Matching
- Input: Natural language text + initial environment
- Process:
  - Uses matching specialized task from library
  - Considers provided files in environment
  - No access to previous executions/history
- Output: 
  - Up to 5 highest-scoring templates
  - Each match includes numeric score and template
  - Templates ordered by descending score

#### AST Node Matching
- Input: Single AST node
- Process:
  - Uses node-specific matching task
  - Only considers node-level context
  - No traversal of parent/child nodes
- Output:
  - Up to 5 highest-scoring templates
  - Each match includes numeric score and template
  - Templates ordered by descending score

**Additional Matching Details:**
- **Heuristic Matching:** The matching is performed heuristically using user-defined associative matching tasks. These tasks compute similarity scores based on fixed input/output conventions without a fixed system-wide metric.
- **Disable Context Flag:** Atomic tasks have an optional "disable context" flag available in their ContextGenerationInput. When enabled, inherited context is omitted from the matching process.
- **Optional Success Score:** Task execution results may include an optional success score in the `notes` field. This score is used for future adaptive template matching (although adaptive behavior is not part of the MVP).

#### Matching Behavior Summary

```mermaid
flowchart TD
    A[User Input: Task Description]
    B[Create ContextGenerationInput]
    C{Disable Context Flag?}
    C -- Yes --> D[Omit inherited context]
    C -- No --> E[Include inherited context]
    D & E --> F[Invoke Associative Matching Task]
    F --> G[Compute Similarity Scores]
    G --> H[Select Highest-Scoring Atomic Template]
    H --> I[Return Selected Template]
```

### Specialized Task Types

#### Reparse Tasks
- Triggered by:
  - Resource exhaustion errors
  - Invalid output errors
  - Progress failure errors
- Uses specialized templates from separate directory
- Returns new task structure or alternative approach

#### Memory Tasks
- Direct memory system access
- Uses associative matching templates
- Returns relevant context selections

## Memory System Integration

### File Operations
- **Memory System**: Manages ONLY metadata (file paths and descriptive strings)
- **Handler**: Performs ALL file I/O operations (reading, writing, deletion)
- For Anthropic models: Handler configures and uses Anthropic's computer use tools
  * computer_20250124 (or 20241022)
  * text_editor_20250124 (or 20241022)
  * bash_20250124 (or 20241022)
- All file content access is always handled by the Handler, never the Memory System

Note: The Memory System (version 3.0) follows a read-only context model with no updateContext capability.

### Context Management
- Context accessed via async getRelevantContextFor
- File metadata accessed via GlobalIndex
- Existing context preserved during task execution
- Structure/parsing handled by associative memory tasks
- Context clearing and regeneration handled through context_management settings
- Three-dimensional model (inherit_context, accumulate_data, fresh_context) provides complete control
- No additional flags or mechanisms needed for context operations

#### Context Operation Patterns

1. **Context Clearing**: Achieved by setting `inherit_context="none"` and `fresh_context="enabled"`
2. **Context Regeneration**: Achieved through `fresh_context="enabled"` with appropriate inheritance settings
3. **Complete Preservation**: Achieved with `inherit_context="full"`, `accumulate_data="true"`, and `fresh_context="disabled"`

### State Management
- No complex file metadata tracking
- Minimal state maintenance between tasks
- Clear task execution boundaries
- Simple file modification tracking

## Delegation Mechanisms

## Tool Interface
The LLM interacts with a unified tool system that presents consistent patterns regardless of implementation mechanism:

```typescript
// All tools appear similar to the LLM
tools.readFile("path/to/file");  // Direct implementation
tools.analyzeData({ data: content });  // Subtask implementation
```

### Direct Tool Implementation
- Handled by Handler component
- Deterministic with fixed APIs
- No continuation mechanism
- Used for file operations, APIs, script execution
- Examples: file access, bash commands, computer tools
- Direct execution without complex context management
- Resource tracking handled by Handler

### Subtask Tool Implementation
- LLM-to-LLM interactions
- Use CONTINUATION status with SubtaskRequest
- Full context management via Memory System
- Support complex reasoning and creative tasks
- Follow ADR 14 context management model
- Depth tracking to prevent infinite recursion
- Template selection via associative matching

See [Pattern:ToolInterface:1.0] for selection criteria and implementation details.

## File Operations

### Clear Responsibility Boundaries
- **Memory System**: Manages ONLY metadata (file paths and descriptive strings)
- **Handler**: Performs ALL file I/O operations (reading, writing, deletion)

### Implementation Details
- Handler configures and uses provider-appropriate tool mechanisms
- Tool selection is determined by provider capabilities and task requirements
- Each provider implementation handles the mapping of abstract tool types to provider-specific formats
- All file content access is always handled by the Handler, never the Memory System
- The Memory System provides only file paths that may be relevant (via associative matching)
- Context Management System decides which files to read based on policy

## Subtask Spawning Behavior

The Task System implements a standardized approach to subtask spawning:

### Request Processing

1. **Validation and Preparation**
   - Validates the SubtaskRequest structure for required fields
   - Checks nesting depth against maximum allowed (default: 5)
   - Performs cycle detection to prevent recursive spawning
   - Prepares context according to context management settings
   - Prioritizes explicit file_paths when provided with inherit_context="subset"

2. **Template Selection and Execution**
   - Uses the description and template_hints for associative matching
   - Selects the highest-scoring template that matches the request
   - Creates a new execution environment with direct parameter passing
   - Executes the subtask with appropriate resource tracking

3. **Result Handling**
   - Adds subtask result as a tool response to parent's Handler session
   - Parent task continues execution with the tool result in its conversation history
   - No special resumption methods or partial results tracking needed
   - Handler session remains preserved throughout the subtask execution
   - From the LLM's perspective, subtasks appear as normal tool calls and responses

### Explicit File Selection

When creating subtasks, parent tasks can explicitly specify which files to include:

```typescript
// The file_paths field takes precedence over associative matching
subtask_request = {
  type: "atomic",
  description: "Analyze specific modules",
  inputs: { /* parameters */ },
  context_management: { inherit_context: "subset" },
  file_paths: ["/src/main.py", "/src/utils.py"]
}
```

### Context Management Defaults

Subtasks have specific default context management settings:

| Setting | Default Value | Description |
|---------|---------------|-------------|
| inherit_context | subset | Inherits only relevant context from parent |
| accumulate_data | false | Does not accumulate previous step outputs |
| accumulation_format | notes_only | Stores only summary information |
| fresh_context | enabled | Generates new context via associative matching |

These defaults can be overridden through explicit configuration in the SubtaskRequest.

### Error Handling

The system uses standard error types (`RESOURCE_EXHAUSTION`, `TASK_FAILURE`) without complex partial results tracking. Errors contain essential information needed for debugging without overengineering the error structure. The parent task receives clear error information and can implement appropriate recovery strategies based on error type and context.

When a subtask fails, a standardized error structure is generated:

```typescript
{
  type: 'TASK_FAILURE',
  reason: 'subtask_failure',
  message: 'Subtask execution failed',
  details: {
    subtaskRequest: {
      type: 'atomic',
      description: 'Process data',
      inputs: { /* original inputs */ }
    },
    subtaskError: {
      type: 'TASK_FAILURE',
      reason: 'execution_halted',
      message: 'Failed to process data'
    },
    nestingDepth: 2
  }
}
```

This structure provides the essential error context, allowing the parent task to implement recovery strategies if needed.

## Context Management Delegation

The Task System implements a hybrid configuration approach with operator-specific defaults and explicit overrides:

| Operator Type | inherit_context | accumulate_data | accumulation_format | fresh_context |
|---------------|-----------------|-----------------|---------------------|---------------|
| atomic        | full            | false           | notes_only          | enabled       |
| sequential    | full            | true            | notes_only          | enabled       |
| reduce        | none            | true            | notes_only          | enabled       |
| script        | full            | false           | notes_only          | disabled      |
| director_evaluator_loop | none  | true            | notes_only          | enabled       |

These defaults apply when no explicit context_management block is provided. When present, the explicit settings override the defaults:

```xml
<context_management>
    <inherit_context>full|none|subset</inherit_context>
    <accumulate_data>true|false</accumulate_data>
    <accumulation_format>notes_only|full_output</accumulation_format>
    <fresh_context>enabled|disabled</fresh_context>
</context_management>
```

The Task System delegates **all context management execution** to the Evaluator according to the final merged configuration. In other words:

1. The Task System's role is to:
   - Define task structure (sequential, map, reduce, etc.)
   - Process context management configuration (defaults + overrides)
   - Signal the Evaluator to execute steps with the final configuration

2. The Evaluator manages all three dimensions of context:
   - **inherit_context**: Controls parent context inheritance ("full", "none", or "subset").
   - **accumulate_data**: Controls accumulation of previous step outputs (true/false).
   - **accumulation_format**: Specifies storage format for accumulated data ("notes_only" or "full_output").
   - **fresh_context**: Controls whether new context is generated via associative matching ("enabled" or "disabled").

3. The Evaluator decides how and when to call `MemorySystem.getRelevantContextFor()` based on these settings.
4. The Handler remains focused on resource tracking (turns, tokens).
5. No direct context accumulation logic occurs in the Task System itself.

## File Paths Feature

The system supports an orthogonal file selection mechanism through the `file_paths` feature:

### Feature Behavior
- Available on all atomic tasks (both direct and subtasks)
- Operates outside the standard three-dimensional context model
- Specified files are always included in the context, regardless of other settings
- Files are fetched by the Handler before task execution

### Implementation Details
- Files are retrieved using Handler tools
- Both absolute paths and paths relative to repo root are supported
- Files are presented in the context with XML tags indicating their paths
- Invalid paths generate warnings but don't prevent task execution

### Integration with Context Management
- When used with `inherit_context="subset"`, specified files become the subset
- When used with `fresh_context="enabled"`, specified files are forcibly included while associative matching still runs
- The feature does not replace or disable other context management settings

### XML Representation
```xml
<task type="atomic">
  <description>Task description</description>
  <context_management>
    <!-- Standard context management settings -->
  </context_management>
  <file_paths>
    <path>./src/main.py</path>
    <path>/absolute/path/file.txt</path>
  </file_paths>
</task>
```

## Error Handling

### Error Detection and Response

#### Resource Exhaustion
- Handler detects limit violations
- Immediate task termination
- Surfaces through standard error type
- Resource accounting completion
- Clean Handler termination
- No retry attempts

#### Invalid Output
- Structural validation failure
- Format validation errors
- No semantic validation
- Preserves partial outputs for diagnostic purposes
- Returns both output and notes sections
- Detailed error reporting with context

#### Progress Failure
- Handler detects stalled execution
- Task-specific progress indicators
- No internal progress tracking
- Comprehensive error reporting
- State preserved in error response
- No automatic retry

#### XML Validation
- Immediate failure on invalid structure
- No warning collection
- Clear error messages
- Location information when available
- May trigger reparse if not disabled

#### Partial Results Preservation

The system preserves partial results for different task types to provide diagnostic context:

1. **Atomic Tasks**
   - Stores partial content in `notes.partialOutput`
   - Preserves as much of the generated output as possible
   - No structure guarantees for partial content
   - Used for error reporting and diagnostics

2. **Sequential Tasks**
   - Stores step-by-step outputs in `details.partialResults`
   - Includes metadata such as `failedStep` and `totalSteps`
   - Each step result includes `stepIndex`, `output`, and optional `notes`
   - Provides execution trace for debugging purposes

3. **Reduce Tasks**
   - Stores processed input results in `details.partialResults`
   - Includes the current accumulator state in `details.currentAccumulator`
   - Tracks processed inputs in `details.processedInputs`
   - Records the failed input index in `details.failedInputIndex`
   - Enables detailed error analysis

#### Format Control

The `accumulation_format` setting in the task's `context_management` block controls how much data is preserved:

```xml
<context_management>
    <inherit_context>full</inherit_context>
    <accumulate_data>true</accumulate_data>
    <accumulation_format>notes_only|full_output</accumulation_format>
    <fresh_context>enabled</fresh_context>
</context_management>
```

- `notes_only`: Only the notes field is preserved (default for memory efficiency)
- `full_output`: Both content and notes fields are preserved (with size limits)

#### Size Management

To prevent memory issues with partial results:
- Individual outputs are kept reasonably sized
- When accumulated data becomes too large, older results may be summarized or truncated
- The system indicates when truncation has occurred in the error details
- A maximum size cap applies regardless of settings

### Error Response

#### Error Surfacing
- Uses standard error type system (see types.md)
- Includes relevant task notes
- Preserves partial outputs when useful
- Includes resource metrics where applicable
- Clear error messages and locations

#### Handler Cleanup
- Clean termination of LLM session
- Resource accounting completion
- No state preservation
- Context window cleanup
- Turn counter finalization

#### Recovery Delegation
- No retry attempts
- Delegates to evaluator
- Provides complete error context
- Includes notes for recovery guidance
</file>
<file path="./components/task-system/spec/qa.md" project="">
# Task System FAQ

This FAQ provides concise answers to common questions about the Task System.

**Q: How many Handlers should be created per task execution?**  
A: One Handler per task execution. See [Task System Requirements](requirements.md) for details.

**Q: Who is responsible for sending prompts to the LLM?**  
A: The Handler is responsible for all LLM interactions.

**Q: How are resource limits enforced?**  
A: Resource limits are passed to the Handler at initialization and enforced during execution. For more details, see [Task System Behaviors](behaviors.md).

For additional questions, please refer to system‑level documentation or contact the Task System maintainers.
</file>
<file path="./components/task-system/spec/interfaces.md" project="">
# Task System Interfaces

// For core type definitions (e.g. TaskResult, TaskTemplate, TaskType, AtomicTaskSubtype),
// please refer to components/task-system/spec/types.md.

import { MemorySystem } from "../../memory/api/interfaces";

/**
 * TaskSystem Interface
 * 
 * Provides methods to execute tasks, validate templates, and find matching templates.
 */
export interface TaskSystem {
    /**
     * Execute a task, automatically handling continuations via tool responses
     * 
     * If a task returns CONTINUATION status with a subtask_request:
     * 1. The subtask is executed
     * 2. The result is added as a tool response to the parent's session
     * 3. The parent task continues execution with the tool result available
     * 
     * @param task - The task to execute
     * @param memory - The Memory System instance
     * @param taskType - Optional task type override
     * @returns Promise resolving to the final task result
     */
    executeTask(
        task: string,
        memory: MemorySystem,
        taskType?: "atomic" | "sequential" | "reduce" | "script"
    ): Promise<TaskResult>;

    validateTemplate(template: TaskTemplate): boolean;
    
    /**
     * findMatchingTasks
     *
     * Finds matching templates based on a provided input string.
     *
     * Note: Matching applies *only* to atomic task templates. The function evaluates the input
     * against atomic task templates using a heuristic scoring mechanism.
     * 
     * @param input - The natural language task description.
     * @param context - The MemorySystem instance providing context data.
     * @returns An array of matching candidates with their associated scores.
     */
    findMatchingTasks(
        input: string,
        context: MemorySystem
    ): Promise<Array<{
        template: TaskTemplate;
        score: number;
        taskType: "atomic" | "sequential" | "reduce" | "script";
    }>>;
    registerTask(taskDef: TaskDefinition): void;
    executeFunctionCall(funcCall: FunctionCall, env: Environment): Promise<any>;
    
    /**
     * Register a template in the TaskLibrary
     * 
     * @param template - The template to register
     * @returns Promise resolving to registration result
     */
    registerTemplate(template: TemplateNode): Promise<void>;
    
    /**
     * Execute a function call
     * 
     * @param call - The function call to execute
     * @param env - The environment for argument evaluation
     * @returns Promise resolving to the function result
     */
    executeCall(call: FunctionCallNode, env: Environment): Promise<TaskResult>;
export interface Environment {
    bindings: Record<string, any>;
    outer?: Environment;
    /**
     * Perform a lexical lookup for varName.
     * Returns the value if found; otherwise, throws an error.
     */
    find(varName: string): any;
    executeScriptTask(scriptTask: ScriptTask, env: Environment): Promise<ScriptTaskResult>;
    
    /**
     * Create a new child environment with additional bindings
     * 
     * @param bindings - New variable bindings to add
     * @returns A new Environment with the added bindings
     */
    extend(bindings: Record<string, any>): Environment;
}

// Handler interface details are maintained in external documentation.
 * Memory System interface - Version 3.0
 * Provides metadata management and context retrieval
 * Follows a read-only context model (no updateContext capability)
 */
type FileMetadata = string;

type GlobalIndex = Map<string, FileMetadata>;

type FileMatch = [string, string | undefined];

interface AssociativeMatchResult {
    context: string;      // Unstructured data context
    matches: FileMatch[]; // Relevant file matches
}

interface MemorySystem {
    // Get global file metadata index
    getGlobalIndex(): Promise<GlobalIndex>;
    
    // Update global file metadata index
    updateGlobalIndex(index: GlobalIndex): Promise<void>;
    
    // Retrieve context using associative matching
    getRelevantContextFor(input: ContextGenerationInput): Promise<AssociativeMatchResult>;
}
```

### Example Definitions

**ContextGenerationInput Example:**
```json
{
    "taskText": "Analyze experimental data",
    "inheritedContext": "Optional inherited context from previous tasks (if not disabled)",
    "previousOutputs": "Optional string summarizing prior outputs"
}
```

**AssociativeMatchResult Example:**
```json
{
    "context": "Relevant retrieved context information",
    "matches": [
        ["fileA.txt", "metadata details"],
        ["fileB.txt", null]
    ]
}
```

### Handler Interface
```typescript
/**
 * Types specific to Handler interface
 */
interface HandlerConfig {
    maxTurns: number;
    maxContextWindowFraction: number;
    defaultModel?: string;
    systemPrompt: string;
}

/**
 * LLM interaction interface
 * Uses [Type:TaskSystem:ResourceMetrics:1.0], [Type:TaskSystem:ResourceLimits:1.0]
 */
interface Handler {
    /**
     * Execute a prompt with the LLM
     * Note: All template substitution should be performed by the Evaluator before calling
     * @param systemPrompt - System-level context and instructions (fully resolved)
     * @param taskPrompt - Task-specific input (fully resolved)
     * @returns Promise resolving to LLM response
     */
    executePrompt(
        systemPrompt: string,
        taskPrompt: string
    ): Promise<string>;

    /**
     * Register a direct tool that will be executed by the Handler
     * @param name - Unique tool name
     * @param handler - Function that implements the tool
     */
    registerDirectTool(name: string, handler: Function): void;

    /**
     * Register a subtask tool that will be implemented via CONTINUATION
     * @param name - Unique tool name
     * @param templateHints - Hints for template selection
     */
    registerSubtaskTool(name: string, templateHints: string[]): void;

    /**
     * Add a tool response to the session
     * Used for adding subtask results to parent tasks
     * @param toolName - Name of the tool that produced the response
     * @param response - The tool response content
     */
    addToolResponse(toolName: string, response: string): void;

    /**
     * Callback for handling agent input requests
     * @param agentRequest - The agent's request for user input
     * @returns Promise resolving to user's input
     */
    onRequestInput: (agentRequest: string) => Promise<string>;
}
```
</file>
<file path="./components/task-system/spec/types.md" project="">
# Task System Types

// Core task types used across the Task System.
export type TaskType = "atomic" | "sequential" | "reduce" | "script" | "director_evaluator_loop";
export type AtomicTaskSubtype = "standard" | "subtask" | "director" | "evaluator";

// Task execution status.
export type ReturnStatus = "COMPLETE" | "CONTINUATION" | "FAILED";

// Core interfaces:

export interface TaskResult {
    /**
     * Output content from the task execution.
     * Contains complete output if status is COMPLETE.
     * May contain partial output if status is FAILED or CONTINUATION.
     */
    content: string;
    
    /**
     * Execution status of the task.
     * COMPLETE: Task finished successfully, content is complete
     * CONTINUATION: Task needs additional steps, content may be partial
     * FAILED: Task encountered an error, content may be partial
     */
    status: ReturnStatus;
    
    /**
     * Optional free-form description used for dynamic evaluation template selection.
     */
    criteria?: string;
    
    /**
     * Parsed content if output was successfully parsed as JSON.
     */
    parsedContent?: any;
    
    /**
     * Task-level metadata about execution, resource usage, and status.
     * Contains ONLY metadata, never content.
     */
    notes: {
        dataUsage?: string;
        successScore?: number;
        [key: string]: any;
    };
}

/**
 * Base task definition interface
 */
export interface BaseTaskDefinition {
    description: string;
    type: TaskType;
    subtype?: string;
    
    /**
     * Specific files to include in task context.
     * These files will always be included regardless of other context settings.
     * Paths can be absolute or relative to repo root.
     * Invalid paths will generate warnings but execution will continue.
     */
    file_paths?: string[];
    
    context_management?: ContextManagement;
    inputs?: Record<string, any>;
}

/**
 * Represents a sequential task which has its own context management block
 * and multiple steps of subtasks.
 */
interface SequentialTask extends BaseTask {
    type: 'sequential';
    contextManagement: ContextManagement;
    steps: Task[];
}

/**
 * A general-purpose task result, now updated to store optional string notes
 * or structured data. This may override or augment the existing TaskResult
 * if needed, but is shown here as the revision plan states.
 */
interface RevisedTaskResult {
    content: string;
    notes: string;
}

/**
 * Defines context management settings using the standardized three-dimensional model.
 * Note: fresh_context="enabled" cannot be combined with inherit_context="full" or "subset"
 */
export interface ContextManagement {
    inheritContext: 'full' | 'none' | 'subset';
    accumulateData: boolean;
    /**
     * Controls what information is preserved during sequential task execution:
     * - notes_only: Only the notes field is preserved (default)
     * - full_output: Both content and notes fields are preserved
     */
    accumulationFormat: 'full_output' | 'notes_only';
    freshContext: 'enabled' | 'disabled';
}

/**
 * Default context management settings for subtasks
 */
export const SUBTASK_CONTEXT_DEFAULTS: ContextManagement = {
    inheritContext: 'none',
    accumulateData: false,
    accumulationFormat: 'notes_only',
    freshContext: 'enabled'
};

/**
 * Input structure for Memory System context requests
 */
interface ContextGenerationInput {
    previousOutputs?: string;   // Outputs accumulated from previous steps
    inheritedContext?: string;  // Context inherited from parent tasks
    taskText: string;          // The primary task description or request
}

/**
 * Task Template Interface
 * 
 * Any task type (atomic, sequential, reduce, script, etc.) can be defined as a template.
 * However, only atomic task templates participate in the template matching process.
 * Composite tasks can be either defined directly, defined as templates, or 
 * constructed by combining multiple atomic task templates.
 */
interface TaskTemplate {
    readonly taskPrompt: string;      // Maps to <instructions> in schema
    readonly systemPrompt: string;    // Maps to <system> in schema
    readonly provider?: string;       // Maps to <provider> in schema
    readonly model?: string;          // Maps to <model> in schema
    readonly inputs?: Record<string, string>;
    readonly isManualXML?: boolean;   // Maps to <manual_xml> in schema
    readonly disableReparsing?: boolean; // Maps to <disable_reparsing> in schema
    readonly taskType?: TaskType;     // The type of task this template defines
    readonly atomicSubtype?: AtomicTaskSubtype; // Only for atomic task templates
}
```

## AST Types

export interface FunctionCall extends ASTNode {
    funcName: string;
    args: ASTNode[];
}
```typescript
interface OperatorSpec {
    type: TaskType;
    subtype?: AtomicTaskSubtype;
    inputs: Record<string, string>;
    disableReparsing?: boolean;
}

interface ASTNode {
    type: string;
    content: string;
    children?: ASTNode[];
    metadata?: Record<string, any>;
    operatorType?: TaskType;
    outputFormat?: {
        type: "json" | "text";
        schema?: string;  // Basic type: "object", "array", "string", etc.
    };
}
```

export interface EvaluationResult {
    success: boolean;
    feedback?: string;
}


/**
 * Lexical Environment for variable scoping
 * Responsible only for variable bindings and lookups
 */
export interface Environment {
    /**
     * Current variable bindings at this scope level
     */
    bindings: Record<string, any>;
    
    /**
     * Reference to outer/parent environment for lexical lookup chain
     */
    outer?: Environment;
    
    /**
     * Perform a lexical lookup for varName in this environment chain
     * @param varName Variable name to look up
     * @returns The variable value if found
     * @throws Error if variable not found in this or any outer environment
     */
    find(varName: string): any;
    
    /**
     * Create a new child environment with additional bindings
     * @param bindings New variable bindings to add
     * @returns A new Environment with the added bindings
     */
    extend(bindings: Record<string, any>): Environment;
}

/**
 * Script task executor interface
 */
export interface ScriptExecutor {
    /**
     * Execute a script task
     * @param scriptTask Script task definition
     * @param env Environment for variable resolution
     * @returns Promise resolving to script execution result
     */
    executeScriptTask(scriptTask: ScriptTask, env: Environment): Promise<ScriptTaskResult>;
}


## Resource Management Types

export interface TaskDefinition {
    name: string;                     // Unique task identifier
    type: TaskType;                   // e.g., "atomic" or "sequential"
    subtype?: string;                 // Optional subtype, e.g., "director", "evaluator", etc.
    provider?: string;                // Optional LLM provider
    model?: string;                   // Optional LLM model
    
    // Function template aspects (optional)
    parameters?: string[];            // Parameter names in order (if function template)
    returns?: string;                 // Optional return type information
    
    // Common elements
    body: ASTNode;                    // Task implementation
    metadata?: Record<string, any>;   // Additional metadata
}

/**
 * TaskLibrary manages the registration and retrieval of task definitions
 */
export class TaskLibrary {
    private tasks: Map<string, TaskDefinition>;

    constructor() {
        this.tasks = new Map();
    }

    /**
     * Register a task definition
     * @param taskDef Task definition to register
     * @throws Error if task with same name already exists
     */
    public registerTask(taskDef: TaskDefinition): void {
        if (this.tasks.has(taskDef.name)) {
            throw new Error(`Task ${taskDef.name} is already registered.`);
        }
        this.tasks.set(taskDef.name, taskDef);
    }
    
    /**
     * Retrieve a task definition by name
     * Used for execution and function calling
     * @param name Name of the task to retrieve
     * @returns The task definition
     * @throws Error if task not found
     */
    public getTask(name: string): TaskDefinition {
        const taskDef = this.tasks.get(name);
        if (!taskDef) {
            throw new Error(`Task ${name} not found in TaskLibrary.`);
        }
        return taskDef;
    }
    
    /**
     * Find matching tasks based on description
     * Used for task matching during execution
     * @param description Task description to match against
     * @param context Optional context to aid matching
     * @returns Array of matching tasks with scores
     */
    public findMatchingTasks(description: string, context?: any): Promise<Array<{
        task: TaskDefinition;
        score: number;
    }>> {
        // Implementation details omitted
    }
}
```typescript
interface HandlerConfig {
    maxTurns: number;
    maxContextWindowFraction: number;
    defaultModel?: string;
    systemPrompt: string;
}

/**
 * Represents the result of executing a script task.
 */
interface ScriptTaskResult extends TaskResult {
    stdout: string;
    stderr: string;
    exitCode: number;
}

interface ResourceMetrics {
    turns: {
        used: number;
        limit: number;
        lastTurnAt: Date;
    };
    context: {
        used: number;
        limit: number;
        peakUsage: number;
    };
}

interface ResourceLimits {
    maxTurns: number;
    maxContextWindow: number;
    warningThreshold: number;
    timeout?: number;
}

/**
 * EvaluationInput interface clarifies that target_content refers to the original output from the Director task.
 * The raw outputs from the script (stdout, stderr, exit_code) are passed directly without preprocessing.
 */
interface EvaluationInput {
    target_content: string; // The original output from the Director task.
    stdout?: string;        // Raw standard output from the script.
    stderr?: string;        // Raw standard error output from the script.
    exit_code?: number;     // Script exit code (non-zero exit codes do not block evaluation but inform decision-making).
}
```


## Error Types
```typescript
type TaskError = 
    | { 
        type: 'RESOURCE_EXHAUSTION';
        resource: 'turns' | 'context' | 'output';
        message: string;
        metrics?: { used: number; limit: number; };
        content?: string;  // May contain partial output before exhaustion
    }
    | { 
        type: 'TASK_FAILURE';
        reason: TaskFailureReason;
        message: string;
        content?: string;  // Contains partial output if available
        notes?: Record<string, any>;  // Contains task metadata
        details?: {
            // Common fields
            partial_context?: any;
            context_metrics?: any;
            violations?: string[];
            
            // Sequential task fields
            failedStep?: number;
            totalSteps?: number;
            partialResults?: Array<{
                stepIndex: number;
                content: string;  // Step output
                notes: any;       // Step metadata
            }>;
            
            // Reduce task fields
            failedInputIndex?: number;
            totalInputs?: number;
            processedInputs?: number[];
            currentAccumulator?: any;
        };
    }
    | { 
        type: 'INVALID_OUTPUT';
        message: string;
        content?: string;  // The invalid output
        violations?: string[];
    }
    | { 
        type: 'VALIDATION_ERROR';
        message: string;
        path?: string;
        invalidModel?: boolean;
    }
    | { 
        type: 'XML_PARSE_ERROR';
        message: string;
        location?: string;
        content?: string;  // The unparseable content
    };
```

## Validation Types
```typescript
// Add a type for sequential history tracking
export interface SequentialHistory {
    outputs: TaskOutput[];
    metadata: {
        startTime: Date;
        currentStep: number;
        resourceUsage: ResourceMetrics;
    };
}

export interface TaskOutput {
    stepId: string;  // or step index
    notes: any;      // Task notes field (always preserved, may include partial results)
    timestamp: Date;
}

interface ValidationResult {
    valid: boolean;
    warnings: string[];
    errors?: string[];
    location?: string;
}

interface XMLValidation extends ValidationResult {
    xmlValid: boolean;
    schemaValid: boolean;
    parsedContent?: any;
}

interface TemplateValidation extends ValidationResult {
    templateValid: boolean;
    modelValid: boolean;
    inputsValid: boolean;
}
```

## Cross-References
- For XML schema definitions, see [Contract:Tasks:TemplateSchema:1.0] in protocols.md
- For interface implementations, see spec/interfaces.md
- For public API surface, see api/interfaces.md

## Notes
1. All types supporting the core task system are defined here
2. Public API types are a subset of these definitions
3. Implementation details for memory system metadata types pending definition
4. All resource limits and metrics are enforced per-Handler
/**
 * Represents a director_evaluator_loop task type with explicit
 * director, evaluator, and optional script_execution components.
 */
interface DirectorEvaluatorLoopTask extends BaseTask {
    type: 'director_evaluator_loop';
    contextManagement: ContextManagement;
    maxIterations?: number;
    director: Task;
    evaluator: Task;
    scriptExecution?: ScriptExecution;
    terminationCondition?: string;
}

/**
 * Specialized result structure for evaluator feedback
 */
export interface EvaluationResult extends TaskResult {
    notes: {
        success: boolean;        // Whether the evaluation passed
        feedback: string;        // Human-readable feedback message
        details?: {              // Optional structured details
            metrics?: Record<string, number>; // Optional evaluation metrics
            violations?: string[];            // Specific validation failures
            suggestions?: string[];           // Suggested improvements
            [key: string]: any;               // Extension point
        };
        scriptOutput?: {         // Present when script execution is involved
            stdout: string;      // Standard output from script
            stderr: string;      // Standard error output from script
            exitCode: number;    // Exit code from script
        };
    };
}

/**
 * Tool Call - Handler-managed operations with deterministic APIs
 * Distinct from subtasks (LLM-to-LLM interactions via continuation)
 */
export interface ToolCall {
  name: string;
  input: Record<string, any>;
}

/**
 * Subtask Request - Used for LLM-to-LLM delegation via continuation
 * Distinct from tool calls (deterministic operations without continuation)
 */
export interface SubtaskRequest {
    /** Type of subtask to spawn */
    type: TaskType;
    
    /** Description of the subtask */
    description: string;
    
    /** Input parameters for the subtask */
    inputs: Record<string, any>;
    
    /** Optional hints for template selection */
    template_hints?: string[];
    
    /** Optional context management overrides */
    context_management?: {
        inherit_context?: 'full' | 'none' | 'subset';
        accumulate_data?: boolean;
        accumulation_format?: 'notes_only' | 'full_output';
        fresh_context?: 'enabled' | 'disabled';
    };
    
    /** Optional maximum nesting depth override */
    max_depth?: number;
    
    /** Optional subtype for atomic tasks */
    subtype?: string;
  
    /** 
     * Specific files to include in subtask context.
     * These files will always be included regardless of other context settings.
     * Paths can be absolute or relative to repo root.
     * Invalid paths will generate warnings but execution will continue.
     */
    file_paths?: string[];
}

/**
 * Error structure for subtask failures.
 * Preserves the complete error context for potential recovery.
 */
export interface SubtaskFailureError extends TaskError {
    type: 'TASK_FAILURE';
    reason: 'subtask_failure';
    message: string;
    details: {
        /** The original subtask request */
        subtaskRequest: SubtaskRequest;
        
        /** The error from the subtask */
        subtaskError: TaskError;
        
        /** Current nesting depth */
        nestingDepth: number;
    };
}

/**
 * Script execution configuration
 */
export interface ScriptExecution {
    command: string;
    timeout?: number;
    inputs: Record<string, string>;
}


/**
 * Represents a function call expression
 */
export interface FunctionCallNode extends ASTNode {
    type: "call";
    taskName: string;
    arguments: ArgumentNode[];  // Evaluated in caller's environment
}

/**
 * Represents an argument to a function call
 */
export interface ArgumentNode extends ASTNode {
    type: "argument";
    value: string | ASTNode;  // String for variables/literals, ASTNode for nested
}

/**
 * Handler session interface for managing conversation state
 */
export interface HandlerSession {
    /**
     * Add a user message to the session
     */
    addUserMessage(content: string): void;
    
    /**
     * Add an assistant message to the session
     */
    addAssistantMessage(content: string): void;
    
    /**
     * Add a tool response to the session
     * Used for adding subtask results as tool responses
     */
    addToolResponse(toolName: string, content: string): void;
    
    /**
     * Construct a payload for the LLM with current session state
     */
    constructPayload(systemPrompt: string, userMessage: string): any;
    
    /**
     * Get resource metrics for the session
     */
    getResourceMetrics(): ResourceMetrics;
}
</file>
<file path="./components/task-system/README.md" project="">
# Task System Component

## Overview

The Task System orchestrates LLM task execution through structured XML templates and handlers. It proovides template-based task definition, resource tracking, and an XML-based interface with the LLM.

## Core Architecture

The system manages task execution through isolated Handler instances, with one Handler per task to enforce resource limits and manage LLM interactions. 

Task definitions use an XML-based template system that supports both manual and LLM-generated structures. 

## Core Interface

```typescript
interface TaskSystem {
    executeTask(
        task: string,
        context: MemorySystem,
        taskType?: TaskType
    ): Promise<TaskResult>;

    validateTemplate(template: TaskTemplate): boolean;
    
    findMatchingTasks(
        input: string,
        context: MemorySystem
    ): Promise<Array<{
        template: TaskTemplate;
        score: number;
        taskType: TaskType;
    }>>;
}
```

## Task Types and Execution

The system supports several task types for different execution patterns. Atomic tasks provide direct LLM execution with resource tracking and output validation. Sequential tasks enable ordered execution with context management between steps. Reduce tasks handle iterative data processing with accumulator management. Script tasks support external command execution with output capture and evaluation flow integration.

For all task types, the Evaluator is responsible for resolving template variables (e.g., `{{variable_name}}` placeholders) before passing fully-resolved content to the Handler for execution. This ensures consistent variable resolution across all task types and execution paths.

All task types (atomic, sequential, reduce, script) can be defined as templates and functions in the TaskLibrary. However, template matching (the process of selecting an appropriate template based on a natural language description) applies only to atomic task templates. Composite tasks can be defined directly, defined as reusable templates, or assembled from matched atomic task templates.

Each task type can specify its context management requirements and explicit file paths through XML configuration:

```xml
<task>
    <description>Task description</description>
    <context_management>
        <inherit_context>none|full|subset</inherit_context>
        <accumulate_data>true|false</accumulate_data>
        <accumulation_format>notes_only|full_output</accumulation_format>
        <fresh_context>enabled|disabled</fresh_context>
    </context_management>
    <file_paths>
        <path>./src/main.py</path>
        <path>/absolute/path/file.txt</path>
    </file_paths>
    <inputs>
        <input name="input_name" from="source_var"/>
    </inputs>
</task>
```

### Function Templates

The system supports function-based templates with explicit parameter declarations:

```xml
<template name="analyze_data" params="dataset,config">
  <task>
    <description>Analyze {{dataset}} using {{config}}</description>
  </task>
</template>
```

These templates are called with positional arguments:

```xml
<call template="analyze_data">
  <arg>weather_data</arg>
  <arg>standard_config</arg>
</call>
```

Key characteristics:

- Templates can only access explicitly passed parameters
- Arguments are evaluated in the caller's environment
- Function calls create a new lexical scope
- Template registration happens automatically during parsing

## Delegation Mechanisms

The system exposes a unified tool interface with two implementation approaches:

1. **Unified Tool Interface**: What the LLM sees and interacts with
   - Consistent invocation patterns for all operations
   - Standardized parameter schemas
   - Unified error handling

2. **Implementation Mechanisms**: How tools are executed
   - **Direct Implementation**: Synchronous Handler execution for simple operations
   - **Subtask Implementation**: Asynchronous execution via CONTINUATION

See [Pattern:ToolInterface:1.0] for complete details on this approach.

## Integration and Dependencies

The Task System integrates with several core components. It uses the Memory System for context access and management, Handler Tools for file and system operations, the Compiler for task parsing and transformation, and the Evaluator for error recovery and task decomposition. These integrations enable comprehensive task execution while maintaining clean component boundaries.

## Usage

Here's how the Task System would be instantiated:

```typescript
const taskSystem = new TaskSystem({
    provider: "anthropic",  // Default provider
    maxTurns: 10,
    maxContextWindowFraction: 0.8,
    systemPrompt: "Default system prompt"
});

// Execute a task
const result = await taskSystem.executeTask(
    "analyze data",
    memorySystem
);

// Execute with provider override
const resultWithOverride = await taskSystem.executeTask(
    "analyze data",
    memorySystem,
    { provider: "openai" }  // Override for this task
);

// Execute task with specific file paths
const resultWithFiles = await taskSystem.executeTask(
    "<task type='atomic'><description>Analyze files</description><file_paths><path>./src/main.py</path></file_paths></task>",
    memorySystem
);

// Validate a template
const validation = taskSystem.validateTemplate({
    taskPrompt: "<task>...</task>",
    systemPrompt: "System context",
    model: "claude-3-sonnet",
    isManualXML: false
});

// Register a template
const templateResult = await taskSystem.registerTemplate({
  name: "process_data",
  parameters: ["input_file", "options"],
  body: {
    type: "atomic",
    description: "Process {{input_file}} with options {{options}}",
    // Additional task properties
  }
});

// Call a template with arguments
const callResult = await taskSystem.executeCall({
  templateName: "process_data",
  arguments: ["data.csv", {format: "standard"}]
});
```

## Error Handling

The system handles several error types during execution, including resource exhaustion (for turns, context, or output), invalid output structure, XML parsing or validation errors, and general task execution failures. Each error type includes relevant context and metrics to aid in recovery and debugging. Errors will be surfaced to the Evaluator, which will use them for control flow.

## Resource Management

Resource management follows strict constraints with fixed context window sizes and limited turn counts. The system ensures clean resource release after task execution and prevents cross-Handler resource sharing. Handler configuration controls these limits:

```typescript
interface HandlerConfig {
    maxTurns: number;
    maxContextWindowFraction: number;
    defaultModel?: string;
    systemPrompt: string;
}
```

For detailed implementation specifications and patterns, refer to the component-level documentation and system contracts.
</file>
<file path="./components/task-system/api/interfaces.md" project="">
# Task System Public API

/**
 * Note: The TaskResult returned by executeTask includes an optional "criteria" field,
 * which is a free-form description provided by the Director task for dynamic evaluation
 * template selection via associative matching.
 */

## References

- Core Types: See [Type:TaskSystem:1.0] (`/components/task-system/spec/types.md`)
- Implementation: See `/components/task-system/spec/interfaces.md`
- XML Schema: See [Contract:Tasks:TemplateSchema:1.0] (`/system/contracts/protocols.md`)

```typescript
/**
 * Public API Surface
 * All types imported from [Type:TaskSystem:1.0]:
 * - TaskTemplate
 * - TaskResult 
 * - TaskType

/** 
 * For the full Handler interface definition, see [Interface:Handler:1.0] in spec/interfaces.md
 */

/**
 * Primary task execution interface
 * Uses [Type:TaskSystem:TaskType:1.0] and [Type:TaskSystem:TaskError:1.0]
 */
interface TaskSystem {
    executeTask(
        task: string,
        context: MemorySystem,
        taskType?: TaskType
    ): Promise<TaskResult>;

    validateTemplate(template: TaskTemplate): boolean;
  
    findMatchingTasks(
        input: string,
        context: MemorySystem
    ): Promise<Array<{
        template: TaskTemplate;
        score: number;
        taskType: TaskType;
    }>>;
}

/**
 * Memory System interface for task execution
 * @see [Interface:Memory:3.0] (`/components/memory/api/interfaces.md`)
 */
import { MemorySystem, GlobalIndex, FileMatch } from '@/components/memory/api/interfaces';
```
</file>
<file path="./components/compiler/README.md" project="">
# Compiler Component

## Purpose
The compiler handles translation and transformation of tasks into executable formats.

## Requirements

### Task Understanding
- Parse task requirements and constraints from natural language
- Identify task type and complexity  
- Validate instruction completeness

### XML Schema Requirements
- Define valid operation types including function templates and calls
- Support explicit parameter declarations via params attribute
- Enable function calling with positional arguments
- Validate template references during compilation
- Support nested function calls in argument position
- Specify input/output formats
- Support task validation

### AST Structure  
- Node type definitions
- Add TemplateNode type for function definitions
  - name: Unique template identifier
  - parameters: Array of parameter names
  - body: TaskNode for implementation
  - returns: Optional type information
  
- Add FunctionCallNode type for invocations
  - templateName: Reference to registered template
  - arguments: Array of ArgumentNodes
  
- Add ArgumentNode type for function arguments
  - value: String (variable/literal) or nested AST node
  
- Tree validation rules
- Traversal requirements
  - Templates are registered, not traversed directly
  - Function calls trigger template lookup and execution
  - Arguments are evaluated in caller's environment

### Validation Rules
- Input format validation
- Schema compliance
- AST structure validation

## Integration Points
[See existing interfaces documentation]
</file>
<file path="./TODOS.md" project="">
# Architecture spec TODOS

This document categorizes all remaining tasks and provides a prioritized implementation plan. The goals are to ensure system consistency, complete documentation, and implement remaining features in a logical order.

## Status Categories

Tasks are organized into four categories:

1. **Complete**: Implemented and documented in the system architecture
2. **Incomplete**: Well-defined goals that need implementation
3. **Inconsistent**: Items that conflict with other components or decisions
4. **Unclear**: Items requiring clarification before implementation

## Complete ✓

### Interface and Memory System
- Memory System interface version defined [Interface:Memory:3.0]
- File content type handling standardized (Handler tools)
- Unified storage approach defined in ADR
- Memory Component Separation with clear interfaces and integration documentation
- File tracking ownership decided (Memory System owns metadata, Handler owns file operations)
- Resource tracking responsibilities documented in Pattern:ResourceManagement:1.0
- File operation responsibility boundaries clearly defined:
  * Memory System: Manages ONLY metadata (file paths and descriptive strings)
  * Handler: Performs ALL file I/O operations using appropriate tools (including Anthropic's computer use tools for Anthropic models)
  * Clear separation of concerns between metadata management and file operations
- Memory System version consistently referenced as 3.0 across all documentation
- updateContext method references removed; documentation consistently reflects read-only context model
- Tool calls vs. subtasks boundaries clearly defined:
  * Tool calls: Handler-managed deterministic operations with rigid APIs
  * Subtasks: LLM-to-LLM interactions using continuation mechanism
  * Clear component responsibilities established for each type

### Context Management
- Document best practices (ADR 004, ADR 14)
- Define efficient subtask patterns (sequential)
- Extend `inherit_context` to map/reduce operators (ADR 14)
- Provide partial-result guidance via accumulation_format (ADR 9, ADR 14)
- Consolidate environment usage across context types
- Add context reuse mechanisms via context_management block
- Update error taxonomy for context failures (ADR 8)

### Task Type System & Patterns
- Basic task types defined and implemented
- Subtask support via standardized mechanism (ADR 11)
- Map operator implementation
- Director-Evaluator pattern with script execution integration
- Function-based templates with parameter declaration (ADR 12)
- Task continuation protocol via CONTINUATION status
- JSON-based output standardization (ADR 13)
- Partial results policy implemented (ADR 9)

### Phase 2: Core Implementation Improvements
5. **Interface updates to align with current patterns**
   - Review interface definitions for consistency with architecture decisions
   - Update type signatures and method names as needed
   - Ensure interfaces follow established patterns (Memory System is read-only, etc.)

6. **Document context management patterns for common scenarios**
   - Create comprehensive examples for context clearing and regeneration
   - Update documentation with recommended settings for different use cases
   - Ensure consistent guidance across all component documentation

7. **Summary output handling in evaluator**
   - Implement mechanism for efficient summary outputs between tasks
   - Define standard summary format
   - Document integration with context management

8. **Document subtask usage patterns with context**
   - Create comprehensive documentation for subtask patterns
   - Include examples of context inheritance models
   - Demonstrate common subtask usage patterns

9. **Provide complete implementation examples**
   - Create end-to-end examples of task execution
   - Include error handling, context management, and file operations
   - Demonstrate component interactions

### Phase 3: Architecture and Use Cases
10. **Architecture Documentation standardization**
    - Ensure consistent documentation structure across components
    - Standardize terminology and diagram formats
    - Create consistent interaction descriptions

11. **Add missing cross-component documentation**
    - Document how components interact
    - Create diagrams showing data flow between components
    - Ensure integration points are clearly described

12. **Write user stories and examples**
    - Create comprehensive user stories showing system usage
    - Include examples of different workflows
    - Demonstrate system capabilities through concrete examples

### Phase 4: Advanced Features
13. **Multiple tries/selection of best candidate result**
    - Design mechanism for generating multiple solution candidates
    - Implement evaluation criteria
    - Create selection process for best results

14. **Agent file storage and recall mechanisms**
    - Design system for agents to store and recall information
    - Define file format and organization
    - Implement access methods

15. **Advanced debugging capabilities**
    - Implement comprehensive logging
    - Add step-by-step execution tracking
    - Create context inspection tools

16. **Agent features (history storage, REPL)**
    - Design and implement storage for agent conversation history
    - Create REPL interface for interactive task execution
    - Integrate with existing components

17. **Multi-LLM support**
    - Design abstraction layer for different LLM providers
    - Implement adapters for each provider's API
    - Ensure consistent behavior across models

18. **DSL optimization or alternative language support**
    - Evaluate current DSL performance and usability
    - Identify optimization opportunities
    - Consider alternative syntax or language support

## Inconsistent ⚠️

~~2. **Task-subtask context inheritance**~~
   ~~- Multiple conflicting descriptions exist~~
   ~~- Standardize on the approach defined in ADR 14~~
   
   *Completed: Implemented atomic task subtypes with different context management defaults. Standard atomic tasks inherit context but don't generate fresh context, while subtasks don't inherit context but do generate fresh context.*

## Unclear ❓

1. **"Genetic" behavior implementation**
   - Clarify how multiple tries and result selection would work
   - Define evaluation criteria for selecting best solutions

2. **Subtask spawning and evaluator interaction**
   - Detail exactly how subtasks spawned by task system work with evaluator
   - Specify control flow and context handling

3. **Architecture documentation "self-similarity"**
   - Define what "self-similarity in structure" means for documentation
   - Provide concrete examples of desired documentation patterns

4. **Agent-style workflow patterns**
   - Clarify implementation of "conversation → json → map spec prompts" pattern
   - Provide examples of intended workflow

5. **Chat queue implementation**
   - Specify requirements for chat queue management
   - Define integration with task system

6. **Shell node integration**
   - Clarify how "shell" nodes fit into the architecture
   - Define interaction with Director-Evaluator pattern

7. **Model temperature/selection handling**
   - Determine whether model parameters should be handled at system level
   - Define configuration approach

8. **Multi-try evaluation criteria**
   - Specify how to implement evaluation criteria for selecting best results
   - Define scoring mechanisms and decision process

## Dependencies and Critical Path

The critical path for completing the system involves:

1. Documentation and interface consistency (Phase 1)
2. Core implementation improvements (Phase 2)
3. Architecture documentation and use cases (Phase 3)
4. Advanced features (Phase 4)

Items within Phase 1 should be completed first as they provide the foundation for all subsequent work. Within each phase, items are listed in priority order based on dependencies and impact.


user story:
make plan docs interactively (or with a prompt queue), write them to file; add them to heritable context; gen spec prompts; subtask -> try to impl spec prompt, up to 3 round of debugging; on failure return to parent task 
CONTINUATION with failure notes; parent tries to debug (either itself or with full context subtask) or maybe something else. 
bonus: failure notes get interpreted as lessons and are used to update plan or project_rules.md

- does handler need a 'dumb' llm backend to prompt the main agent to continue?
- how will git integration work?
- option to pass list of file paths to subtask

## Resolved Questions

- ✓ RESOLVED: Template substitution is an Evaluator responsibility. The Evaluator resolves all {{variable_name}} placeholders before dispatching tasks to the Handler.
</file>
<file path="./IDL.md" project="">
// === IDL-CREATION-GUIDLINES === // Object Oriented: Use OO Design. // Design Patterns: Use Factory, Builder and Strategy patterns where possible // ** Complex parameters JSON : Use JSON where primitive params are not possible and document them in IDL like "Expected JSON format: { "key1": "type1", "key2": "type2" }" // == !! BEGIN IDL TEMPLATE !! === // === CODE-CREATION-RULES === // Strict Typing: Always use strict typing. Avoid using ambiguous or variant types. // Primitive Types: Favor the use of primitive types wherever possible. // Portability Mandate: Python code must be written with the intent to be ported to Java, Go, and JavaScript. Consider language-agnostic logic and avoid platform-specific dependencies. // No Side Effects: Functions should be pure, meaning their output should only be determined by their input without any observable side effects. // Testability: Ensure that every function and method is easily testable. Avoid tight coupling and consider dependency injection where applicable. // Documentation: Every function, method, and module should be thoroughly documented, especially if there's a nuance that's not directly evident from its signature. // Contractual Obligation: The definitions provided in this IDL are a strict contract. All specified interfaces, methods, and constraints must be implemented precisely as defined without deviation. // =======================

module GenericSystemName {

// Interface for a generic entity
interface EntityName {

    // Action/method definition
    // Preconditions:
    // - Define any preconditions here.
    // - Expected JSON format: { "key1": "type1", "key2": "type2" } 
    // Postconditions:
    // - Define the expected outcomes here.
    returnType methodName(parameterType parameterName);

    // Additional methods...
};

// Another entity or component
interface AnotherEntity {

    // Action/method definition
    // Preconditions:
    // - Define any preconditions here.
    // - Expected JSON format: { "key1": "type1", "key2": "type2" } 
    // Postconditions:
    // - Define the expected outcomes here.
    returnType anotherMethodName(parameterType parameterName);

    // Additional methods...
};
// == !! END IDL TEMPLATE !! ===

// EXAMPLE // === CODE-CREATION-RULES === // Strict Typing: Always use strict typing. Avoid using ambiguous or variant types. // Primitive Types: Favor the use of primitive types wherever possible. // Portability Mandate: Python code must be written with the intent to be ported to Java, Go, and JavaScript. Consider language-agnostic logic and avoid platform-specific dependencies. // No Side Effects: Functions should be pure, meaning their output should only be determined by their input without any observable side effects. // Testability: Ensure that every function and method is easily testable. Avoid tight coupling and consider dependency injection where applicable. // Documentation: Every function, method, and module should be thoroughly documented, especially if there's a nuance that's not directly evident from its signature. // Contractual Obligation: The definitions provided in this IDL are a strict contract. All specified interfaces, methods, and constraints must be implemented precisely as defined without deviation. // ======================= // == !! BEGIN TEMPLATE EXAMPLE !! === interface Tweets { // Preconditions: // - userID exists. // - tweetContent is non-null and within allowable size limits. // Postconditions: // - A new tweet is created and stored. // Expected JSON format: { "userID": "string", "content": "string" } void postTweet(string tweetJSON); // Preconditions: // - userID and tweetID exist. // Postconditions: // - The tweet with tweetID is marked as liked by userID. void likeTweet(string userID, string tweetID); // Preconditions: // - userID and tweetID exist. // Postconditions: // - The tweet with tweetID is marked as retweeted by userID. // Expected JSON format: { "userID": "string", "originalTweetID": "string" } void retweet(string retweetJSON); // Preconditions: // - tweetID exists. // Postconditions: // - Returns the details of the tweet as JSON. string getTweetDetails(string tweetID); // Invariants: // - Filesystem storage maintains a list of tweets, likes, and retweets for each tweetID. }; // == !! END TEMPLATE EXAMPLE !! ===
)
</file>
<file path="./plan.md" project="">
Implementation Priority for Remaining Critical Path Issues
</file>
