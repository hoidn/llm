<file path="./plan.md" project="ptycho">
# Enhanced Implementation Plan for PyTorch Port

## 1. Call Structure Analysis and System Boundaries

### Core Call Sequence
The simulation follows this primary call structure:

1. **`run_np()`** in `run_debug.py`
   - Sets up logging, parameters
   - Creates OnePhonon instance
   - Calls apply_disorder() to generate diffuse intensity
   - Saves results

2. **`OnePhonon.__init__`** in `models.py`
   - Stores sampling parameters
   - Calls `_setup()` to initialize data structures
   - Calls `_setup_phonons()` to prepare phonon calculations

3. **`OnePhonon._setup()`**
   - Creates AtomicModel from PDB data
   - Generates reciprocal space grid
   - Creates Crystal object with supercell

4. **`AtomicModel.__init__`** in `pdb.py`
   - Loads PDB structure using Gemmi
   - Extracts cell parameters and symmetry operations
   - Processes atomic data (coordinates, form factors)

5. **`OnePhonon._setup_phonons()`**
   - Initializes tensor arrays for calculations
   - Builds displacement projection matrix (A)
   - Builds mass matrix (M)
   - Computes k-vectors in Brillouin zone
   - Sets up Gaussian Network Model
   - Computes phonon modes and covariance matrix

6. **`GaussianNetworkModel.__init__`**
   - Sets up atomic model
   - Builds gamma matrix (spring constants)
   - Builds neighbor lists
   - Provides methods to compute Hessian and dynamical matrices

7. **`OnePhonon.compute_gnm_phonons()`**
   - Gets Hessian matrix
   - For each k-vector, computes dynamical matrix
   - Performs eigendecomposition to get frequencies and modes

8. **`OnePhonon.compute_covariance_matrix()`**
   - Computes atomic displacement covariances from phonon modes
   - Scales to match experimental ADPs

9. **`OnePhonon.apply_disorder()`**
   - Core computation that produces diffuse intensity
   - For each k-vector, computes structure factors
   - Combines structure factors with phonon modes
   - Returns diffuse intensity map

10. **`structure_factors()`** in `scatter.py`
    - Batches calculations for efficiency
    - Computes atomic form factors
    - Applies Debye-Waller factors
    - Computes complex structure factors

### System Boundaries

#### Non-Differentiable Components (Keep in NumPy)
- **Data Loading**:
  - `AtomicModel._get_gemmi_structure()` - PDB loading
  - `AtomicModel._extract_cell()` - Cell parameter extraction
  - `AtomicModel._get_sym_ops()` - Symmetry operation extraction
  - Reading configuration files and parameters

- **Preprocessing**:
  - `AtomicModel.extract_frame()` - Frame extraction
  - `AtomicModel._extract_ff_coefs()` - Form factor coefficient extraction
  - `GaussianNetworkModel.build_neighbor_list()` - Neighbor list construction
  - Initial grid setup and mask creation

- **Postprocessing**:
  - Saving results to disk
  - Visualization operations in `visuals.py`
  - Statistical analysis in `stats.py` (when not in training loop)

#### Differentiable Components (Convert to PyTorch)
- **Core Physics Simulation**:
  - `OnePhonon._build_A()`, `_build_M()` - Matrix construction
  - `OnePhonon.compute_hessian()` - Hessian matrix computation
  - `OnePhonon.compute_gnm_phonons()` - Phonon mode calculation
  - `OnePhonon.compute_covariance_matrix()` - Covariance computation

- **Structure Factor Calculation**:
  - `compute_form_factors()` - Form factor calculation
  - `structure_factors_batch()` - Structure factor computation
  - Complex exponential operations and phase calculations

- **Model Application**:
  - `OnePhonon.apply_disorder()` - Main calculation combining all components
  - Other disorder models (`RigidBodyTranslations.apply_disorder()`, etc.)

- **Grid Operations**:
  - `generate_grid()` - Grid generation
  - `get_symmetry_equivalents()` - Symmetry operations
  - Fourier transforms and convolutions

## 2. Project Directory Structure

```
eryx/
├── __init__.py
├── pdb.py                      # Original NumPy PDB handling
├── pdb_torch.py                # PyTorch adaptation (partial)
├── map_utils.py                # Original NumPy map utilities
├── map_utils_torch.py          # PyTorch map utilities
├── scatter.py                  # Original structure factor calculations
├── scatter_torch.py            # PyTorch structure factor calculations
├── models.py                   # Original disorder models
├── models_torch.py             # PyTorch disorder models
├── base.py                     # Original transform calculations
├── base_torch.py               # PyTorch transform calculations
├── stats.py                    # Original statistical utilities
├── stats_torch.py              # PyTorch statistical utilities
├── reference.py                # Original reference implementations
├── adapters.py                 # NEW: NumPy to PyTorch adapters
│   ├── PDBToTensor             # Convert PDB data to tensors
│   ├── GridToTensor            # Convert grid data to tensors
│   ├── TensorToNumpy           # Convert results back to NumPy
│   └── ModelAdapters           # Adapt between model representations
├── torch_utils.py              # NEW: PyTorch-specific utilities
│   ├── Complex operations      # Handling complex numbers
│   ├── FFT utilities           # Fourier transform operations
│   ├── Gradient utilities      # Gradient calculation helpers
│   └── Eigendecomposition      # Differentiable eigendecomposition
├── logging_utils.py            # Original logging utilities
├── visuals.py                  # Visualization (no port needed)
├── run_debug.py                # Original debugging script
├── run_torch.py                # NEW: PyTorch equivalent of run_debug.py
├── autotest/                   # Testing framework
│   ├── __init__.py
│   ├── debug.py
│   ├── logger.py
│   ├── serializer.py
│   ├── configuration.py
│   ├── functionmapping.py
│   ├── testing.py
│   └── torch_testing.py        # NEW: PyTorch testing extensions
└── tests/                      # Test implementations
    ├── __init__.py
    ├── test_pdb.py
    ├── test_pdb_torch.py       # NEW: PyTorch version of tests
    ├── test_scatter.py
    ├── test_scatter_torch.py   # NEW: PyTorch version of tests
    ├── test_models.py
    ├── test_models_torch.py    # NEW: PyTorch version of tests
    ├── test_base.py
    ├── test_base_torch.py      # NEW: PyTorch version of tests
    ├── test_map_utils.py
    ├── test_map_utils_torch.py # NEW: PyTorch version of tests
    ├── test_adapters.py        # NEW: Tests for adapters
    ├── test_torch_utils.py     # NEW: Tests for PyTorch utilities
    ├── test_integration.py     # NEW: End-to-end integration tests
    └── test_gradients.py       # NEW: Tests for gradient calculation
```

https://claude.ai/chat/28eae72e-1cd0-42d2-8012-06a2f1075e02
</file>
<file path="./phased_plan.md" project="ptycho">
# Final Implementation Plan for PyTorch Port


// TODO: the plan is unclear about which components can be tested using 
// the ground truth values generated from running run_np() with @debug outputs,
// vs. which components aren't 1 to 1 with a np / reference implementation 
// component and therefore have to be validated in a different way

## Current Status Overview

- **Completed**: Most PyTorch stub files have been created with placeholders, docstrings, and type hints.
- **Not Started**: Ground truth data generation, adapter implementation, and core function implementation.

## System Boundaries and Differentiability Constraints

Based on plan.md, we must clearly distinguish between components that should remain in NumPy and those that should be converted to PyTorch with differentiability:

### Non-Differentiable Components (Keep in NumPy)
- **Data Loading**: PDB loading, cell parameter extraction, symmetry operations
- **Preprocessing**: Frame extraction, form factor extraction, neighbor list construction
- **Postprocessing**: Result saving, visualization, statistical analysis

### Differentiable Components (Convert to PyTorch)
- **Core Physics Simulation**: Matrix construction, Hessian computation, phonon calculations
- **Structure Factor Calculation**: Form factors, structure factors, complex operations
- **Model Application**: Disorder models (OnePhonon, RigidBodyTranslations, etc.)
- **Grid Operations**: Grid generation, symmetry operations, Fourier transforms

## Revised Implementation Plan

### Phase 1: Ground Truth Generation (1 week)

#### Task 1.1: Add Debug Decorators to Source Files
- Directly add `@debug` decorators to all functions identified in `to_convert.json`
- Modify the original source files in place to ensure direct decoration
- Add import statements: `from eryx.autotest.debug import debug`

**Key Files to Modify (based on to_convert.json prioritization):**

1. `eryx/scatter.py`:
   ```python
   @debug
   def compute_form_factors(q_grid, ff_a, ff_b, ff_c): ...

   @debug
   def structure_factors_batch(q_grid, xyz, ff_a, ff_b, ff_c, U=None, ...): ...

   @debug
   def structure_factors(q_grid, xyz, ff_a, ff_b, ff_c, U=None, ...): ...
   ```

2. `eryx/map_utils.py`:
   ```python
   @debug
   def generate_grid(A_inv, hsampling, ksampling, lsampling, return_hkl=False): ...

   @debug
   def get_symmetry_equivalents(hkl_grid, sym_ops): ...

   # Continue with other functions from to_convert.json
   ```

3. `eryx/models.py` - Add decorators to OnePhonon class methods:
   ```python
   class OnePhonon:
       @debug
       def __init__(self, pdb_path, hsampling, ksampling, lsampling, ...): ...
       
       @debug
       def _setup(self, pdb_path, expand_p1, res_limit, group_by): ...
       
       # Continue with other methods from to_convert.json
   ```

4. Additional files as specified in `to_convert.json`

#### Task 1.2: Configure Autotest Framework
- Create configuration file for autotest at `eryx/autotest_config.py`
- Ensure debug mode is enabled and log directory is properly set

#### Task 1.3: Generate Ground Truth Data

- Leverage the existing `run_np()` function in `run_debug.py` instead of creating a new script
- Modify the function slightly to ensure comprehensive ground truth data generation:

```python
# In run_debug.py, update run_np() to ensure comprehensive ground truth:

def run_np(variant=None):
    """
    Run NumPy version of diffuse scattering simulation to generate ground truth data.
    The @debug decorators will automatically capture inputs/outputs for testing.
    
    Args:
        variant: Optional string to specify parameter variations for comprehensive testing
    """
    # Base configuration
    pdb_path = "tests/pdbs/5zck_p1.pdb"
    
    if variant == "small":
        # Small grid for quick testing
        hsampling, ksampling, lsampling = [-2, 2, 2], [-2, 2, 2], [-2, 2, 2]
    elif variant == "medium":
        # Medium grid with different parameters
        hsampling, ksampling, lsampling = [-4, 4, 3], [-8, 8, 3], [-8, 8, 3]
        gnm_cutoff, gamma_intra, gamma_inter = 5.0, 0.8, 1.2
    else:
        # Default/full configuration
        hsampling, ksampling, lsampling = [-4, 4, 3], [-17, 17, 3], [-29, 29, 3]
        gnm_cutoff, gamma_intra, gamma_inter = 4.0, 1.0, 1.0
    
    logging.info(f"Starting NP branch computation with variant: {variant}")
    
    # Create model with parameters
    onephonon_np = OnePhonon(
        pdb_path,
        hsampling, ksampling, lsampling,
        expand_p1=True,
        res_limit=0.0,
        gnm_cutoff=gnm_cutoff if 'gnm_cutoff' in locals() else 4.0,
        gamma_intra=gamma_intra if 'gamma_intra' in locals() else 1.0,
        gamma_inter=gamma_inter if 'gamma_inter' in locals() else 1.0
    )
    
    # Apply disorder to generate diffuse intensity
    Id_np = onephonon_np.apply_disorder(use_data_adp=True)
    
    # Log statistics for verification
    logging.info(f"NP branch diffuse intensity stats: min={np.nanmin(Id_np)}, max={np.nanmax(Id_np)}")
    
    # Save results
    output_filename = f"np_diffuse_intensity{'_'+variant if variant else ''}.npy"
    np.save(output_filename, Id_np)
    
    return Id_np
```

- Create a small driver script to run different variants:

```python
# Create scripts/generate_ground_truth.py:

import os
import logging
from eryx.autotest_config import config
from eryx.run_debug import run_np

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

logger.info(f"Autotest debug mode: {config.getDebugFlag()}")
logger.info(f"Log directory: {config.getLogFilePrefix()}")

# Create output directory if needed
os.makedirs(config.getLogFilePrefix(), exist_ok=True)

# Run simulation with different parameter sets to ensure comprehensive coverage
logger.info("Running simulation to generate ground truth data")
run_np()  # Default parameters
run_np("small")  # Small grid for quick testing
run_np("medium")  # Different parameters
logger.info("Ground truth data generation complete")
```

- Verify ground truth data was captured properly:
  - Check that log files were created in the configured log directory
  - Confirm all instrumented functions generated logs
  - Validate that logs contain required input/output data

### Phase 2: Core Utilities Implementation (2 weeks)

#### Task 2.1: Implement Differentiable Tensor Operations
- Complete `ComplexTensorOps` in `eryx/torch_utils.py`:
  - Implement differentiable complex exponential for phase calculations
  - Ensure complex multiplication preserves gradients
  - Create differentiable Debye-Waller factor calculations
- Implement `EigenOps` with special focus on differentiable eigendecomposition:
  - Create SVD-based approach for eigenvalue decomposition
  - Ensure proper backpropagation through eigen operations
  - Document gradient flow and stability considerations
- Document all differentiability decisions

#### Task 2.2: Implement FFT Operations
- Complete `FFTOps` in `eryx/torch_utils.py`:
  - Implement differentiable FFT convolution for LiquidLikeMotions model
  - Create helpers for complex FFT operations
  - Ensure proper normalization that preserves gradients
- Document FFT implementation choices for differentiability

#### Task 2.3: Implement Gradient Utilities
- Complete `GradientUtils` in `eryx/torch_utils.py`:
  - Create tools for finite difference validation
  - Implement gradient norm calculations
  - Add gradient validation helpers
- Document validation methodology

### Phase 3: Adapter Implementation (2 weeks)

#### Task 3.1: Implement PDBToTensor Adapter
- Complete `convert_atomic_model(model)` method for AtomicModel conversion:
  - Convert only differentiable components to tensors
  - Keep non-differentiable components (like neighbor lists) in NumPy
- Implement `convert_crystal(crystal)` and `convert_gnm(gnm)` methods:
  - Document which components should be differentiable
- Support explicit device placement with proper defaults
- Add tests to verify bidirectional conversion

#### Task 3.2: Implement GridToTensor Adapter
- Complete `convert_grid(q_grid, map_shape)` method:
  - Ensure grid generation is differentiable
- Implement `convert_mask(mask)` and `convert_symmetry_ops(sym_ops)` methods:
  - Keep masks non-differentiable (boolean)
  - Ensure symmetry operations preserve gradients
- Add tests for grid conversion

#### Task 3.3: Implement TensorToNumpy and ModelAdapters
- Complete conversions with gradient preservation
- Implement model-specific adapters based on differentiability needs
- Add comprehensive tests for each adapter

### Phase 4: Core Function Implementation (3 weeks)

#### Task 4.1: Implement Map Utilities
- Complete `generate_grid` in `eryx/map_utils_torch.py`:
  - Use differentiable PyTorch grid operations
  - Preserve shape information for gradient flow
- Implement symmetry operations with gradient preservation
- Document grid generation differentiability

#### Task 4.2: Implement Structure Factor Calculations
- Complete `compute_form_factors` in `eryx/scatter_torch.py`:
  - Ensure differentiable form factor calculation
  - Preserve gradients through exponential operations
- Implement `structure_factors_batch` with proper gradient flow:
  - Use complex number operations that preserve gradients
  - Implement differentiable Debye-Waller factor application
- Add device management with proper defaults
- Document structure factor gradient flow

#### Task 4.3: Implement Base Transforms
- Complete `compute_molecular_transform` in `eryx/base_torch.py`:
  - Preserve gradients through transform calculations
- Implement `compute_crystal_transform` with gradient preservation
- Document transform differentiability considerations

### Phase 5: Model Implementation - OnePhonon (3 weeks)

#### Task 5.1: Implement OnePhonon Matrix Construction
- Complete `_build_A` method:
  - Ensure differentiable projection matrix construction
  - Document gradient flow through matrix operations
- Implement `_build_M` method:
  - Create differentiable mass matrix construction
  - Use PyTorch's matrix operations for gradient preservation
- Complete `_build_kvec_Brillouin` with differentiable operations
- Document matrix construction differentiability

#### Task 5.2: Implement OnePhonon Physics Calculations
- Complete `compute_hessian` method:
  - Ensure differentiable Hessian construction
  - Preserve gradients through matrix operations
- Implement `compute_gnm_phonons` with special attention to eigendecomposition:
  - Use differentiable eigendecomposition from `EigenOps`
  - Ensure proper handling of numerical instabilities
  - Document eigendecomposition differentiability approach
- Complete `compute_covariance_matrix` with gradient preservation
- Document physics calculation differentiability

#### Task 5.3: Implement OnePhonon Disorder Application
- Complete `apply_disorder` method:
  - Ensure end-to-end gradient flow from inputs to diffuse intensity
  - Optimize memory usage while preserving computation graph
  - Implement differentiable structure factor operations
- Add tests comparing to ground truth output
- Document differentiability through the complete model

### Phase 6: Alternative Models Implementation (3 weeks)

#### Task 6.1: Implement RigidBodyTranslations Model
- Complete `apply_disorder` method:
  - Implement differentiable Debye-Waller factor calculation
  - Ensure gradient flow through Wilson parameters
- Implement `optimize` method:
  - Add gradient-based optimization option
- Document model-specific differentiability

#### Task 6.2: Implement LiquidLikeMotions Model
- Complete `fft_convolve` method:
  - Use differentiable FFT operations from `FFTOps`
  - Ensure gradient preservation through convolution
- Implement `apply_disorder` method:
  - Preserve gradients through kernel operations
  - Ensure differentiable scaling with displacement parameters
- Document FFT-based differentiability approach

#### Task 6.3: Implement RigidBodyRotations Model
- Complete `generate_rotations_around_axis` method:
  - Ensure differentiable rotation matrix generation
  - Use PyTorch's rotation functions with gradient support
- Implement `apply_disorder` method:
  - Preserve gradients through rotational disorder application
- Document rotational differentiability considerations

### Phase 7: Integration and Optimization (2 weeks)

#### Task 7.1: Complete Run Script
- Finish implementation of `eryx/run_torch.py`:
  - Add device management and error handling
  - Create end-to-end gradient flow demonstration
- Add end-to-end tests with gradient validation

#### Task 7.2: Optimize Performance
- Implement batching for large datasets with gradient preservation
- Optimize memory usage with careful tensor management
- Document optimization decisions and tradeoffs

#### Task 7.3: Validation and Documentation
- Add comprehensive gradient validation
- Complete documentation of all differentiability considerations
- Create example notebooks demonstrating gradient-based optimization

## Implementation Priorities (based on to_convert.json and differentiability requirements)

1. Differentiable tensor operations (especially complex numbers and eigendecomposition)
2. Structure factor calculations with gradient preservation
3. Map utilities with differentiable grid operations
4. OnePhonon model with focus on eigendecomposition and matrix operations
5. Alternative models with model-specific differentiability requirements

## Timeline and Dependencies

- **Phase 1** (1 week): No dependencies, can start immediately
- **Phase 2** (2 weeks): Depends on Phase 1 for ground truth data
- **Phase 3** (2 weeks): Depends on Phase 2 for tensor operations
- **Phase 4** (3 weeks): Depends on Phase 3 for adapter components
- **Phase 5** (3 weeks): Depends on Phase 4 for core functions
- **Phase 6** (3 weeks): Depends on Phase 5 for tensor operations and physics implementations
- **Phase 7** (2 weeks): Depends on all previous phases

**Total Timeline: 16 weeks**

## Model-Specific Differentiability Requirements

### OnePhonon Model
- Matrix operations (`_build_A`, `_build_M`) must preserve gradients
- Eigendecomposition in `compute_gnm_phonons` requires special handling for differentiability
- Covariance matrix calculation must maintain the computational graph
- Complex number operations in structure factors must preserve gradients

### LiquidLikeMotions Model
- FFT convolution must be implemented with gradient preservation
- Kernel operations need to maintain the computational graph
- Complex exponential operations must preserve gradients

### RigidBodyTranslations Model
- Debye-Waller factor calculations must be differentiable
- Wilson parameter scaling must preserve gradients

### RigidBodyRotations Model
- Rotation matrix generation must be differentiable
- Ensemble averaging must preserve gradients when needed

## Documentation Requirements

Throughout all phases:
- Document all architectural decisions with focus on differentiability
- Include mathematical explanations of gradient flow through complex operations
- Document shape information and gradient handling in all operations
- Explain model-specific differentiability considerations
- Provide examples of gradient calculation and propagation

## Device Management Strategy

- All PyTorch implementations should support explicit device placement
- Default to CUDA if available, CPU otherwise
- Implement device context managers for consistent device handling
- Document device placement strategies in all components
- Ensure consistent device handling across model components

## Testing Strategy

- Use ground truth data generated in Phase 1 for all component tests
- Implement tests for each component using `torch_testing.py`
- Add gradient validation tests for all differentiable operations
- Create end-to-end tests that verify gradient flow
- Document testing methodology and tolerance settings
</file>
<file path="./TODOS.md" project="ptycho">

This file contains a detailed list of tasks derived 1-to-1 from the spec prompts outlined in plan.md.

## 1. Test Framework Specification
- **Autotest Extension**: Extend the autotest framework with PyTorch-specific functionality (e.g., implement a TorchTesting class).
- **Numerical Comparison Utilities**: Develop utilities for tolerance-based numerical comparisons between PyTorch and NumPy outputs.
- **Gradient Checking**: Implement gradient checking utilities using finite differences to validate differentiable operations.
- **Ground Truth Generation**: Create a script to generate and serialize ground truth data from NumPy implementations for regression testing.

## 2. Adapter Component Specification
- **PDBToTensor**: Implement conversion of AtomicModel and related crystallographic data from NumPy arrays to PyTorch tensors, preserving the computational graph.
- **GridToTensor**: Create an adapter to convert reciprocal space grids from NumPy arrays to PyTorch tensors.
- **TensorToNumpy**: Implement conversion of PyTorch tensor outputs back to NumPy arrays for visualization and further processing.
- **ModelAdapters**: Develop adapters for model-specific conversions to seamlessly interoperate between NumPy and PyTorch implementations.
- **Error Handling and Device Management**: Ensure robust error handling and proper device (CPU/GPU) assignment during conversions.

## 3. Grid and Transform Operations Specification
- **Grid Generation**: Port grid generation functions (e.g., generate_grid) to PyTorch, ensuring differentiability.
- **Symmetry Operations**: Implement differentiable functions to compute symmetry-equivalent Miller indices and perform raveling using PyTorch.
- **Resolution and Masking Calculations**: Convert resolution computation and masking functions to PyTorch.
- **Transform Operations**: Create differentiable operations for transformations (e.g., Fourier transforms) using PyTorch.

## 4. Core Physics - Structure Factor Calculation Specification
- **compute_form_factors**: Develop a PyTorch version to compute atomic form factors.
- **structure_factors_batch and structure_factors**: Implement PyTorch versions of batch and overall structure factor calculations.
- **Complex Operations and Gradient Flow**: Ensure proper handling of complex number operations and preservation of gradient flow throughout the calculations.

## 5. Core Physics - Gaussian Network Model (GNM) Specification
- **GNM Calculations**: Port Gaussian Network Model calculations to PyTorch, including the construction of tensor-based spring constant matrices.
- **Neighbor List Adaptation**: Adapt neighbor list computation to work with PyTorch tensors.
- **Hessian Computation**: Implement differentiable Hessian computations for the GNM using PyTorch.

## 6. Core Physics - Phonon Calculations Specification
- **Eigendecomposition**: Implement differentiable eigendecomposition (or SVD) using PyTorch functions.
- **Covariance Matrix Calculation**: Port covariance matrix computations to PyTorch.
- **Phonon Mode Calculation**: Ensure that phonon mode calculations (frequency and mode extraction) are differentiable and physically consistent.

## 7. Alternative Disorder Models Specification
- **RigidBodyTranslations**: Implement a PyTorch version of the rigid body translations disorder model.
- **LiquidLikeMotions**: Develop a PyTorch variant of the liquid-like motions disorder model with FFT-based convolution.
- **RigidBodyRotations**: Create a PyTorch implementation for rigid body rotational disorder.
- **API Consistency and Optimization Routines**: Ensure consistency with the NumPy versions and adapt optimization routines for gradient-based optimization; manage internal state appropriately.

## 8. Integration and Execution Specification
- **run_torch.py Script**: Develop the PyTorch equivalent of the simulation script (run_torch.py) for full end-to-end diffuse scattering simulation.
- **Module Integration**: Integrate grid, physics, model, and adapter components into a cohesive PyTorch pipeline.
- **Result Comparison Utilities**: Implement utilities to compare PyTorch outputs with legacy NumPy outputs.
- **Demonstration Notebooks and Visualization**: Create demonstration notebooks and scripts for visualization and workflow presentation.
- **Performance Benchmarking**: Add performance and profiling utilities to measure execution time and memory usage.

## 9. Optimization and Validation Specification
- **Comprehensive Testing**: Develop unit tests, component tests, and integration tests for all PyTorch modules.
- **Performance Profiling**: Implement profiling tools to measure execution time and memory usage; identify bottlenecks.
- **GPU Optimization**: Optimize tensor operations for efficient GPU execution and improved memory usage.
- **Documentation and Examples**: Finalize documentation, update in-code comments, and create user-facing example notebooks.
- **Gradient and Numerical Accuracy Validation**: Validate gradient computations and ensure numerical accuracy throughout the pipeline.

## Additional Tasks (from Spec Prompts)
- **Ground Truth Generation Strategy**: Instrument NumPy functions to capture and serialize ground truth results.
- **Testing Strategy Implementation**: Establish testing pipelines for unit, component, and end-to-end tests as outlined in the plan.
- **Timeline and Dependencies Documentation**: Document project timelines and inter-module dependencies as specified in plan.md.
</file>
<file path="./run_debug.py" project="ptycho">
#!/usr/bin/env python3
import os
os.environ["DEBUG_MODE"] = "1"
import logging
import numpy as np
from eryx.models import OnePhonon

logging.basicConfig(
    level=logging.DEBUG,
    format="%(asctime)s %(levelname)s: %(message)s",
    filename="debug_output.log",
    filemode="w"
)
# Also set up console logging here if desired:
console = logging.StreamHandler()
console.setLevel(logging.DEBUG)
console.setFormatter(logging.Formatter("%(asctime)s %(levelname)s: %(message)s"))
logging.getLogger("").addHandler(console)


def setup_logging():
    # Remove any existing handlers.
    for handler in logging.root.handlers[:]:
        logging.root.removeHandler(handler)
    
    logging.basicConfig(
        level=logging.DEBUG,
        format="%(asctime)s %(levelname)s: %(message)s",
        filename="debug_output.log",
        filemode="w"
    )
    # Also output to console
    console = logging.StreamHandler()
    console.setLevel(logging.DEBUG)
    formatter = logging.Formatter("%(asctime)s %(levelname)s: %(message)s")
    console.setFormatter(formatter)
    logging.getLogger("").addHandler(console)

def run_np():
    # Use a small grid for testing; adjust parameters as necessary.
    logging.info("Starting NP branch computation")
    pdb_path = "tests/pdbs/5zck_p1.pdb"
    onephonon_np = OnePhonon(
        pdb_path,
        [-4, 4, 3], [-17, 17, 3], [-29, 29, 3],
        expand_p1=True,
        res_limit=0.0,
        gnm_cutoff=4.0,
        gamma_intra=1.0,
        gamma_inter=1.0
    )
    Id_np = onephonon_np.apply_disorder(use_data_adp=True)
    logging.debug(f"NP: hkl_grid shape = {onephonon_np.hkl_grid.shape}")
    logging.debug("NP: hkl_grid coordinate ranges:")
    logging.debug(f"  Dimension 0: min = {onephonon_np.hkl_grid[:,0].min()}, max = {onephonon_np.hkl_grid[:,0].max()}")
    logging.debug(f"  Dimension 1: min = {onephonon_np.hkl_grid[:,1].min()}, max = {onephonon_np.hkl_grid[:,1].max()}")
    logging.debug(f"  Dimension 2: min = {onephonon_np.hkl_grid[:,2].min()}, max = {onephonon_np.hkl_grid[:,2].max()}")
    logging.debug(f"NP: q_grid range: min = {onephonon_np.q_grid.min()}, max = {onephonon_np.q_grid.max()}")
    logging.info("NP branch diffuse intensity stats: min=%s, max=%s", np.nanmin(Id_np), np.nanmax(Id_np))
    # Save for later comparison
    np.save("np_diffuse_intensity.npy", Id_np)

def run_torch():
    # TODO parallel torch implementation that computes the same thing as run_np()
    pass

if __name__ == "__main__":
    setup_logging()
    
    # After setting up, call the run routines.
    run_np()
    run_torch()
    logging.info("Completed debug run. Please check debug_output.log, np_diffuse_intensity.npy and torch_diffuse_intensity.npy")
</file>
<file path="./progress.md" project="ptycho">
# Project Progress Report

This document summarizes the current implementation state as compared to the detailed plan outlined in `plan.md`.

## Overview of the Plan

The plan was organized into multiple phases:
1. **Test Framework & Adapter Components:**  
   - Set up a robust testing framework including PyTorch-specific testing (e.g., TorchTesting).
   - Develop adapter components (e.g. PDBToTensor, GridToTensor, TensorToNumpy, ModelAdapters) to bridge the legacy NumPy components with the new PyTorch implementations.

2. **Grid and Transform Operations:**  
   - Implement PyTorch versions of grid generation and transformation utilities with gradient-preserving operations.
   
3. **Core Physics Components:**  
   - Port key computational functions such as `compute_form_factors`, `structure_factors_batch`, and related structure factor calculations.
   - Develop a full PyTorch version of the Gaussian Network Model (GNM) and phonon calculations to compute the diffuse intensity maps.
   
4. **Integration and Optimization:**  
   - Integrate all components into end-to-end simulation scripts (`run_torch.py`).
   - Add performance profiling, GPU optimization, and gradient validation.

## Current Implementation State

After reviewing the repository files and the plan, here are our findings:

- **Scaffolding and Stubs:**  
  Many modules in the PyTorch port (e.g., `eryx/map_utils_torch.py`, `eryx/models_torch.py`, and sections within adapter files) contain numerous `raise NotImplementedError` placeholders. This indicates that while the file structure and API layout mimic the intended design from `plan.md`, the core computational functionality has yet to be fully implemented.

- **Adapter Components:**  
  The adapter components in `eryx/adapters.py` provide skeleton classes (PDBToTensor, GridToTensor, TensorToNumpy, and ModelAdapters). Their method stubs follow the intended interfaces but do not yet perform the complete conversions, which is consistent with an early scaffold phase.

- **Testing Framework:**  
  The PyTorch testing framework (e.g., in `eryx/autotest/torch_testing.py` and the tests for PyTorch implementations like in `tests/test_scatter_torch.py`) is outlined. However, many tests include commented-out code or TODO markers, suggesting that automated verification of the PyTorch modules is still pending.

- **Legacy vs. New Code:**  
  The legacy NumPy implementations continue to reside in files like `eryx/map_utils.py`, `eryx/models.py`, and others. In contrast, the new PyTorch modules are structured similarly but are largely incomplete.

- **Plan vs. Implementation:**  
  The detailed phased plan in `plan.md` is very comprehensive. In practice, the repository’s progress reflects the early phases (scaffolding and API design) without the full realization of the core physics, optimized tensor operations, or gradient validation functionality.

## Summary

Overall, the project is in its early development phase. The directory structure, file organization, and method signatures closely follow the design specification in `plan.md`. However, the majority of the computational logic for the PyTorch port—including grid generation, structure factor evaluation, the implementation of GNM-based phonon calculations, and the convergence of the test suite—remains to be implemented. The current state is consistent with the first phase of the plan (creating stubs and scaffolding), with clear markers (NotImplementedError and TODO comments) indicating planned work for later phases.

This report should serve as both a progress indicator and a checklist for the remaining implementation tasks.
</file>
<file path="./scripts/parse_yaml.py" project="ptycho">
import numpy as np

class AttrDict(dict):
    """ Nested Attribute Dictionary
    A class to convert a nested Dictionary into an object with key-values
    accessible using attribute notation (AttrDict.attribute) in addition to
    key notation (Dict["key"]). This class recursively sets Dicts to objects,
    allowing you to recurse into nested dicts (like: AttrDict.attr.attr)
    Adapted from: https://stackoverflow.com/a/48806603
    """

    def __init__(self, mapping=None, *args, **kwargs):
        """Return a class with attributes equal to the input dictionary."""
        super(AttrDict, self).__init__(*args, **kwargs)
        if mapping is not None:
            for key, value in mapping.items():
                self.__setitem__(key, value)

    def __setitem__(self, key, value):
        if isinstance(value, dict):
            value = AttrDict(value)
        super(AttrDict, self).__setitem__(key, value)
        self.__dict__[key] = value  

    def __getattr__(self, item):
        try:
            return self.__getitem__(item)
        except KeyError:
            raise AttributeError(item)

def list_to_tuples(config):
    """
    Convert all lists in config dictionary to tuples.
    """
    for key in config.keys():
        for subkey in config[key].keys():
            if type(config[key][subkey]) == list:
                config[key][subkey] = tuple(config[key][subkey])
    return config

def expand_sampling(config, force_int=False):
    """
    Try to expand xsampling keys to to P1 by making them 
    symmetric about the reciprocal space origin. 
    
    Parameters
    ----------
    config : AttrDict object
        dictionary with setup key that contains xsampling keys
    force_int : bool
        if True, force boundaries to nearest integer
    """
    for key in ['hsampling','ksampling','lsampling']:
        if config.setup[key][0] != -1*config.setup[key][1]:
            new_val = max(np.abs(config.setup[key][0]), np.abs(config.setup[key][1]))
            config.setup[key] = (-1*new_val, new_val, config.setup[key][2])
        if force_int:
            config.setup[key] = (np.around(config.setup[key][0]),
                                 np.around(config.setup[key][1]),
                                 config.setup[key][2])
</file>
<file path="./scripts/main.py" project="ptycho">
import argparse
import sys
import traceback
import yaml
from tasks import *
from parse_yaml import *

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('-c', '--config', required=True, type=str, help='Path to config file')
    parser.add_argument('-t', '--task', required=True, type=str, help='Disorder model')
    
    config_filepath = parser.parse_args().config
    task = parser.parse_args().task
    with open(config_filepath, "r") as config_file:
        config = AttrDict(yaml.safe_load(config_file))
        config = list_to_tuples(config)
        
    try:
        os.makedirs(config.setup.root_dir, exist_ok=True)
        for subdir in ['models', 'base', 'figs']:
            os.makedirs(os.path.join(config.setup.root_dir, subdir), exist_ok=True)
    except:
        print(f"Error: cannot create root path.") 
        return -1 

    try:
        globals()[task]
    except Exception as e:
        print(f'{task} not found.')
    globals()[task](config)

    return 0, 'Task successfully executed'

if __name__ == '__main__':
    try:
        retval, status_message = main()
    except Exception as e:
        print(traceback.format_exc(), file=sys.stderr)
        retval = 1
        status_message = 'Error: Task failed.'

    print(status_message)
    exit(retval)
</file>
<file path="./scripts/tasks.py" project="ptycho">
import logging
import numpy as np
import os
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)

def Test(config):
    pass

def RigidBodyTranslations(config):
    """ Model rigid body translations, optimizing sigma. """
    from eryx.models import RigidBodyTranslations
    task = config.RigidBodyTranslations
    logger.debug('Setting up model')
    model = RigidBodyTranslations(config.setup.pdb_path, 
                                  config.setup.hsampling,
                                  config.setup.ksampling,
                                  config.setup.lsampling,
                                  res_limit=config.setup.res_limit,
                                  batch_size=config.setup.batch_size,
                                  n_processes=config.setup.n_processes,
                                  expand_friedel=task.get('expand_friedel') if task.get('expand_friedel') is not None else True)
    logger.debug('Optimizing model')
    ccs, sigmas = model.optimize(np.load(config.setup.exp_map), 
                                 task.sigmas_min, 
                                 task.sigmas_max, 
                                 n_search=task.n_search)
    model.plot_scan(output=os.path.join(config.setup.root_dir, "figs/scan_rigidbodytranslations.png"))
    np.save(os.path.join(config.setup.root_dir, "models/rigidbodytranslations.npy"), model.opt_map)
    np.save(os.path.join(config.setup.root_dir, "base/mtransform.npy"), model.transform)

def LiquidLikeMotions(config):
    """ Model liquid like motions, optimizing sigma and gamma. """
    from eryx.models import LiquidLikeMotions
    from parse_yaml import expand_sampling
    task = config.LiquidLikeMotions
    logger.debug('Setting up model')
    expand_sampling(config)
    model = LiquidLikeMotions(config.setup.pdb_path,
                              config.setup.hsampling,
                              config.setup.ksampling,
                              config.setup.lsampling,
                              res_limit=config.setup.res_limit,
                              batch_size=config.setup.batch_size,
                              n_processes=config.setup.n_processes,
                              asu_confined=task.asu_confined)
    logger.debug('Optimizing model')
    model.optimize(np.load(config.setup.exp_map),
                   task.sigmas_min,
                   task.sigmas_max,
                   task.gammas_min,
                   task.gammas_max,
                   ns_search=task.ns_search,
                   ng_search=task.ng_search)
    ntransform, extension = "ctransform", ""
    if task.asu_confined:
        ntransform, extension = "mtransform", "_asuconfined"
    model.plot_scan(output=os.path.join(config.setup.root_dir, f"figs/scan_liquidlikemotions{extension}.png"))
    np.save(os.path.join(config.setup.root_dir, f"models/liquidlikemotions{extension}.npy"), model.opt_map)
    np.save(os.path.join(config.setup.root_dir, f"base/{ntransform}.npy"), model.transform)

def OnePhonon(config):
    """ Model elastic network-based motions in the one-phonon approximation. """
    from eryx.models import OnePhonon
    from parse_yaml import expand_sampling
    task = config.OnePhonon
    logger.debug('Setting up model')
    expand_sampling(config, force_int=True)
    model = OnePhonon(config.setup.pdb_path,
                      config.setup.hsampling,
                      config.setup.ksampling,
                      config.setup.lsampling,
                      gnm_cutoff=task.gnm_cutoff,
                      gamma_intra=task.gamma_intra,
                      gamma_inter=task.gamma_inter,
                      batch_size=config.setup.batch_size,
                      n_processes=config.setup.n_processes,
                      expand_p1=task.get('expand_p1') if task.get('expand_p1') is not None else True)
    logger.debug('Computing diffuse map')
    Id = model.apply_disorder(use_data_adp=True)
    Id = Id.reshape(model.map_shape)
    np.save(os.path.join(config.setup.root_dir, f"models/onephonon.npy"), Id)
</file>
<file path="./to_convert.json" project="ptycho">
{
  "models.py": {
    "class OnePhonon": {
      "__init__": "Main constructor, needs to initialize PyTorch tensors",
      "_setup": "Core setup that initializes tensor structures",
      "_setup_phonons": "Sets up phonon calculations with tensors",
      "_build_A": "Builds displacement projection matrix using tensor operations",
      "_build_M": "Builds mass matrix using tensor operations",
      "_build_M_allatoms": "Creates atomic mass matrix",
      "_project_M": "Projects all-atom mass matrix",
      "_build_kvec_Brillouin": "Computes k-vectors in Brillouin zone",
      "_center_kvec": "Helper function for k-vector centering",
      "_at_kvec_from_miller_points": "Maps Miller indices to reciprocal space",
      "compute_gnm_phonons": "Core physics for phonon calculations",
      "compute_hessian": "Builds the Hessian matrix for elastic network",
      "compute_covariance_matrix": "Computes atomic displacement covariances",
      "apply_disorder": "Main computation that produces diffuse intensity"
    },
    "class RigidBodyTranslations": {
      "__init__": "Initializes tensor structures",
      "_setup": "Core setup for tensor operations",
      "apply_disorder": "Main computation applying disorder to molecular transform",
      "optimize": "Optimization routine (if used in training)"
    },
    "class LiquidLikeMotions": {
      "__init__": "Initializes tensor structures",
      "_setup": "Core tensor setup",
      "fft_convolve": "FFT convolution operation",
      "apply_disorder": "Applies liquid-like disorder model",
      "optimize": "Optimization routine (if used in training)"
    },
    "class RigidBodyRotations": {
      "__init__": "Initializes tensor structures",
      "_setup": "Core tensor setup",
      "generate_rotations_around_axis": "Generates rotation matrices",
      "apply_disorder": "Applies rotational disorder model",
      "optimize": "Optimization routine (if used in training)"
    }
  },
  "scatter.py": {
    "compute_form_factors": "Calculates atomic form factors",
    "structure_factors_batch": "Core physics for structure factor calculation",
    "structure_factors": "Wrapper for structure factor calculation"
  },
  "pdb.py": {
    "class GaussianNetworkModel": {
      "__init__": "Initializes network model",
      "_setup_atomic_model": "Setup for atomic model",
      "_setup_gaussian_network_model": "Core setup for GNM",
      "build_gamma": "Builds spring constant matrix",
      "build_neighbor_list": "Builds spatial relationship data",
      "compute_hessian": "Core physics calculation",
      "compute_K": "Computes dynamical matrix at k-vector",
      "compute_Kinv": "Computes inverse dynamical matrix"
    },
    "class AtomicModel": {
      "_get_xyz_asus": "Transforms atomic coordinates - needed for computation",
      "flatten_model": "Reshapes tensors for computation"
    },
    "class Crystal": {
      "get_asu_xyz": "Gets atomic coordinates for computation"
    },
    "sym_str_as_matrix": "Converts symmetry string to matrix"
  },
  "map_utils.py": {
    "generate_grid": "Creates computational grid",
    "get_symmetry_equivalents": "Gets symmetry equivalent Miller indices",
    "get_ravel_indices": "Maps 3D indices to 1D",
    "compute_resolution": "Calculates resolution from cell parameters",
    "get_resolution_mask": "Creates resolution mask for computation",
    "get_dq_map": "Calculates distance to nearest Bragg peak",
    "get_centered_sampling": "Gets sampling parameters",
    "resize_map": "Resizes maps for computation"
  },
  "stats.py": {
    "compute_cc": "Computes correlation coefficient",
    "compute_cc_by_shell": "Computes CC by resolution shell",
    "compute_cc_by_dq": "Computes CC by distance to Bragg peak"
  },
  "base.py": {
    "compute_molecular_transform": "Computes molecular transform",
    "compute_crystal_transform": "Computes crystal transform",
    "incoherent_sum_real": "Sums scattering in real space",
    "incoherent_sum_reciprocal": "Sums scattering in reciprocal space"
  },
  "reference.py": {
    "structure_factors": "Reference implementation of structure factors",
    "diffuse_covmat": "Computes diffuse scattering from covariance matrix"
  },
  "adapter_components": {
    "PDBToTensor": "Adapter to convert PDB data to PyTorch tensors",
    "NPGridToTensor": "Adapter to convert NumPy grids to PyTorch tensors",
    "TensorToCrystal": "Adapter to convert PyTorch tensor results to Crystal format",
    "TensorToMap": "Adapter to convert PyTorch tensor results to map format"
  }
}
</file>
<file path="./specs/spec1.md" project="ptycho">
# PyTorch Port Implementation Specification

## High-Level Objective
- Create a parallel PyTorch project structure with comprehensive stub implementations for all components that need to be ported from NumPy to PyTorch

## Mid-Level Objectives
- Create the directory structure for the PyTorch implementation
- Generate stub files for all modules that need PyTorch equivalents
- Implement adapter components to bridge NumPy and PyTorch
- Create comprehensive function stubs with detailed TODOs, docstrings, and type hints
- Set up scaffolding for testing components

## Implementation Notes

### Code Organization
- Create `_torch.py` files parallel to existing NumPy implementations
- Maintain consistent function and class names
- Include imports for both NumPy and PyTorch in stub files

### Documentation Standards
- Each module should have a detailed module docstring
- Each function/method should have a detailed docstring with parameter types, return types, and references to original implementation
- Use Google style docstrings for consistency

### Type Hints
- Use full type hints throughout with `torch.Tensor` instead of `np.ndarray`
- Include shape information in comments
- Use `Optional`, `Union`, etc. as appropriate

### Implementation Considerations
- Clearly mark functions that operate on tensors vs. arrays
- Include placeholder error handling with NotImplementedError
- Consider gradient flow in all operations
- Support explicit device placement (GPU/CPU)

## System Boundaries

### Non-Differentiable Components (Keep in NumPy)
- **Data Loading**: PDB loading, cell parameter extraction, symmetry operations
- **Preprocessing**: Frame extraction, form factor extraction, neighbor list construction
- **Postprocessing**: Result saving, visualization, statistical analysis

### Differentiable Components (Convert to PyTorch)
- **Core Physics Simulation**: Matrix construction, Hessian computation, phonon calculations
- **Structure Factor Calculation**: Form factors, structure factors, complex operations
- **Model Application**: Disorder models (OnePhonon, RigidBodyTranslations, etc.)
- **Grid Operations**: Grid generation, symmetry operations, Fourier transforms

## Directory Structure
```
eryx/
├── models_torch.py              # PyTorch disorder models
├── scatter_torch.py             # PyTorch structure factors
├── map_utils_torch.py           # PyTorch map utilities
├── base_torch.py                # PyTorch transforms
├── stats_torch.py               # PyTorch statistics
├── adapters.py                  # NumPy-PyTorch adapters
├── torch_utils.py               # PyTorch-specific utilities
├── run_torch.py                 # PyTorch run script
├── autotest/
│   └── torch_testing.py         # PyTorch testing extensions
└── tests/
    ├── test_*_torch.py          # PyTorch component tests
    ├── test_adapters.py         # Adapter tests
    ├── test_integration.py      # Integration tests
    └── test_gradients.py        # Gradient tests
```

## Low-Level Tasks

1. **Create Models Module Stubs (`eryx/models_torch.py`)**
   - Create PyTorch versions of OnePhonon, RigidBodyTranslations, LiquidLikeMotions, and RigidBodyRotations classes
   - Include all methods from original classes with tensor-based implementations
   - Ensure all matrix operations use PyTorch functions for differentiability
   - Add detailed TODOs explaining conversion approach for each method
   - Reference original implementation file and line numbers

2. **Create Scatter Module Stub (`eryx/scatter_torch.py`)**
   - Implement PyTorch versions of compute_form_factors, structure_factors_batch, and structure_factors
   - Convert array operations to tensor operations
   - Ensure proper handling of complex numbers for differentiability
   - Replace multiprocessing with PyTorch's parallelization mechanisms
   - Reference original NumPy implementations

3. **Create Map Utilities Stub (`eryx/map_utils_torch.py`)**
   - Implement PyTorch versions of grid generation, symmetry operations, and resolution calculations
   - Convert mesh grid and raveling operations to PyTorch equivalents
   - Ensure proper tensor shape handling
   - Maintain the same function signatures as NumPy versions

4. **Create Base Module Stub (`eryx/base_torch.py`)**
   - Implement PyTorch versions of transform calculations
   - Convert incoherent sum operations to use tensor operations
   - Ensure symmetry operations are differentiable
   - Maintain the same interfaces as NumPy versions

5. **Create Adapter Components (`eryx/adapters.py`)**
   - Implement PDBToTensor, GridToTensor, TensorToNumpy, and ModelAdapters classes
   - Create conversion functions for domain objects and arrays/tensors
   - Ensure gradient preservation during conversion
   - Handle complex data structures like symmetry operations

6. **Create PyTorch Utilities (`eryx/torch_utils.py`)**
   - Implement ComplexTensorOps, FFTOps, EigenOps, and GradientUtils classes
   - Ensure all operations support gradient flow
   - Create helpers for common operations with complex numbers
   - Implement differentiable versions of eigendecomposition

7. **Create Run Script (`eryx/run_torch.py`)**
   - Create PyTorch equivalent of run_debug.py
   - Support device placement (CPU/GPU)
   - Include functions to run and compare NumPy and PyTorch implementations
   - Provide logging similar to NumPy version

8. **Create PyTorch Testing Utilities (`eryx/autotest/torch_testing.py`)**
   - Extend autotest framework with PyTorch-specific functionality
   - Implement tensor comparison with tolerances
   - Create utilities for gradient checking
   - Support NumPy-PyTorch conversion for testing

9. **Create Example Tests (`tests/test_scatter_torch.py`)**
   - Create test stubs for PyTorch scatter functions
   - Include ground truth data generation
   - Implement tensor comparison and gradient validation
   - Set up disabled tests that can be enabled as implementation progresses

## Testing Strategy

### Ground Truth Generation
- Instrument NumPy functions using autotest.debug to capture inputs/outputs
- Run standard test cases to generate ground truth data
- Store serialized data for each function call
- Use captured data to validate PyTorch implementations

### Test Types
- Unit tests for each component
- Tensor comparison tests (tolerance-based)
- Gradient validation tests
- Integration tests for end-to-end simulation

## Final Deliverables

1. **Stub Files**
   - Complete `_torch.py` files with detailed stubs
   - Comprehensive TODOs in all functions
   - Type hints and docstrings that match NumPy versions

2. **Adapter Components**
   - NumPy-to-PyTorch conversion utilities
   - Domain-specific adapters for model classes

3. **Test Scaffolding**
   - Unit test stubs for all components
   - Test data generation utilities
   - PyTorch-specific test utilities

4. **Documentation**
   - Module-level docstrings explaining PyTorch adaptations
   - Function-level docstrings referencing NumPy equivalents
   - Implementation notes on differentiability
</file>
<file path="./specs/spec2.md" project="ptycho">
# Specification: Ground Truth Generation for PyTorch Port
> Ingest the information from this file, implement the Low-Level Tasks, and generate the code that will satisfy the High and Mid-Level Objectives.

## High-Level Objective

- Generate comprehensive ground truth data from the original NumPy implementation to enable accurate testing and validation of the PyTorch port

## Mid-Level Objectives

- Add `@debug` decorators to all designated functions in the original NumPy implementation
- Configure the autotest framework for capturing function inputs and outputs
- Implement ground truth data generation with multiple parameter sets
- Validate the captured data for completeness

## Implementation Notes

- Directly decorate original functions with `@debug` as specified in project rules
- Do NOT create wrapper functions or duplicate implementations
- Follow exactly the function list in `to_convert.json`
- Reuse the existing `run_np()` function in `run_debug.py` with minimal modifications
- Generate data with at least 3 parameter sets (small, medium, and default)
- Add comprehensive logging to verify data capture

## Context

### Beginning Context

- Original implementation files (`scatter.py`, `map_utils.py`, `models.py`, etc.)
- `run_debug.py` with `run_np()` function
- Existing autotest framework in `eryx/autotest/`
- `to_convert.json` listing functions to be ported

### Ending Context

- Modified source files with `@debug` decorators
- Configuration file for autotest at `eryx/autotest_config.py`
- Modified `run_np()` function supporting parameter variations
- Ground truth generation script at `eryx/scripts/generate_ground_truth.py`
- Generated ground truth data in configured log directory
- Verification of captured data

## Low-Level Tasks
> Ordered from start to finish

1. Add Debug Decorators to Scatter Module Functions
```aider
UPDATE eryx/scatter.py:
    ADD import statement: from eryx.autotest.debug import debug
    
    ADD @debug decorator to the following functions:
    - compute_form_factors(q_grid, ff_a, ff_b, ff_c)
    - structure_factors_batch(q_grid, xyz, ff_a, ff_b, ff_c, U=None, ...)
    - structure_factors(q_grid, xyz, ff_a, ff_b, ff_c, U=None, ...)
    
    DO NOT modify the function implementations, only add the decorators
```

2. Add Debug Decorators to Map Utilities Module Functions
```aider
UPDATE eryx/map_utils.py:
    ADD import statement: from eryx.autotest.debug import debug
    
    ADD @debug decorator to the following functions:
    - generate_grid(A_inv, hsampling, ksampling, lsampling, return_hkl=False)
    - get_symmetry_equivalents(hkl_grid, sym_ops)
    - get_ravel_indices(hkl_grid_sym, sampling)
    - compute_resolution(cell, hkl)
    - get_resolution_mask(cell, hkl_grid, res_limit)
    - get_dq_map(A_inv, hkl_grid)
    - get_centered_sampling(map_shape, sampling)
    - resize_map(new_map, old_sampling, new_sampling)
    
    DO NOT modify the function implementations, only add the decorators
```

3. Add Debug Decorators to OnePhonon Class Methods
```aider
UPDATE eryx/models.py:
    ADD import statement: from eryx.autotest.debug import debug
    
    ADD @debug decorator to the following methods of the OnePhonon class:
    - __init__(self, pdb_path, hsampling, ksampling, lsampling, expand_p1=True, group_by='asu', res_limit=0., model='gnm', gnm_cutoff=4., gamma_intra=1., gamma_inter=1., batch_size=10000, n_processes=8)
    - _setup(self, pdb_path, expand_p1, res_limit, group_by)
    - _setup_phonons(self, pdb_path, model, gnm_cutoff, gamma_intra, gamma_inter)
    - _build_A(self)
    - _build_M(self)
    - _build_M_allatoms(self)
    - _project_M(self, M_allatoms)
    - _build_kvec_Brillouin(self)
    - _center_kvec(self, x, L)
    - _at_kvec_from_miller_points(self, hkl_kvec)
    - compute_gnm_phonons(self)
    - compute_hessian(self)
    - compute_covariance_matrix(self)
    - apply_disorder(self, rank=-1, outdir=None, use_data_adp=False)
    
    DO NOT modify the method implementations, only add the decorators
```

4. Add Debug Decorators to Base Module Functions
```aider
UPDATE eryx/base.py:
    ADD import statement: from eryx.autotest.debug import debug
    
    ADD @debug decorator to the following functions:
    - compute_molecular_transform(pdb_path, hsampling, ksampling, lsampling, U=None, expand_p1=True, expand_friedel=True, res_limit=0, batch_size=10000, n_processes=8)
    - compute_crystal_transform(pdb_path, hsampling, ksampling, lsampling, U=None, expand_p1=True, res_limit=0, batch_size=5000, n_processes=8)
    - incoherent_sum_real(model, hkl_grid, sampling, U=None, mask=None, batch_size=10000, n_processes=8)
    - incoherent_sum_reciprocal(model, hkl_grid, sampling, U=None, batch_size=10000, n_processes=8)
    
    DO NOT modify the function implementations, only add the decorators
```

5. Add Debug Decorators to Additional Model Classes
```aider
UPDATE eryx/models.py:
    ADD @debug decorator to the following methods in RigidBodyTranslations class:
    - __init__(self, pdb_path, hsampling, ksampling, lsampling, expand_friedel=True, res_limit=0, batch_size=10000, n_processes=8)
    - _setup(self, pdb_path, expand_friedel, res_limit, batch_size, n_processes)
    - apply_disorder(self, sigmas)
    - optimize(self, target, sigmas_min, sigmas_max, n_search=20)
    
    ADD @debug decorator to the following methods in LiquidLikeMotions class:
    - __init__(self, pdb_path, hsampling, ksampling, lsampling, expand_p1=True, border=1, res_limit=0, batch_size=5000, n_processes=8, asu_confined=False)
    - _setup(self, pdb_path, expand_p1, border, res_limit, batch_size, n_processes, asu_confined)
    - fft_convolve(self, transform, kernel)
    - apply_disorder(self, sigmas, gammas)
    - optimize(self, target, sigmas_min, sigmas_max, gammas_min, gammas_max, ns_search=20, ng_search=10)
    
    ADD @debug decorator to the following methods in RigidBodyRotations class:
    - __init__(self, pdb_path, hsampling, ksampling, lsampling, expand_p1=True, res_limit=0, batch_size=10000, n_processes=8)
    - _setup(self, pdb_path, expand_p1, res_limit)
    - generate_rotations_around_axis(sigma, num_rot, axis=np.array([0,0,1.0]))
    - apply_disorder(self, sigmas, num_rot=100, ensemble_dir=None)
    - optimize(self, target, sigma_min, sigma_max, n_search=20, num_rot=100)
    
    DO NOT modify the method implementations, only add the decorators
```

6. Add Debug Decorators to PDB Module Functions
```aider
UPDATE eryx/pdb.py:
    ADD import statement: from eryx.autotest.debug import debug
    
    ADD @debug decorator to the following functions and methods:
    - sym_str_as_matrix(sym_str)
    
    In AtomicModel class:
    - _get_xyz_asus(self, xyz)
    - flatten_model(self)
    
    In Crystal class:
    - get_asu_xyz(self, asu_id=0, unit_cell=None)
    
    In GaussianNetworkModel class:
    - __init__(self, pdb_path, enm_cutoff, gamma_intra, gamma_inter)
    - _setup_atomic_model(self, pdb_path)
    - _setup_gaussian_network_model(self)
    - build_gamma(self)
    - build_neighbor_list(self)
    - compute_hessian(self)
    - compute_K(self, hessian, kvec=None)
    - compute_Kinv(self, hessian, kvec=None, reshape=True)
    
    DO NOT modify the function implementations, only add the decorators
```

7. Add Debug Decorators to Reference Module Functions
```aider
UPDATE eryx/reference.py:
    ADD import statement: from eryx.autotest.debug import debug
    
    ADD @debug decorator to the following functions:
    - structure_factors(q_grid, xyz, elements, U=None)
    - diffuse_covmat(q_grid, xyz, elements, V)
    
    DO NOT modify the function implementations, only add the decorators
```

8. Create Autotest Configuration
```aider
CREATE eryx/autotest_config.py:
    IMPLEMENT a configuration file that:
    - Imports Configuration from eryx.autotest.configuration
    - Sets debug mode to True
    - Sets log directory to "ground_truth_data"
    - Ensures the log directory exists
    
```

9. Modify Run Debug Function for Parameter Variations
```aider
UPDATE eryx/run_debug.py:
    MODIFY run_np function to:
    - Accept an optional 'variant' parameter for different test cases
    - Support 'small', 'medium', and default parameter sets
    - Log parameter choices
    - Save output with variant-specific filename
    
    DO NOT change the core computation logic
    ENSURE the function remains compatible with existing code
```

10. Create Ground Truth Generation Script
```aider
CREATE eryx/scripts/generate_ground_truth.py:
    IMPLEMENT a script that:
    - Configures autotest debug mode appropriately
    - Imports the run_np function from run_debug and runs it
    - Verifies that log files were created
```

11. Verify Ground Truth Data Capture
```aider
CREATE eryx/scripts/verify_ground_truth.py:
    IMPLEMENT a script that:
    - Searches the ground truth data directory for log files
    - Groups log files by function
    - Counts the number of captured function calls for each function
    - Verifies that important functions have been captured
    - Reports any functions from to_convert.json that are missing logs
    - Prints a summary of the ground truth data capture
```
</file>
<file path="./eryx/visuals.py" project="ptycho">
import matplotlib.pyplot as plt
import matplotlib.colors as mcolors
from matplotlib.gridspec import GridSpec
from matplotlib import cm
import numpy as np
import plotly.graph_objects as go
import plotly.express as px

def visualize_central_slices(I, vmax_scale=5, contour=False, contour_cmap=None, output=None, ylabel=None):
    """
    Plot central slices from the input map,  assuming
    that the map is centered around h,k,l=(0,0,0).

    Parameters
    ----------
    I : numpy.ndarray, 3d
        intensity map
    vmax_scale : float
        vmax will be vmax_scale*mean(I)
    output : str
        path to save figure to
    ylabel : str
        label for leftmost y axis
    """
    f, (ax1,ax2,ax3) = plt.subplots(1, 3, figsize=(12,4))
    map_shape = I.shape
    vmax = I[~np.isnan(I)].mean()*vmax_scale

    if contour:
        if contour_cmap is None:
            contour_cmap = 'viridis'
        ax1.contourf(I[int(map_shape[0] / 2), :, :], origin='upper', cmap=contour_cmap)
        ax2.contourf(I[:, int(map_shape[1] / 2), :], origin='upper', cmap=contour_cmap)
        ax3.contourf(I[:, :, int(map_shape[2] / 2)], origin='upper', cmap=contour_cmap)

        ax1.set_title("View along h", fontsize=14)
        ax2.set_title("View along k", fontsize=14)
        ax3.set_title("View along l", fontsize=14)
        if ylabel is not None:
            ax1.set_ylabel(ylabel, fontsize=14)
        
    else:
        ax1.imshow(I[int(map_shape[0]/2),:,:], vmax=vmax)
        ax2.imshow(I[:,int(map_shape[1]/2),:], vmax=vmax)
        ax3.imshow(I[:,:,int(map_shape[2]/2)], vmax=vmax)

        ax1.set_title("(0,k,l)", fontsize=14)
        ax2.set_title("(h,0,l)", fontsize=14)
        ax3.set_title("(h,k,0)", fontsize=14)
        if ylabel is not None:
            ax1.set_ylabel(ylabel, fontsize=14)

    ax1.set_aspect(map_shape[2]/map_shape[1])
    ax2.set_aspect(map_shape[2]/map_shape[0])
    ax3.set_aspect(map_shape[1]/map_shape[0])

    for ax in [ax1,ax2,ax3]:
        ax.set_xticks([])
        ax.set_yticks([])

    if output is not None:
        f.savefig(output, bbox_inches='tight', dpi=300)

def visualize_brillouin_zone(Id, hkl, hsampling, ksampling, lsampling, hkl_grid=None,
                             A_inv=None, plot_3d=False, output=None, ylabel=None):
    """
    Visualize slices through a specific reciprocal lattice point. 
    
    Parameters
    ----------
    Id : numpy.ndarray, 3d
        diffuse intensity map
    hkl : tuple, shape (3)
        reciprocal lattice point of interest
    hsampling : tuple, shape (3,)
        (hmin, hmax, oversampling) relative to Miller indices
    ksampling : tuple, shape (3,)
        (kmin, kmax, oversampling) relative to Miller indices
    lsampling : tuple, shape (3,)
        (lmin, lmax, oversampling) relative to Miller indices
    hkl_grid : np.array, shape (n_grid_points, 3)
        grid of hkl vectors if precomputed
    A_inv : numpy.ndarray, shape (3,3)
        fractional cell orthogonalization matrix
    plot_3d : bool
        if True, plot 3d surfacse rather than 2d contour plots
    output : str
        path to save figure to
    ylabel : str
        ylabel for leftmost plot, not used in plot_3d mode
    """
    if hkl_grid is None:
        if A_inv is None:
            raise ValueError("If hkl_grid is None, A_inv must be supplied.")
        hkl_grid, map_shape = generate_grid(A_inv, 
                                            hsampling,
                                            ksampling, 
                                            lsampling, 
                                            return_hkl=True)
    else:
        map_shape = Id.shape
        
    # find origin and then center of (h,k,l) of interest
    origin = np.argmin(np.sum(np.abs(hkl_grid), axis=1))
    origin = np.unravel_index(origin, map_shape)
    center = origin + np.array([hkl[0] * hsampling[2], hkl[1] * lsampling[2], hkl[2] * ksampling[2]])
    center = np.around(center).astype(int)
    
    hwidths = (int(hsampling[2]/2), int(ksampling[2]/2), int(lsampling[2]/2))
    bzone = Id[center[0]-hwidths[0]:center[0]+hwidths[0]+1,
               center[1]-hwidths[1]:center[1]+hwidths[1]+1,
               center[2]-hwidths[2]:center[2]+hwidths[2]+1].copy()
    
    if not plot_3d:
        visualize_central_slices(bzone, contour=True, output=output, ylabel=ylabel)
        
    else:
        bzone[np.isnan(bzone)] = 0
        hkl_grid_reshape = hkl_grid.reshape(map_shape + (3,))
        hklzone = hkl_grid_reshape[center[0]-hwidths[0]:center[0]+hwidths[0]+1,
                                   center[1]-hwidths[1]:center[1]+hwidths[1]+1,
                                   center[2]-hwidths[2]:center[2]+hwidths[2]+1]
        
        f, (ax1,ax2,ax3) = plt.subplots(1,3, figsize=(12,4), subplot_kw={"projection": "3d"})

        max_val = bzone.max()
        X,Y = hklzone[hwidths[0],:,:][:,:,1], hklzone[hwidths[0],:,:][:,:,2]
        ax1.plot_surface(X,Y,bzone[hwidths[0],:,:]/max_val, cmap=cm.coolwarm)
        ax1.set_xlabel("k", fontsize=12)
        ax1.set_ylabel("l", fontsize=12)

        X,Y = hklzone[:,hwidths[1],:][:,:,0], hklzone[:,hwidths[1],:][:,:,2]
        ax2.plot_surface(X,Y,bzone[:,hwidths[1],:]/max_val, cmap=cm.coolwarm)
        ax2.set_xlabel("h", fontsize=12)
        ax2.set_ylabel("l", fontsize=12)

        X,Y = hklzone[:,:,hwidths[2]][:,:,0], hklzone[:,:,hwidths[2]][:,:,1]
        ax3.plot_surface(X,Y,bzone[:,:,hwidths[2]]/max_val, cmap=cm.coolwarm)
        ax3.set_xlabel("h", fontsize=12)
        ax3.set_ylabel("k", fontsize=12)
        ax3.set_zlabel("Intensity", fontsize=12)

        hklzone = hkl_grid_reshape[center[0]-hwidths[0]:center[0]+hwidths[0]+1,
                                   center[1]-hwidths[1]:center[1]+hwidths[1]+1,
                                   center[2]-hwidths[2]:center[2]+hwidths[2]+1]
        
        if output is not None:
            f.savefig(output, bbox_inches='tight', dpi=300)
        
def slice_traversal(I, hkl_grid, traversed_index=0, traversed_range=None):
    """
    Create animation traversing through the h, k, or l planes
    of the input intensity map. 
    """
    if traversed_range is None:
        traversed_range = np.arange(I.shape[traversed_index])

    if traversed_index == 0:
        fig = px.imshow(I[traversed_range[0]:traversed_range[-1]+1,:,:], animation_frame=0)
    elif traversed_index == 1:
        fig = px.imshow(I[:,traversed_range[0]:traversed_range[-1]+1,:], animation_frame=1)
    else:
        fig = px.imshow(I[:,:,traversed_range[0]:traversed_range[-1]+1], animation_frame=2)

    for i in range(traversed_range.shape[0]):
        if traversed_index == 0:
            hklval = hkl_grid[:,0].reshape(I.shape)[traversed_range[i],0,0]
            fig["frames"][i]["layout"]["title"] = f'h={hklval:.2f}'
        elif traversed_index == 1:
            hklval = hkl_grid[:,0].reshape(I.shape)[0,traversed_range[i],0]
            fig["frames"][i]["layout"]["title"] = f'k={hklval:.2f}'
        else:
            hklval = hkl_grid[:,0].reshape(I.shape)[0,0,traversed_range[i]]
            fig["frames"][i]["layout"]["title"] = f'l={hklval:.2f}'

    fig.show()

class VisualizeCrystal:

    def __init__(self, crystal, gnm=None, nidm=None, onephonon=None):
        """
        Plotly helper functions to visualize the Crystal object.
        Parameters
        ----------
        crystal : eryx.pdb.Crystal object
        gnm : (optional) eryx.pdb.GaussianNetworkModel object
        nidm : (optional) eryx.models.NonInteractingDeformableMolecules object
        """
        self.crystal = crystal
        self.draw_data = None
        self.color_by = 'asu_id'
        self.color_palette = 'xkcd'
        self.opacity = 1.0
        self.network_width = 10
        self.gnm = gnm   # if not None, show inter ASU contacts
        if self.gnm is not None:
            self.gnm_contacts_indices = self._setup_gnm_contacts()
        self.nidm = nidm # if not None, show intra ASU covariances
        if self.nidm is not None:
            self.nidm_covar_indices = self._setup_nidm_covar()
        self.onephonon = onephonon
        if self.onephonon is not None:
            self.onephonon_covar_indices = self._setup_onephonon_covar()

    def show(self, background=True):
        """
        Show the crystal's supercell in a plotly figure.
        """
        self.draw_supercell()
        self.fig.update_scenes(xaxis_visible=background,
                               yaxis_visible=background,
                               zaxis_visible=background)
        self.fig.show()

    def draw_supercell(self):
        """
        For every cell in the crystal' supercell,
        draw its cell axes and the asymmetric units it contains.
        """
        for h in self.crystal.xrange:
            for k in self.crystal.yrange:
                for l in self.crystal.zrange:
                    self.draw_unit_cell_axes(origin=self.crystal.get_unitcell_origin(unit_cell=[h,k,l]))
                    for i_asu in np.arange(self.crystal.model.n_asu):
                        self.draw_asu(i_asu, unit_cell=[h,k,l],
                                      name=f'Cell #{self.crystal.hkl_to_id(unit_cell=[h,k,l])} | ASU #{i_asu}')
                        if self.gnm is not None:
                            self._draw_network(self.gnm_contacts_indices,
                                               i_asu, unit_cell=[h,k,l])
                        if self.nidm is not None:
                            self._draw_network(self.nidm_covar_indices,
                                               i_asu, unit_cell=[h,k,l])
                        if self.onephonon is not None:
                            self._draw_network(self.onephonon_covar_indices,
                                               i_asu, unit_cell=[h,k,l])

    def draw_unit_cell_axes(self, origin=np.array([0., 0., 0.]), showlegend=False):
        """
        Draw the axes of a cell as a Scatter3D plotly object.

        Parameters
        ----------
        origin : numpy.ndarray, shape (3,), default: np.array([0.,0.,0.])
            3d coordinate of the unit cell origin.
        showlegend : bool, default: False
            Whether the object appears in the legend or not.
        """
        u_xyz = self.crystal.model.unit_cell_axes
        p_xyz = np.zeros((16, 3))
        p_sign_sequence = [1, 1, -1, 1, 1, -1, -1, 1, -1, -1, 1, 1, -1, 1, 1]
        p_id_sequence = [2, 1, 2, 0, 2, 0, 1, 0, 2, 0, 1, 0, 1, 2, 1]
        p_xyz[0] = origin
        for i in range(15):
            p_xyz[i + 1] = p_xyz[i] + p_sign_sequence[i] * u_xyz[p_id_sequence[i]]
        self._draw_go_vector(self._build_go_vector(p_xyz,
                                                   line=dict(color="gray", width=1),
                                                   showlegend=showlegend))

    def draw_asu(self, asu_id=0, unit_cell=None, name='asu0', showlegend=True):
        """
        Draw an asymmetric unit as a Scatter3d plotly object.

        Parameters
        ----------
        asu_id : asymmetric unit index
        unit_cell : list of 3 integer indices.
            Index of the unit cell along the 3 dimensions.
        name : string, default: 'asu0'
            Name displayed in legend.
        showlegend : bool, default: True
            Whether the object appears in the legend or not
        """
        if unit_cell is None:
            unit_cell = [0, 0, 0]
        self._draw_go_vector(self._build_go_vector(self.crystal.get_asu_xyz(asu_id, unit_cell),
                                                   mode='markers',
                                                   marker=dict(size=5,
                                                               color=self._get_color(asu_id, unit_cell),
                                                               opacity=self.opacity),
                                                   name=name,
                                                   showlegend=showlegend))

    def _setup_gnm_contacts(self):
        indices = []
        for i_asu in range(self.crystal.model.n_asu):
            indices.append([])
            for j_cell in range(self.crystal.n_cell):
                indices[i_asu].append([])
                for j_asu in range(self.crystal.model.n_asu):
                    indices[i_asu][j_cell].append([])
                    if i_asu == j_asu and j_cell == self.crystal.hkl_to_id([0, 0, 0]):
                        indices[i_asu][j_cell][j_asu].append([])
                    else:
                        indices[i_asu][j_cell][j_asu] = self.gnm.asu_neighbors[i_asu][j_cell][j_asu]
        return indices

    def _setup_nidm_covar(self):
        threshold = np.mean(self.nidm.covar.flatten()) \
                    + 5*np.std(self.nidm.covar.flatten())
        asu_indices = []
        pairs = np.where(self.nidm.covar > threshold)
        for i in range(self.crystal.get_asu_xyz().shape[0]):
            i_indices = np.where(pairs[0]==i)
            j = pairs[1][i_indices]
            j_indices = np.where(pairs[1][i_indices]>i)[0]
            asu_indices.append(j[j_indices].tolist())

        indices = []
        for i_asu in range(self.crystal.model.n_asu):
            indices.append([])
            for j_cell in range(self.crystal.n_cell):
                indices[i_asu].append([])
                for j_asu in range(self.crystal.model.n_asu):
                    indices[i_asu][j_cell].append([])
                    if i_asu == j_asu and j_cell == self.crystal.hkl_to_id([0,0,0]):
                        indices[i_asu][j_cell][j_asu] = asu_indices
        return indices

    def _setup_onephonon_covar(self):
        # inter first
        covar_inter = np.copy(self.onephonon.covar)
        covar_inter[:,:,self.onephonon.crystal.hkl_to_id([0,0,0]),:,:] *= 0.
        threshold = np.mean(covar_inter.flatten()) + 2*np.std(covar_inter.flatten())

        indices = []
        for i_asu in range(self.onephonon.n_asu):
            indices.append([])
            for j_cell in range(self.onephonon.n_cell):
                indices[i_asu].append([])
                for j_asu in range(self.onephonon.n_asu):
                    indices[i_asu][j_cell].append([])
                    asu_indices = []
                    pairs = np.where(covar_inter[i_asu,:,j_cell,j_asu,:] > threshold)
                    for i in range(self.crystal.get_asu_xyz().shape[0]):
                        i_indices = np.where(pairs[0]==i)
                        j = pairs[1][i_indices]
                        #j_indices = np.where(pairs[1][i_indices]>i)[0]
                        asu_indices.append(j.tolist())
                    indices[i_asu][j_cell][j_asu] = asu_indices
        return indices


    def _draw_network(self, indices, asu_id=0, unit_cell=None, showlegend=False):
        if unit_cell is None:
            unit_cell = [0, 0, 0]
        xyz = np.zeros((2, 3))
        for i_asu in range(self.crystal.model.n_asu):
            pairs = indices[i_asu][self.crystal.hkl_to_id(unit_cell)][asu_id]
            for i_at in range(len(pairs)):
                if len(pairs[i_at]) == 0:
                    continue
                for j_at in pairs[i_at]:
                    xyz[0] = self.crystal.get_asu_xyz(i_asu,[0,0,0])[i_at]
                    xyz[1] = self.crystal.get_asu_xyz(asu_id,unit_cell)[j_at]
                    self._draw_go_vector(
                        self._build_go_vector(xyz=xyz,
                                              line=dict(width=self.network_width,
                                                        color='black'),
                                              showlegend=showlegend))

    def _draw_go_vector(self, go_vector):
        if self.draw_data is None:
            self.draw_data = [go_vector]
        else:
            self.draw_data.append(go_vector)
        self.fig = go.Figure(data=self.draw_data)

    def _build_go_vector(self, xyz, mode='lines', line=None, marker=None,
                         name=None, showlegend=True):
        if line is None:
            line = {}
        if marker is None:
            marker = {}
        return go.Scatter3d(x=xyz[:, 0],
                            y=xyz[:, 1],
                            z=xyz[:, 2],
                            mode=mode,
                            line=line,
                            marker=marker,
                            name=name,
                            showlegend=showlegend)

    def _get_color(self, asu_id, unit_cell=None):
        """
        Return the ASU color.

        Parameters
        ----------
        asu_id : asymmetric unit index
        unit_cell : list of 3 integer indices.
            Index of the unit cell along the 3 dimensions.
        """
        if unit_cell is None:
            unit_cell = [0, 0, 0]
        idx = 0
        ndx = 1
        if self.color_by == 'asu_id':
            idx = asu_id
            ndx = self.crystal.model.n_asu
        elif self.color_by == 'unit_cell':
            idx = self.crystal.hkl_to_id(unit_cell)
            ndx = self.crystal.n_cell
        else:
            return self.color_by
        if self.color_palette == 'xkcd':
            color_dict = mcolors.XKCD_COLORS
        elif self.color_palette == 'tableau':
            color_dict = mcolors.TABLEAU_COLORS
        else:
            color_dict = mcolors.CSS4_COLORS
        color_array = np.array(list(color_dict.items()))
        return color_array[::color_array.shape[0]//ndx,1][idx]

class PhononPlots:

    def __init__(self, phonon):
        self.phonon = phonon

    def _get_dispersion(self, h=True, k=True, l=True):
        w = np.sqrt(1. / np.real(self.phonon.Winv))
        k_norm = np.zeros((self.phonon.hsampling[2]))
        w_curve = np.zeros((self.phonon.hsampling[2], w.shape[-1]))
        for i in range(self.phonon.hsampling[2]):
            w_curve[i] = w[h * i, k * i, l * i]
            k_norm[i] = self.phonon.kvec_norm[h * i, k * i, l * i]
        return k_norm, w_curve

    def dispersion_curve(self):
        nrows = 2
        ncols = 4
        fig = plt.figure(figsize=(2 * ncols, 4 * nrows), dpi=180,
                         constrained_layout=True)
        gs = GridSpec(nrows, ncols, figure=fig)

        title   = ['0->h','0->k','0->l','0->h+k','0->h+l','0->k+l','0->h+k+l']
        h_curve = [True,  False, False, True,  True,  False, True]
        k_curve = [False, True,  False, True,  False, True,  True]
        l_curve = [False, False, True,  False, True,  True,  True]

        for i_curve in range(8):
            gs_j = i_curve % ncols
            gs_i = i_curve // ncols
            ax = fig.add_subplot(gs[gs_i, gs_j])
            if i_curve == 0:
                ax_save = ax
            if i_curve < 7:
                ax.sharex(ax_save)
                ax.sharey(ax_save)
                knorm, wvec = self._get_dispersion(h=h_curve[i_curve],
                                                   k=k_curve[i_curve],
                                                   l=l_curve[i_curve])
                for i in range(wvec.shape[-1]):
                    ax.plot(knorm, wvec[:, i], 'o', label=f'#{i}')
                    if gs_i == 1:
                        ax.set_xlabel('phonon wavevector ($\mathrm{\AA}^{-1}$)')
                    if gs_j == 0:
                        ax.set_ylabel('phonon frequency')
                if i_curve == 3:
                    ax.legend(bbox_to_anchor=(1.1,0.5))
                ax.set_title(title[i_curve])
            else:
                ax.hist(np.sqrt(1. / np.real(self.phonon.Winv).flatten()),
                        bins=50, orientation='horizontal')
                ax.set_title('density of states')
        plt.tight_layout()
        plt.show()

    def contribution_curve(self):
        nrows=2
        ncols=3
        fig = plt.figure(figsize=(2 * ncols, 3 * nrows), dpi=180,
                         constrained_layout=True)
        gs = GridSpec(nrows, ncols, figure=fig)
        knorm = self.phonon.kvec_norm.reshape(-1,1)
        Winv2 = np.real(self.phonon.Winv).reshape(-1,6)
        Vvec  = self.phonon.V.reshape(-1,6,6)
        A = ['x', 'y', 'z', 'r1', 'r2', 'r3']

        for i_curve in range(6):
            gs_j = i_curve % ncols
            gs_i = i_curve // ncols
            ax = fig.add_subplot(gs[gs_i, gs_j])
            if i_curve == 0:
                ax_save = ax
            ax.sharex(ax_save)
            ax.sharey(ax_save)
            for i in range(6):
                ax.plot(knorm,
                        Winv2[:,i_curve]*np.abs(Vvec[:,i,i_curve]),
                        '.', label=f'{A[i]}')
            ax.set_title(f'#{i_curve}')
            if gs_i == 1:
                ax.set_xlabel('phonon wavevector ($\mathrm{\AA}^{-1}$)')
            if gs_j == 0:
                ax.set_ylabel('phonon intensity')
            ax.set_yscale('log')
            if i_curve == 2:
                ax.legend(bbox_to_anchor=(2.1,0.5))
        plt.show()

class DeltaPDF:

    def __init__(self, disorder_model, Id=None, fill_bragg=True):
        self.disorder_model = disorder_model
        self.q_grid = self.disorder_model.q_grid
        self.hsampling = self.disorder_model.hsampling
        self.ksampling = self.disorder_model.ksampling
        self.lsampling = self.disorder_model.lsampling
        self.map_shape = self.disorder_model.map_shape
        self.pdf = None
        if Id is not None:
            self.Id = Id
        else:
            self.Id = self.disorder_model.apply_disorder()
        if fill_bragg:
            self._fill_integral_Miller_points()
        self._subtract_radial_average()

    def _fill_integral_Miller_points(self):
        Id_filled = np.copy(self.Id)
        for q in self._at_kvec_from_miller_points((0, 0, 0)):
            if q < Id_filled.shape[0] - 1:
                Id_filled[q] = 0.5 * Id_filled[q - 1] + 0.5 * Id_filled[q + 1]
            else:
                Id_filled[q] = Id_filled[q - 1]
        self.Id = Id_filled

    def _at_kvec_from_miller_points(self, hkl_kvec):
        """
        Return the indices of all q-vector that are k-vector away from any
        Miller index in the map.

        Parameters
        ----------
        hkl_kvec : tuple of ints
            fractional Miller index of the desired k-vector
        """
        hsteps = int(self.hsampling[2] * (self.hsampling[1] - self.hsampling[0]) + 1)
        ksteps = int(self.ksampling[2] * (self.ksampling[1] - self.ksampling[0]) + 1)
        lsteps = int(self.lsampling[2] * (self.lsampling[1] - self.lsampling[0]) + 1)

        index_grid = np.mgrid[
                     hkl_kvec[0]:hsteps:self.hsampling[2],
                     hkl_kvec[1]:ksteps:self.ksampling[2],
                     hkl_kvec[2]:lsteps:self.lsampling[2]]

        return np.ravel_multi_index((index_grid[0].flatten(),
                                     index_grid[1].flatten(),
                                     index_grid[2].flatten()),
                                    self.map_shape)

    def _subtract_radial_average(self):
        q2 = np.linalg.norm(self.q_grid, axis=1) ** 2
        q2_unique, q2_unique_inverse = np.unique(np.round(q2, 2),
                                                 return_inverse=True)
        for i in range(q2_unique.shape[0]):
            self.Id[np.round(q2, 2) == q2_unique[i]] -= \
                np.mean(self.Id[np.round(q2, 2) == q2_unique[i]])

    def compute_patterson(self):
        np.nan_to_num(self.Id, copy=False, nan=0.0)
        self.pdf = np.real(np.fft.fftshift(np.fft.fftn(np.fft.ifftshift(self.Id.reshape(self.disorder_model.map_shape)))))

    def show(self, contour=False):
        if self.pdf is None:
            self.compute_patterson()
        visualize_central_slices(self.pdf, contour=contour, contour_cmap='seismic')

</file>
<file path="./eryx/models.py" project="ptycho">
import numpy as np
import scipy.signal
import scipy.spatial
from scipy.linalg import block_diag
import glob
import os
from tqdm import tqdm
from .pdb import AtomicModel, Crystal, GaussianNetworkModel
from .map_utils import *
from .scatter import structure_factors
from .stats import compute_cc
from .base import compute_molecular_transform, compute_crystal_transform
from eryx.autotest.debug import debug

class RigidBodyTranslations:
    
    @debug
    def __init__(self, pdb_path, hsampling, ksampling, lsampling, expand_friedel=True, res_limit=0, batch_size=10000, n_processes=8):
        self.hsampling = hsampling
        self.ksampling = ksampling
        self.lsampling = lsampling
        self._setup(pdb_path, expand_friedel, res_limit, batch_size, n_processes)
        
    @debug
    def _setup(self, pdb_path, expand_friedel=True, res_limit=0, batch_size=10000, n_processes=8):
        """
        Set up class, including computing the molecular transform.
        
        Parameters
        ----------
        pdb_path : str
            path to coordinates file of asymmetric unit
        expand_friedel : bool
            if True, expand to include portion of reciprocal space related by Friedel's law
        res_limit : float
            high resolution limit
        batch_size : int     
            number of q-vectors to evaluate per batch
        n_processes : int
            number of processors for structure factor calculation
        """
        self.q_grid, self.transform = compute_molecular_transform(pdb_path, 
                                                                  self.hsampling, 
                                                                  self.ksampling, 
                                                                  self.lsampling,
                                                                  expand_friedel=expand_friedel,
                                                                  res_limit=res_limit,
                                                                  batch_size=batch_size,
                                                                  n_processes=n_processes)
        self.transform[self.transform==0] = np.nan # compute_cc expects masked values to be np.nan
        self.q_mags = np.linalg.norm(self.q_grid, axis=1)
        self.map_shape = self.transform.shape

        self.scan_sigmas = []
        self.scan_ccs = []

    def plot_scan(self, output=None):
        """
        Plot results of scan, CC(sigma).

        Parameters
        ----------
        output : str
            if provided, save plot to given path
        """
        import matplotlib.pyplot as plt

        sigmas = np.array(self.scan_sigmas)
        ccs = np.array(self.scan_ccs)
        plt.scatter(sigmas, ccs, c='black')
        plt.plot(sigmas, ccs, c='black')
        plt.xlabel("$\sigma$ ($\mathrm{\AA}$)", fontsize=14)
        plt.ylabel("CC", fontsize=14)

        if output is not None:
            plt.savefig(output, dpi=300, bbox_inches='tight')
        
    @debug
    def apply_disorder(self, sigmas):
        """
        Compute the diffuse map(s) from the molecular transform:
        I_diffuse = I_transform * (1 - q^2 * sigma^2)
        for a single sigma or set of (an)isotropic sigmas.

        Parameters
        ----------
        sigma : float or array of shape (n_sigma,) or (n_sigma, 3)
            (an)isotropic displacement parameter for asymmetric unit 

        Returns
        -------
        Id : numpy.ndarray, (n_sigma, q_grid.shape[0])
            diffuse intensity maps for the corresponding sigma(s)
        """
        if type(sigmas) == float:
            sigmas = np.array([sigmas])

        if len(sigmas.shape) == 1:
            wilson = np.square(self.q_mags) * np.square(sigmas)[:,np.newaxis]
        else:
            wilson = np.sum(self.q_grid.T * np.dot(np.square(sigmas)[:,np.newaxis] * np.eye(3), self.q_grid.T), axis=1)

        Id = self.transform.flatten() * (1 - np.exp(-1 * wilson))
        return Id
    
    @debug
    def optimize(self, target, sigmas_min, sigmas_max, n_search=20):
        """
        Scan to find the sigma that maximizes the overall Pearson
        correlation between the target and computed maps. 
        
        Parameters
        ----------
        target : numpy.ndarray, 3d
            target map, of shape self.map_shape
        sigmas_min : float or tuple of shape (3,)
            lower bound of (an)isotropic sigmas
        sigmas_max : float or tuple of shape (3,)
            upper bound of (an)isotropic sigmas, same type/dimension as sigmas_min
        n_search : int
            sampling frequency between sigmas_min and sigmas_max
        
        Returns
        -------
        ccs : numpy.ndarray, shape (n_search,)
            Pearson correlation coefficients to target maps
        sigmas : numpy.ndarray, shape (n_search,) or (n_search, n_search, n_search)
            sigmas that were scanned over, ordered as ccs
        """
        assert target.shape == self.map_shape
        
        if (type(sigmas_min) == float) and (type(sigmas_max) == float):
            sigmas = np.linspace(sigmas_min, sigmas_max, n_search)
        else:
            sa, sb, sc = [np.linspace(sigmas_min[i], sigmas_max[i], n_search) for i in range(3)]
            sigmas = np.array(list(itertools.product(sa, sb, sc)))
        
        Id = self.apply_disorder(sigmas)
        ccs = compute_cc(Id, np.expand_dims(target.flatten(), axis=0))
        opt_index = np.argmax(ccs)
        self.opt_sigma = sigmas[opt_index]
        self.opt_map = Id[opt_index].reshape(self.map_shape)
        self.scan_sigmas.extend(list(sigmas))
        self.scan_ccs.extend(list(ccs))
        
        print(f"Optimal sigma: {self.opt_sigma}, with correlation coefficient {ccs[opt_index]:.4f}")
        return ccs, sigmas

class LiquidLikeMotions:
    
    """
    Model in which collective motions decay exponentially with distance
    across the crystal. Mathematically the predicted diffuse scattering 
    is the convolution between the crystal transform and a disorder kernel.
    In the asu_confined regime, disorder is confined to the asymmetric unit
    and the convolution is with the molecular rather than crystal transform.
    """
    
    @debug
    def __init__(self, pdb_path, hsampling, ksampling, lsampling, expand_p1=True, 
                 border=1, res_limit=0, batch_size=5000, n_processes=8, asu_confined=False):
        self.hsampling = hsampling
        self.ksampling = ksampling
        self.lsampling = lsampling
        self._setup(pdb_path, expand_p1, border, res_limit, batch_size, n_processes, asu_confined)
                
    @debug
    def _setup(self, pdb_path, expand_p1, border, res_limit, batch_size, n_processes, asu_confined):
        """
        Set up class, including calculation of the crystal or molecular 
        transform for the classic and asu-confined variants of the LLM,
        respectively. The transform is evaluated to a higher resolution 
        to reduce convolution artifacts at the map's boundary.
        
        Parameters
        ----------
        pdb_path : str
            path to coordinates file of asymmetric unit
        expand_p1 : bool
            if True, pdb corresponds to asymmetric unit; expand to unit cell
        border : int
            number of border (integral) Miller indices along each direction 
        res_limit : float
            high-resolution limit in Angstrom
        batch_size : int
            number of q-vectors to evaluate per batch
        n_processes : int
            number of processes for structure factor calculation
        asu_confined : bool
            False for crystal transform, True for molecular trasnsform
        """
        # generate atomic model
        model = AtomicModel(pdb_path, expand_p1=expand_p1)
        model.flatten_model()
        
        # get grid for padded map
        hsampling_padded = (self.hsampling[0]-border, self.hsampling[1]+border, self.hsampling[2])
        ksampling_padded = (self.ksampling[0]-border, self.ksampling[1]+border, self.ksampling[2])
        lsampling_padded = (self.lsampling[0]-border, self.lsampling[1]+border, self.lsampling[2])
        hkl_grid, self.map_shape = generate_grid(model.A_inv, 
                                                 hsampling_padded,
                                                 ksampling_padded,
                                                 lsampling_padded,
                                                 return_hkl=True)
        self.res_mask, res_map = get_resolution_mask(model.cell, hkl_grid, res_limit)
        
        # compute crystal or molecular transform
        if not asu_confined:
            self.q_grid, self.transform = compute_crystal_transform(pdb_path,
                                                                    hsampling_padded,
                                                                    ksampling_padded,
                                                                    lsampling_padded,
                                                                    expand_p1=expand_p1,
                                                                    res_limit=res_limit,
                                                                    batch_size=batch_size,
                                                                    n_processes=n_processes)
        else:
            self.q_grid, self.transform = compute_molecular_transform(pdb_path,
                                                                      hsampling_padded,
                                                                      ksampling_padded,
                                                                      lsampling_padded,
                                                                      expand_p1=expand_p1,
                                                                      expand_friedel=False,
                                                                      res_limit=res_limit,
                                                                      batch_size=batch_size, 
                                                                      n_processes=n_processes)
        self.q_mags = np.linalg.norm(self.q_grid, axis=1)
        
        # generate mask for padded region
        self.mask = np.zeros(self.map_shape)
        self.mask[border*self.hsampling[2]:-border*self.hsampling[2],
                  border*self.ksampling[2]:-border*self.ksampling[2],
                  border*self.lsampling[2]:-border*self.lsampling[2]] = 1
        self.map_shape_nopad = tuple(np.array(self.map_shape) - np.array([2*border*self.hsampling[2], 
                                                                          2*border*self.ksampling[2], 
                                                                          2*border*self.lsampling[2]]))

        # empty lists to populate with all sigmas/gammas that have been scanned
        self.scan_sigmas = []
        self.scan_gammas = []
        self.scan_ccs = []
        self.opt_map = None
        
    @debug
    def fft_convolve(self, transform, kernel):
        """ 
        Convolve the transform and kernel by multiplying their 
        Fourier transforms. This approach requires less memory 
        than scipy.signal.fftconvolve.

        Parameters
        ----------
        transform : numpy.ndarray, 3d
            crystal or molecular transform map
        kernel : numpy.ndarray, 3d
            disorder kernel, same shape as transform

        Returns
        -------
        conv : numpy.ndarray 
            convolved map
        """
        ft_transform = np.fft.fftn(transform)
        ft_kernel = np.fft.fftn(kernel/kernel.sum()) 
        ft_conv = ft_transform * ft_kernel
        return np.fft.ifftshift(np.fft.ifftn(ft_conv).real)

    def plot_scan(self, output=None):
        """
        Plot a heatmap of the overall correlation coefficient
        as a function of sigma and gamma.
        Parameters
        ----------
        output : str
            if provided, save plot to given path
        """
        import matplotlib.pyplot as plt

        sigmas = np.array(self.scan_sigmas)
        gammas = np.array(self.scan_gammas)
        ccs = np.array(self.scan_ccs)

        xi = np.linspace(sigmas.min(), sigmas.max(), 25)
        yi = np.linspace(gammas.min(), gammas.max(), 25)
        zi = scipy.interpolate.griddata((sigmas, gammas), ccs, (xi[None,:], yi[:,None]), method='cubic')

        plt.contourf(xi,yi,zi,25,linewidths=0.5)
        plt.xlabel("$\sigma$ ($\mathrm{\AA}$)", fontsize=14)
        plt.ylabel("$\gamma$ ($\mathrm{\AA}$)", fontsize=14)
        cb = plt.colorbar()
        cb.ax.set_ylabel("CC", fontsize=14)

        if output is not None:
            plt.savefig(output, dpi=300, bbox_inches='tight')
    
    @debug
    def apply_disorder(self, sigmas, gammas):
        """
        Compute the diffuse map(s) from the crystal transform as:
        I_diffuse = q2s2 * np.exp(-q2s2) * [I_transform * kernel(q)]
        where q2s2 = q^2 * s^2, and the kernel models covariances as 
        decaying exponentially with interatomic distance: 
        kernel(q) = 8 * pi * gamma^3 / (1 + q^2 * gamma^2)^2
        
        Parameters
        ----------
        sigmas : float or array of shape (n_sigma,) or (n_sigma, 3)
            (an)isotropic displacement parameter for asymmetric unit 
        gammas : float or array of shape (n_gamma,)
            kernel's correlation length
            
        Returns
        -------
        Id : numpy.ndarray, (n_sigma*n_gamma, q_grid.shape[0])
            diffuse intensity maps for the corresponding parameters
        """
        
        if type(gammas) == float or type(gammas) == int:
            gammas = np.array([gammas])   
        if type(sigmas) == float or type(sigmas) == int:
            sigmas = np.array([sigmas])

        # generate kernel and convolve with transform
        Id = np.zeros((len(gammas), self.q_grid.shape[0]))
        kernels = 8.0 * np.pi * (gammas[:,np.newaxis]**3) / np.square(1 + np.square(gammas[:,np.newaxis] * self.q_mags))
        for num in range(len(gammas)):
            if np.prod(self.map_shape) < 1e7:
                Id[num] = scipy.signal.fftconvolve(self.transform,
                                                   kernels[num].reshape(self.map_shape)/np.sum(kernels[num]),
                                                   mode='same').flatten()
            else:
                Id[num] = self.fft_convolve(self.transform, kernels[num].reshape(self.map_shape)).flatten()
        Id = np.tile(Id, (len(sigmas), 1))

        # scale with displacement parameters
        if len(sigmas.shape)==1:
            sigmas = np.repeat(sigmas, len(gammas))
            q2s2 = np.square(self.q_mags) * np.square(sigmas)[:,np.newaxis]
        else:
            sigmas = np.repeat(sigmas, len(gammas), axis=0)
            q2s2 = np.sum(self.q_grid.T * np.dot(np.square(sigmas)[:,np.newaxis] * np.eye(3), self.q_grid.T), axis=1)

        Id *= np.exp(-1*q2s2) * q2s2
        Id[:,~self.res_mask] = np.nan
        return Id

    @debug
    def optimize(self, target, sigmas_min, sigmas_max, gammas_min, gammas_max, ns_search=20, ng_search=10):
        """
        Scan to find the sigma that maximizes the overall Pearson
        correlation between the target and computed maps. 
        
        Parameters
        ----------
        target : numpy.ndarray, 3d
            target map, of shape self.map_shape
        sigmas_min : float or tuple of shape (3,)
            lower bound of (an)isotropic sigmas
        sigmas_max : float or tuple of shape (3,)
            upper bound of (an)isotropic sigmas, same type/dimension as sigmas_min
        gammas_min : float 
            lower bound of gamma
        gammas_max : float 
            upper bound of gamma
        ns_search : int
            sampling frequency between sigmas_min and sigmas_max
        ng_search : int
            sampling frequency between gammas_min and gammas_max
        
        Returns
        -------
        ccs : numpy.ndarray, shape (n_search,)
            Pearson correlation coefficients to target maps
        sigmas : numpy.ndarray, shape (n_search,) or (n_search, n_search, n_search)
            sigmas that were scanned over, ordered as ccs
        """
        assert target.shape == self.map_shape_nopad
        
        if (type(sigmas_min) == float) and (type(sigmas_max) == float):
            sigmas = np.linspace(sigmas_min, sigmas_max, ns_search)
        else:
            sa, sb, sc = [np.linspace(sigmas_min[i], sigmas_max[i], ns_search) for i in range(3)]
            sigmas = np.array(list(itertools.product(sa, sb, sc)))
        gammas = np.linspace(gammas_min, gammas_max, ng_search)
        
        Id = self.apply_disorder(sigmas, gammas)
        Id = Id[:,self.mask.flatten()==1]
        ccs = compute_cc(Id, np.expand_dims(target.flatten(), axis=0))
        opt_index = np.argmax(ccs)
        if self.opt_map is None:
            self.opt_map = Id[opt_index].reshape(self.map_shape_nopad)
        if self.scan_ccs and np.max(ccs) > np.max(np.array(self.scan_ccs)):
            self.opt_map = Id[opt_index].reshape(self.map_shape_nopad)
        
        sigmas = np.repeat(sigmas, len(gammas), axis=0)
        gammas = np.tile(gammas, int(len(sigmas)/len(gammas)))
        self.opt_sigma = sigmas[opt_index]
        self.opt_gamma = gammas[opt_index]
        self.scan_sigmas.extend(list(sigmas))
        self.scan_gammas.extend(list(gammas))
        self.scan_ccs.extend(list(ccs))
        
        print(f"Optimal sigma: {self.opt_sigma}, optimal gamma: {self.opt_gamma}, with correlation coefficient {ccs[opt_index]:.4f}")
        return ccs, sigmas, gammas
    
class RigidBodyRotations:
    
    """
    Model of rigid body rotational disorder, in which all atoms in 
    each asymmetric unit rotate as a rigid unit around a randomly 
    oriented axis with a normally distributed rotation angle.
    """
    
    @debug
    def __init__(self, pdb_path, hsampling, ksampling, lsampling, expand_p1=True, res_limit=0, batch_size=10000, n_processes=8):
        self.hsampling = hsampling
        self.ksampling = ksampling
        self.lsampling = lsampling
        self._setup(pdb_path, expand_p1, res_limit)
        self.batch_size = batch_size
        self.n_processes = n_processes 
        
    @debug
    def _setup(self, pdb_path, expand_p1, res_limit=0):
        """
        Compute q-vectors to evaluate.
        
        Parameters
        ----------
        pdb_path : str
            path to coordinates file of asymmetric unit
        expand_p1 : bool
            if True, expand to p1 (i.e. if PDB corresponds to the asymmetric unit)
        res_limit : float
            high-resolution limit in Angstrom
        """
        self.model = AtomicModel(pdb_path, expand_p1=expand_p1, frame=-1)
        hkl_grid, self.map_shape = generate_grid(self.model.A_inv, 
                                                 self.hsampling, 
                                                 self.ksampling, 
                                                 self.lsampling,
                                                 return_hkl=True)
        self.q_grid = 2*np.pi*np.inner(self.model.A_inv.T, hkl_grid).T
        self.q_mags = np.linalg.norm(self.q_grid, axis=1)
        self.mask, res_map = get_resolution_mask(self.model.cell, hkl_grid, res_limit)
    
    @staticmethod
    @debug
    def generate_rotations_around_axis(sigma, num_rot, axis=np.array([0,0,1.0])):
        """
        Generate uniform random rotations about an axis.
        Parameters
        ----------
        sigma : float
            standard deviation of angular sampling around axis in degrees
        num_rot : int
            number of rotations to generate
        axis : numpy.ndarray, shape (3,)
            axis about which to generate rotations
        Returns
        -------
        rot_mat : numpy.ndarray, shape (num, 3, 3)
            rotation matrices
        """
        axis /= np.linalg.norm(axis)
        random_R = scipy.spatial.transform.Rotation.random(num_rot).as_matrix()
        random_ax = np.inner(random_R, np.array([0,0,1.0]))
        thetas = np.deg2rad(sigma) * np.random.randn(num_rot)
        rot_vec = thetas[:,np.newaxis] * random_ax
        rot_mat = scipy.spatial.transform.Rotation.from_rotvec(rot_vec).as_matrix()
        return rot_mat
    
    @debug
    def apply_disorder(self, sigmas, num_rot=100, ensemble_dir=None):
        """
        Compute the diffuse maps(s) resulting from rotational disorder for 
        the given sigmas by applying Guinier's equation to an ensemble of 
        rotated molecules, and then taking the incoherent sum of all of the
        asymmetric units.
        
        Parameters
        ----------
        sigmas : float or array of shape (n_sigma,) 
            standard deviation(s) of angular sampling in degrees
        num_rot : int
            number of rotations to generate per sigma, or
            if ensemble_dir is not None, the nth ensemble member to generate
        ensemble_dir : str
            save unmasked structure factor amplitudes to given path

        Returns
        -------
        Id : numpy.ndarray, (n_sigma, q_grid.shape[0])
            diffuse intensity maps for the corresponding parameters
        """
        if type(sigmas) == float or type(sigmas) == int:
            sigmas = np.array([sigmas])

        if ensemble_dir is not None:
            out_prefix = f"rot_{num_rot:05}"
            num_rot=1
            
        Id = np.zeros((len(sigmas), self.q_grid.shape[0]))
        for n_sigma,sigma in enumerate(sigmas):
            for asu in range(self.model.n_asu):
                # rotate atomic coordinates
                rot_mat = self.generate_rotations_around_axis(sigma, num_rot)
                com = np.mean(self.model.xyz[asu], axis=0)
                xyz_rot = np.matmul(self.model.xyz[asu] - com, rot_mat) 
                xyz_rot += com
                
                # apply Guinier's equation to rotated ensemble
                fc = np.zeros(self.q_grid.shape[0], dtype=complex)
                fc_square = np.zeros(self.q_grid.shape[0])
                U = self.model.adp[asu] / (8 * np.pi * np.pi)
                for rnum in range(num_rot):
                    A = structure_factors(self.q_grid[self.mask], 
                                          xyz_rot[rnum], 
                                          self.model.ff_a[asu], 
                                          self.model.ff_b[asu], 
                                          self.model.ff_c[asu], 
                                          U=U, 
                                          batch_size=self.batch_size,
                                          n_processes=self.n_processes)
                    if ensemble_dir is not None:
                        np.save(os.path.join(ensemble_dir, out_prefix + f"_asu{asu}.npy"), A)
                    fc[self.mask] += A
                    fc_square[self.mask] += np.square(np.abs(A)) 
                Id[n_sigma] += fc_square / num_rot - np.square(np.abs(fc / num_rot))

        Id[:,~self.mask] = np.nan
        return Id 
    
    @debug
    def optimize(self, target, sigma_min, sigma_max, n_search=20, num_rot=100):
        """
        Scan to find the sigma that maximizes the overall Pearson
        correlation between the target and computed maps. 
        
        Parameters
        ----------
        target : numpy.ndarray, 3d
            target map, of shape self.map_shape
        sigma_min : float 
            lower bound of sigma
        sigma_max : float 
            upper bound of sigma
        n_search : int
            sampling frequency between sigma_min and sigma_max
        num_rot : int
            number of rotations to generate per sigma
        
        Returns
        -------
        ccs : numpy.ndarray, shape (n_search,)
            Pearson correlation coefficients to target maps
        sigmas : numpy.ndarray, shape (n_search,) or (n_search, n_search, n_search)
            sigmas that were scanned over, ordered as ccs
        """
        assert target.shape == self.map_shape
        
        sigmas = np.linspace(sigma_min, sigma_max, n_search)        
        Id = self.apply_disorder(sigmas, num_rot)
        ccs = compute_cc(Id, np.expand_dims(target.flatten(), axis=0))
        opt_index = np.argmax(ccs)
        self.opt_sigma = sigmas[opt_index]
        self.opt_map = Id[opt_index].reshape(self.map_shape)

        print(f"Optimal sigma: {self.opt_sigma}, with correlation coefficient {ccs[opt_index]:.4f}")
        return ccs, sigmas
    
class Ensemble:
    
    """
    Model of ensemble disorder, in which the components of the
    asymmetric unit populate distinct biological states.
    """
    
    def __init__(self, pdb_path, hsampling, ksampling, lsampling, expand_p1=True, 
                 res_limit=0, batch_size=10000, n_processes=8, frame=-1):
        self.hsampling = hsampling
        self.ksampling = ksampling
        self.lsampling = lsampling
        self._setup(pdb_path, expand_p1, res_limit, frame)
        self.batch_size = batch_size
        self.n_processes = n_processes
        
    def _setup(self, pdb_path, expand_p1, res_limit, frame):
        """
        Load model and compute q-vectors to evaluate / mask.
        
        Parameters
        ----------
        pdb_path : str
            path to coordinates file of asymmetric unit
        expand_p1 : bool
            if True, expand to p1 (i.e. if PDB corresponds to the asymmetric unit)
        res_limit : float
            high-resolution limit in Angstrom
        frame : int
            load specified conformation or all states if -1
        """
        self.model = AtomicModel(pdb_path, expand_p1=expand_p1, frame=frame)
        hkl_grid, self.map_shape = generate_grid(self.model.A_inv, 
                                                 self.hsampling, 
                                                 self.ksampling, 
                                                 self.lsampling,
                                                 return_hkl=True)
        self.q_grid = 2*np.pi*np.inner(self.model.A_inv.T, hkl_grid).T
        self.q_mags = np.linalg.norm(self.q_grid, axis=1)
        self.mask, res_map = get_resolution_mask(self.model.cell, hkl_grid, res_limit)
        self.frame = frame
        
    def apply_disorder(self, weights=None, ensemble_dir=None):
        """
        Compute the diffuse maps(s) resulting from ensemble disorder using
        Guinier's equation, and then taking the incoherent sum of all the
        asymmetric units. If an ensemble_dir is provided, the unweighted 
        results for the indicated state will be saved to disk.
        
        Parameters
        ----------
        weights : shape (n_sets, n_conf) 
            set(s) of probabilities associated with each conformation
        ensemble_dir : str
            save path for structure factor amplitudes for given state

        Returns
        -------
        Id : numpy.ndarray, (n_sets, q_grid.shape[0])
            diffuse intensity map for the corresponding parameters
        """
        if weights is None:
            weights = 1.0 / self.model.n_conf * np.array([np.ones(self.model.n_conf)])
        if len(weights.shape) == 1:
            weights = np.array([weights])
        if weights.shape[1] != self.model.n_conf:
            raise ValueError("Second dimension of weights must match number of conformations.")
         
        n_maps = weights.shape[0]    
        Id = np.zeros((weights.shape[0], self.q_grid.shape[0]))
        
        for asu in range(self.model.n_asu):

            fc = np.zeros((weights.shape[0], self.q_grid.shape[0]), dtype=complex)
            fc_square = np.zeros((weights.shape[0], self.q_grid.shape[0]))

            for conf in range(self.model.n_conf):
                index = conf * self.model.n_asu + asu
                U = self.model.adp[index] / (8 * np.pi * np.pi)
                A = structure_factors(self.q_grid[self.mask], 
                                      self.model.xyz[index], 
                                      self.model.ff_a[index], 
                                      self.model.ff_b[index], 
                                      self.model.ff_c[index], 
                                      U=U, 
                                      batch_size=self.batch_size,
                                      n_processes=self.n_processes)

                if ensemble_dir is not None:
                    np.save(os.path.join(ensemble_dir, f"conf{self.frame:05}_asu{asu}.npy"), A)

                for nm in range(n_maps):
                    fc[nm][self.mask] += A * weights[nm][conf]
                    fc_square[nm][self.mask] += np.square(np.abs(A)) * weights[nm][conf]

            for nm in range(n_maps):
                Id[nm] += fc_square[nm] - np.square(np.abs(fc[nm]))
                
        Id[:,~self.mask] = np.nan
        return Id

class NonInteractingDeformableMolecules:

    """
    Lattice model with non-interacting deformable molecules.
    Each asymmetric unit is a Gaussian Network Model.
    """

    def __init__(self, pdb_path, hsampling, ksampling, lsampling,
                 expand_p1=True, res_limit=0, gnm_cutoff=4.,
                 batch_size=10000, n_processes=8):
        self.hsampling = hsampling
        self.ksampling = ksampling
        self.lsampling = lsampling
        self.batch_size = batch_size
        self.n_processes = n_processes
        self._setup(pdb_path, expand_p1, res_limit)
        self._setup_gnm(pdb_path, gnm_cutoff)
        self._setup_covmat()
        
    def _setup(self, pdb_path, expand_p1, res_limit, q2_rounding=3):
        """
        Compute q-vectors to evaluate.

        Parameters
        ----------
        pdb_path : str
            path to coordinates file of asymmetric unit
        res_limit : float
            high-resolution limit in Angstrom
        q2_rounding : int
            number of decimals to round q squared to, default: 3
        """
        self.model = AtomicModel(pdb_path, expand_p1=expand_p1)

        hkl_grid, self.map_shape = generate_grid(self.model.A_inv,
                                                 self.hsampling,
                                                 self.ksampling,
                                                 self.lsampling,
                                                 return_hkl=True)
        self.res_mask, res_map = get_resolution_mask(self.model.cell,
                                                     hkl_grid,
                                                     res_limit)
        self.q_grid = 2 * np.pi * np.inner(self.model.A_inv.T, hkl_grid).T

        q2 = np.linalg.norm(self.q_grid, axis=1) ** 2
        self.q2_unique, \
        self.q2_unique_inverse = np.unique(np.round(q2, q2_rounding),
                                           return_inverse=True)

    def _setup_gnm(self, pdb_path, gnm_cutoff):
        """
        Build Gaussian Network Model.

        Parameters
        ----------
        pdb_path : str
            path to coordinates file of asymmetric unit
        gnm_cutoff : float
            distance cutoff used to define atom pairs
        """
        self.gnm = GaussianNetworkModel(pdb_path,
                                        enm_cutoff=gnm_cutoff,
                                        gamma_intra=1.,
                                        gamma_inter=0.)

    def _setup_covmat(self):
        """
        Compute the covariance matrix and perform low rank truncation.
        """
        self.compute_covariance_matrix()
        self.u, self.s = self._low_rank_truncation(self.covar)
        
    @debug
    def compute_covariance_matrix(self):
        """
        Compute covariance matrix for one asymmetric unit.
        The covariance matrix results from modelling pairwise
        interactions with a Gaussian Network Model where atom
        pairs belonging to different asymmetric units are not
        interacting. It is scaled to match the ADPs in the input PDB file.
        """
        Kinv = self.gnm.compute_Kinv(self.gnm.compute_hessian())
        Kinv = np.real(Kinv[0, :, 0, :])  # only need that
        ADP_scale = np.mean(self.model.adp[0]) / \
                    (8 * np.pi * np.pi * np.mean(np.diag(Kinv)))
        self.covar = Kinv * ADP_scale
        self.ADP = np.diag(self.covar)

    def _low_rank_truncation(self, sym_matrix, rank=None, rec_error_threshold=0.01):
        """
        Perform low rank approximation of a symmetric matrix.
        Either the desired rank is provided, or it is found
        when the reconstructed matrix Froebenius norm is the
        same as the original matrix, up to a provided
        reconstruction error.

        Parameters
        ----------
        sym_matrix : numpy.ndarray, shape (n, n)
            must be a 2D symmetric array
        rank : int or None
            if not None, the desired rank
        rec_error_threshold : float
            if rank is None, the maximal reconstruction error
        Returns
        -------
        u : numpy.ndarray, shape (n, rank)
            First rank-th components of sym_matrix
        s : numpy.ndarray, shape (rank,)
            First rank-th singular values of sym_matrix
        """
        u, s, vh = np.linalg.svd(sym_matrix)
        if rank is None:
            sym_matrix_norm = np.linalg.norm(sym_matrix)
            for rank in range(s.shape[0]):
                rec_matrix = (u[:, :rank] * s[:rank]) @ vh[:rank, :]
                rec_matrix_norm = np.linalg.norm(rec_matrix)
                rec_error = 1. - rec_matrix_norm / sym_matrix_norm
                if rec_error < rec_error_threshold:
                    break
        return u[:, :rank], s[:rank]

    def compute_scl_intensity(self, rank=-1, outdir=None):
        """
        Compute diffuse intensity of non-interacting deformable
        molecules, in the soft-coupling limit.
        Namely, given a Gaussian Network Model (GNM) for the
        asymmetric unit (ASU), the covariance C is factorized:
        C = B * D @ B.T and eventually low-rank truncated.
        The diffuse intensity is then the incoherent sum over
        its components, weighted by q**2, where we introduce
        the component factors G = F * B:
        I(q) = q**2 \sum_r D_r \sum_asu |G_asu,r|**2

        Parameters
        ----------
        rank : int
            if -1, sum across ranks; else, save rank's results
        outdir : str
            path for storing rank results

        Returns
        -------
        Id : numpy.ndarray, shape (q_grid.shape[0],)
            diffuse intensity map
        """
        Id = np.zeros((self.q_grid.shape[0]))
        for i_asu in range(self.model.n_asu):
            if rank == -1:
                Id[self.res_mask] += np.dot(np.square(np.abs(structure_factors(self.q_grid[self.res_mask],
                                                                               self.model.xyz[i_asu],
                                                                               self.model.ff_a[i_asu],
                                                                               self.model.ff_b[i_asu],
                                                                               self.model.ff_c[i_asu],
                                                                               U=self.ADP,
                                                                               batch_size=self.batch_size,
                                                                               n_processes=self.n_processes,
                                                                               project_on_components=self.u,
                                                                               sum_over_atoms=False))), self.s)
            else:
                Id[self.res_mask] += np.square(np.abs(structure_factors(self.q_grid[self.res_mask],
                                                                        self.model.xyz[i_asu],
                                                                        self.model.ff_a[i_asu],
                                                                        self.model.ff_b[i_asu],
                                                                        self.model.ff_c[i_asu],
                                                                        U=self.ADP,
                                                                        batch_size=self.batch_size,
                                                                        n_processes=self.n_processes,
                                                                        project_on_components=self.u[:,rank],
                                                                        sum_over_atoms=False))) * self.s[rank]
        Id = np.multiply(self.q2_unique[self.q2_unique_inverse], Id)
        if outdir is not None:
            np.save(os.path.join(outdir, f"rank_{rank:05}.npy"), Id)
        return Id

    def compute_intensity_naive(self):
        """
        Compute diffuse intensity of non-interacting deformable
        molecules in a non-efficient / naive way.
        Namely, given a Gaussian Network Model (GNM) for the
        asymmetric unit (ASU), we can compute its covariance C
        and for each q and each atom pair T_ij(q) = exp(q**2 C_ij).
        Noting F_i(q) the structure factor for atom i, the
        contribution of one ASU to the intensity reads:
        I(q) = \sum_ij F_i(q) (T_ij(q) - 1.) F_j(q)
        The diffuse intensity is an incoherent sum over ASUs.
        """
        Id = np.zeros((self.q_grid.shape[0]), dtype='complex')

        self.compute_covariance_matrix()
        Tmat = np.exp(self.covar).astype(complex)

        F = np.zeros((self.model.n_asu,
                      self.q_grid.shape[0],
                      self.gnm.n_atoms_per_asu),
                     dtype='complex')
        for i_asu in range(self.model.n_asu):
            F[i_asu] = structure_factors(self.q_grid,
                                         self.model.xyz[i_asu],
                                         self.model.ff_a[i_asu],
                                         self.model.ff_b[i_asu],
                                         self.model.ff_c[i_asu],
                                         U=self.ADP,
                                         batch_size=self.batch_size,
                                         n_processes=self.n_processes,
                                         sum_over_atoms=False)

        for iq in tqdm(range(self.q_grid.shape[0])):
            Jq = np.power(Tmat,
                          self.q2_unique[self.q2_unique_inverse][iq]) - 1.
            for i_asu in range(self.model.n_asu):
                Id[iq] += np.matmul(F[i_asu,iq],
                                   np.matmul(Jq,
                                             np.conj(F[i_asu,iq])))
        return np.real(Id)

    def apply_disorder(self, scl=True):
        """
        Compute diffuse intensity of non-interacting deformable
        molecules.
        Namely, given a Gaussian Network Model (GNM) for the
        asymmetric unit (ASU), the covariance C between atomic
        displacements is computed. Then the coupling between
        structure factors F can be defined as J = exp(q2*C) - 1.
        The diffuse intensity is then the incoherent sum over
        asymmetric units: Id = \sum_asu F_asu.T J F_asu.
        Because computing this in the general case would not
        scale well, several approximations/simplifications are
        offered (at the moment, only the soft-coupling limit).

        Parameters
        ----------
        scl : bool
            whether we are in the soft-coupling limit or not.
        """
        if scl:
            Id = self.compute_scl_intensity()
        else:
            Id = self.compute_intensity_naive()
        Id[~self.res_mask] = np.nan
        return Id

class OnePhonon:

    """
    Lattice of interacting rigid bodies in the one-phonon
    approximation (a.k.a small-coupling regime).
    """

    @debug
    def __init__(self, pdb_path, hsampling, ksampling, lsampling,
                 expand_p1=True, group_by='asu',
                 res_limit=0., model='gnm',
                 gnm_cutoff=4., gamma_intra=1., gamma_inter=1.,
                 batch_size=10000, n_processes=8):
        self.hsampling = hsampling
        self.ksampling = ksampling
        self.lsampling = lsampling
        self.batch_size = batch_size
        self.n_processes = n_processes
        self._setup(pdb_path, expand_p1, res_limit, group_by)
        self._setup_phonons(pdb_path, model,
                            gnm_cutoff, gamma_intra, gamma_inter)

    @debug
    def _setup(self, pdb_path, expand_p1, res_limit, group_by):
        """
        Compute q-vectors to evaluate and build the unit cell
        and its nearest neighbors while storing useful dimensions.

        Parameters
        ----------
        pdb_path : str
            path to coordinates file of asymmetric unit
        expand_p1: bool
            expand_p1 : bool
            if True, expand to p1 (i.e. if PDB corresponds to the asymmetric unit)
        res_limit : float
            high-resolution limit in Angstrom
        group_by : str
            level of rigid-body assembly.
            For now, only None and 'asu' have been implemented.
        """
        self.model = AtomicModel(pdb_path, expand_p1)

        self.hkl_grid, self.map_shape = generate_grid(self.model.A_inv,
                                                      self.hsampling,
                                                      self.ksampling,
                                                      self.lsampling,
                                                      return_hkl=True)
        self.res_mask, res_map = get_resolution_mask(self.model.cell,
                                                     self.hkl_grid,
                                                     res_limit)
        self.q_grid = 2 * np.pi * np.inner(self.model.A_inv.T, self.hkl_grid).T

        self.crystal = Crystal(self.model)
        self.crystal.supercell_extent(nx=1, ny=1, nz=1)
        self.id_cell_ref = self.crystal.hkl_to_id([0,0,0])
        self.n_cell = self.crystal.n_cell
        self.n_asu = self.crystal.model.n_asu
        self.n_atoms_per_asu = self.crystal.get_asu_xyz().shape[0]
        self.n_dof_per_asu_actual = self.n_atoms_per_asu * 3

        self.group_by = group_by
        if self.group_by is None:
            self.n_dof_per_asu = np.copy(self.n_dof_per_asu_actual)
        else:
            self.n_dof_per_asu = 6
        self.n_dof_per_cell = self.n_asu * self.n_dof_per_asu

    @debug
    def _setup_phonons(self, pdb_path, model,
                       gnm_cutoff, gamma_intra, gamma_inter):
        """
        Compute phonons either from a Gaussian Network Model of the
        molecules or by direct definition of the dynamical matrix.

        Parameters
        ----------
        pdb_path : str
            path to coordinates file of asymmetric unit
        model : str
            chosen phonon model: 'gnm' or 'rb'
        gnm_cutoff : float
            distance cutoff used to define the GNM
            see eryx.pdb.GaussianNetworkModel.compute_hessian()
        gamma_intra: float
            spring constant for atom pairs belonging to the same molecule
            see eryx.pdb.GaussianNetworkModel.build_gamma()
        gamma_inter: float
            spring constant for atom pairs belonging to distinct molecules
            see eryx.pdb.GaussianNetworkModel.build_gamma()
        """
        self.kvec = np.zeros((self.hsampling[2],
                              self.ksampling[2],
                              self.lsampling[2],
                              3))
        self.kvec_norm = np.zeros((self.hsampling[2],
                                   self.ksampling[2],
                                   self.lsampling[2],
                                   1))
        self.V = np.zeros((self.hsampling[2],
                           self.ksampling[2],
                           self.lsampling[2],
                           self.n_asu * self.n_dof_per_asu,
                           self.n_asu * self.n_dof_per_asu),
                          dtype='complex')
        self.Winv = np.zeros((self.hsampling[2],
                              self.ksampling[2],
                              self.lsampling[2],
                              self.n_asu * self.n_dof_per_asu),
                             dtype='complex')

        self._build_A()
        self._build_M()
        self._build_kvec_Brillouin()
        if model == 'gnm':
            self._setup_gnm(pdb_path, gnm_cutoff, gamma_intra, gamma_inter)
            self.compute_gnm_phonons()
            self.compute_covariance_matrix()
        else:
            self.compute_rb_phonons()

    def _setup_gnm(self, pdb_path, gnm_cutoff, gamma_intra, gamma_inter):
        """
        Instantiate the Gaussian Network Model

        Parameters
        ----------
        pdb_path : str
            path to coordinates file of asymmetric unit
        gnm_cutoff : float
            distance cutoff used to define the GNM
            see eryx.pdb.GaussianNetworkModel.compute_hessian()
        gamma_intra: float
            spring constant for atom pairs belonging to the same molecule
            see eryx.pdb.GaussianNetworkModel.build_gamma()
        gamma_inter: float
            spring constant for atom pairs belonging to distinct molecules
            see eryx.pdb.GaussianNetworkModel.build_gamma()
        """
        self.gnm = GaussianNetworkModel(pdb_path,
                                        enm_cutoff=gnm_cutoff,
                                        gamma_intra=gamma_intra,
                                        gamma_inter=gamma_inter)

    @debug
    def _build_A(self):
        """
        Build the matrix A that projects small rigid-body displacements
        to the individual atoms in the rigid body.
        More specifically, consider the set of cartesian coordinates {r_i}_m
        of all atoms in the m-th rigid body and o_m their center of mass.
        Also consider their instantaneous displacement {u_i}_m translating
        from instantaneous rigid-body displacements w_m = [t_m, l_m] where
        t_m and l_m are respectively the 3-dimensional translation and libration
        vector of group m.
        For each atom i in group m, the conversion reads:
        u_i = A(r_i - o_m).w_m
        where A is the following 3x6 matrix:
        A(x,y,z) = [[ 1 0 0  0  z -y ]
                    [ 0 1 0 -z  0  x ]
                    [ 0 0 1  y -x  0 ]]
        """
        if self.group_by == 'asu':
            self.Amat = np.zeros((self.n_asu, self.n_atoms_per_asu, 3, 6))
            Atmp = np.zeros((3, 3))
            Adiag = np.copy(Atmp)
            np.fill_diagonal(Adiag, 1.)
            for i_asu in range(self.n_asu):
                xyz = np.copy(self.crystal.get_asu_xyz(i_asu))
                xyz -= np.mean(xyz, axis=0)
                for i_atom in range(self.n_atoms_per_asu):
                    Atmp[0, 1] = xyz[i_atom, 2]
                    Atmp[0, 2] = -xyz[i_atom, 1]
                    Atmp[1, 2] = xyz[i_atom, 0]
                    Atmp -= Atmp.T
                    self.Amat[i_asu, i_atom] = np.hstack([Adiag, Atmp])
            self.Amat = self.Amat.reshape((self.n_asu,
                                           self.n_dof_per_asu_actual,
                                           self.n_dof_per_asu))
        else:
            self.Amat = None

    @debug
    def _build_M(self):
        """
        Build the mass matrix M.
        If all atoms are considered, M = M_0 is diagonal (see _build_M_allatoms())
        and Linv = 1./sqrt(M_0) is diagonal also.
        If atoms are grouped as rigid bodies, the all-atoms M matrix is
        projected using the A matrix: M = A.T M_0 A and Linv is obtained
        via Cholesky decomposition: M = LL.T
        """
        M_allatoms = self._build_M_allatoms()
        if self.group_by is None:
            M_allatoms = M_allatoms.reshape((self.n_asu * self.n_dof_per_asu_actual,
                                             self.n_asu * self.n_dof_per_asu_actual))
            self.Linv = 1. / np.sqrt(M_allatoms)
        else:
            Mmat = self._project_M(M_allatoms)
            Mmat = Mmat.reshape((self.n_asu * self.n_dof_per_asu,
                                 self.n_asu * self.n_dof_per_asu))
            self.Linv = np.linalg.inv(np.linalg.cholesky(Mmat))

    @debug
    def _project_M(self, M_allatoms):
        """
        Project all-atom mass matrix M_0 using the A matrix: M = A.T M_0 A

        Parameters
        ----------
        M_allatoms : numpy.ndarray, shape (n_asu, n_atoms*3, n_asu, n_atoms*3)

        Returns
        -------
        Mmat: numpy.ndarray, shape (n_asu, n_dof_per_asu, n_asu, n_dof_per_asu)
        """
        Mmat = np.zeros((self.n_asu, self.n_dof_per_asu,
                         self.n_asu, self.n_dof_per_asu))
        for i_asu in range(self.n_asu):
            for j_asu in range(self.n_asu):
                Mmat[i_asu, :, j_asu, :] = \
                    np.matmul(self.Amat[i_asu].T,
                              np.matmul(M_allatoms[i_asu, :, j_asu, :],
                                        self.Amat[j_asu]))
        return Mmat

    @debug
    def _build_M_allatoms(self):
        """
        Build all-atom mass matrix M_0

        Returns
        -------
        M_allatoms : numpy.ndarray, shape (n_asu, n_atoms*3, n_asu, n_atoms*3)
        """
        mass_array = np.array([element.weight for structure in self.crystal.model.elements for element in structure])
        mass_list = [np.kron(mass_array, np.eye(3))[:, 3 * i:3 * (i + 1)] for i in
                     range(self.n_asu * self.n_atoms_per_asu)]
        return block_diag(*mass_list).reshape((self.n_asu, self.n_dof_per_asu_actual,
                                               self.n_asu, self.n_dof_per_asu_actual))

    @debug
    def _center_kvec(self, x, L):
        """
        For x and L integers such that 0 < x < L, return -L/2 < x < L/2
        by applying periodic boundary condition in L/2
        Parameters
        ----------
        x : int
            the index to center
        L : int
            length of the periodic box
        """
        return int(((x - L / 2) % L) - L / 2) / L

    @debug
    def _build_kvec_Brillouin(self):
        """
        Compute all k-vectors and their norm in the first Brillouin zone.
        This is achieved by regularly sampling [-0.5,0.5[ for h, k and l.
        """
        for dh in range(self.hsampling[2]):
            k_dh = self._center_kvec(dh, self.hsampling[2])
            for dk in range(self.ksampling[2]):
                k_dk = self._center_kvec(dk, self.ksampling[2])
                for dl in range(self.lsampling[2]):
                    k_dl = self._center_kvec(dl, self.lsampling[2])
                    self.kvec[dh, dk, dl] = 2 * np.pi * np.inner(self.model.A_inv.T,
                                                                 (k_dh, k_dk, k_dl)).T
                    self.kvec_norm[dh, dk, dl] = np.linalg.norm(self.kvec[dh, dk, dl])

    @debug
    def _at_kvec_from_miller_points(self, hkl_kvec):
        """
        Return the indices of all q-vector that are k-vector away from any
        Miller index in the map.

        Parameters
        ----------
        hkl_kvec : tuple of ints
            fractional Miller index of the desired k-vector
        """
        hsteps = int(self.hsampling[2] * (self.hsampling[1] - self.hsampling[0]) + 1)
        ksteps = int(self.ksampling[2] * (self.ksampling[1] - self.ksampling[0]) + 1)
        lsteps = int(self.lsampling[2] * (self.lsampling[1] - self.lsampling[0]) + 1)

        index_grid = np.mgrid[
                     hkl_kvec[0]:hsteps:self.hsampling[2],
                     hkl_kvec[1]:ksteps:self.ksampling[2],
                     hkl_kvec[2]:lsteps:self.lsampling[2]]

        return np.ravel_multi_index((index_grid[0].flatten(),
                                     index_grid[1].flatten(),
                                     index_grid[2].flatten()),
                                    self.map_shape)

    def _kvec_map(self):
        """
        Build a map where the intensity at each fractional Miller index
        is set as the norm of the corresponding k-vector.
        For example, at each integral Miller index, k-vector is zero and
        it increases and then decreases as we sample between them.

        Returns
        -------
        map : numpy.ndarray, shape (npoints, 1)
        """
        map = np.zeros((self.q_grid.shape[0]))
        for dh in tqdm(range(self.hsampling[2])):
            for dk in range(self.ksampling[2]):
                for dl in range(self.lsampling[2]):
                    q_indices = self._at_kvec_from_miller_points((dh, dk, dl))
                    map[q_indices] = np.linalg.norm(self.kvec[dh, dk, dl])
        return map

    def _q_map(self):
        """
        Build a map where the intensity at each fractional Miller index
        is set as the norm of the corresponding q-vector.

        Returns
        -------
        map : numpy.ndarray, shape (npoints, 1_
        """
        map = np.zeros((self.q_grid.shape[0]))
        for dh in tqdm(range(self.hsampling[2])):
            for dk in range(self.ksampling[2]):
                for dl in range(self.lsampling[2]):
                    q_indices = self._at_kvec_from_miller_points((dh, dk, dl))
                    map[q_indices] = np.linalg.norm(self.q_grid[q_indices], axis=1)
        return map

    @debug
    def compute_hessian(self):
        """
        Build the projected Hessian matrix for the supercell.

        Returns
        -------
        hessian : numpy.ndarray,
                  shape (n_asu, n_dof_per_asu, n_cell, n_asu, n_dof_per_asu),
                  dtype 'complex'
            Hessian matrix for the assembly of rigid bodies in the supercell.
        """
        hessian = np.zeros((self.n_asu, self.n_dof_per_asu,
                            self.n_cell, self.n_asu, self.n_dof_per_asu),
                           dtype='complex')

        hessian_allatoms = self.gnm.compute_hessian()

        for i_cell in range(self.n_cell):
            for i_asu in range(self.n_asu):
                for j_asu in range(self.n_asu):
                    hessian[i_asu, :, i_cell, j_asu, :] = \
                        np.matmul(self.Amat[i_asu].T,
                                  np.matmul(np.kron(hessian_allatoms[i_asu, :, i_cell, j_asu, :],
                                                    np.eye(3)),
                                            self.Amat[j_asu]))

        return hessian

    def compute_covariance_matrix(self):
        """
        Compute covariance matrix for all asymmetric units.
        The covariance matrix results from modelling pairwise
        interactions with a Gaussian Network Model where atom
        pairs belonging to different asymmetric units are not
        interacting. It is scaled to match the ADPs in the input PDB file.
        """
        self.covar = np.zeros((self.n_asu*self.n_dof_per_asu,
                               self.n_cell, self.n_asu*self.n_dof_per_asu),
                              dtype='complex')

        hessian = self.compute_hessian()
        for dh in range(self.hsampling[2]):
            for dk in range(self.ksampling[2]):
                for dl in range(self.lsampling[2]):
                    kvec = self.kvec[dh,dk,dl]
                    Kinv = self.gnm.compute_Kinv(hessian, kvec=kvec, reshape=False)
                    for j_cell in range(self.n_cell):
                        r_cell = self.crystal.get_unitcell_origin(self.crystal.id_to_hkl(j_cell))
                        phase = np.dot(kvec, r_cell)
                        eikr = np.cos(phase) + 1j * np.sin(phase)
                        self.covar[:,j_cell,:] += Kinv * eikr
        #ADP_scale = np.mean(self.model.adp[0]) / \
        #            (8 * np.pi * np.pi * np.mean(np.diag(self.covar[:,self.crystal.hkl_to_id([0,0,0]),:])) / 3.)
        #self.covar *= ADP_scale
        self.ADP = np.real(np.diag(self.covar[:,self.crystal.hkl_to_id([0,0,0]),:]))
        Amat = np.transpose(self.Amat, (1,0,2)).reshape(self.n_dof_per_asu_actual, self.n_asu*self.n_dof_per_asu)
        self.ADP = Amat @ self.ADP
        self.ADP = np.sum(self.ADP.reshape(int(self.ADP.shape[0]/3),3),axis=1)
        ADP_scale = np.mean(self.model.adp) / (8*np.pi*np.pi*np.mean(self.ADP)/3)
        self.ADP *= ADP_scale
        self.covar *= ADP_scale
        self.covar = np.real(self.covar.reshape((self.n_asu, self.n_dof_per_asu,
                                                 self.n_cell, self.n_asu, self.n_dof_per_asu)))

    @debug
    def compute_gnm_phonons(self):
        """
        Compute the dynamical matrix for each k-vector in the first
        Brillouin zone, from the supercell's GNM.
        The squared inverse of their eigenvalues is
        stored for intensity calculation and their eigenvectors are
        mass-weighted to be used in the definition of the phonon
        structure factors.
        """
        hessian = self.compute_hessian()
        for dh in range(self.hsampling[2]):
            for dk in range(self.ksampling[2]):
                for dl in range(self.lsampling[2]):
                    Kmat = self.gnm.compute_K(hessian, kvec=self.kvec[dh, dk, dl])
                    Kmat = Kmat.reshape((self.n_asu * self.n_dof_per_asu,
                                         self.n_asu * self.n_dof_per_asu))
                    Dmat = np.matmul(self.Linv,
                                     np.matmul(Kmat, self.Linv.T))
                    v, w, _ = np.linalg.svd(Dmat)
                    w = np.sqrt(w)
                    w = np.where(w < 1e-6, np.nan, w)
                    w = w[::-1]
                    v = v[:,::-1]
                    self.Winv[dh, dk, dl] = 1. / w ** 2
                    self.V[dh, dk, dl] = np.matmul(self.Linv.T, v)

    def compute_rb_phonons(self):
        """
        Compute the dynamical matrix for each k-vector in the first
        Brillouin zone as a decaying Gaussian of k.
        (in development, not fully tested or understood).
        """
        Kmat = np.zeros((self.n_asu * self.n_dof_per_asu,
                         self.n_asu * self.n_dof_per_asu))
        for dh in range(self.hsampling[2]):
            for dk in range(self.ksampling[2]):
                for dl in range(self.lsampling[2]):
                    np.fill_diagonal(Kmat,
                                     np.exp(-0.5 * (
                                             np.linalg.norm(self.kvec[dh, dk, dl] /
                                                            np.linalg.norm(self.kvec[3, 0, 0])) ** 2))
                                     )
                    u, s, _ = np.linalg.svd(Kmat)
                    self.Winv[dh, dk, dl] = s
                    self.V[dh, dk, dl] = u

    @debug
    def apply_disorder(self, rank=-1, outdir=None, use_data_adp=False):
        """
        Compute the diffuse intensity in the one-phonon scattering
        disorder model originating from a Gaussian Network Model
        representation of the asymmetric units, optionally reduced
        to a set of interacting rigid bodies.
        """
        if use_data_adp:
            ADP = self.model.adp[0] / (8 * np.pi * np.pi)
        else:
            ADP = self.ADP
        Id = np.zeros((self.q_grid.shape[0]), dtype='complex')
        for dh in tqdm(range(self.hsampling[2])):
            for dk in range(self.ksampling[2]):
                for dl in range(self.lsampling[2]):

                    q_indices = self._at_kvec_from_miller_points((dh, dk, dl))
                    q_indices = q_indices[self.res_mask[q_indices]]

                    F = np.zeros((q_indices.shape[0],
                                  self.n_asu,
                                  self.n_dof_per_asu),
                                 dtype='complex')
                    for i_asu in range(self.n_asu):
                        F[:, i_asu, :] = structure_factors(
                            self.q_grid[q_indices],
                            self.model.xyz[i_asu],
                            self.model.ff_a[i_asu],
                            self.model.ff_b[i_asu],
                            self.model.ff_c[i_asu],
                            U=ADP,
                            batch_size=self.batch_size,
                            n_processes=self.n_processes,
                            compute_qF=True,
                            project_on_components=self.Amat[i_asu],
                            sum_over_atoms=False)
                    F = F.reshape((q_indices.shape[0],
                                   self.n_asu * self.n_dof_per_asu))

                    if rank == -1:
                        Id[q_indices] += np.dot(
                            np.square(np.abs(np.dot(F, self.V[dh, dk, dl]))),
                            self.Winv[dh, dk, dl])
                    else:
                        Id[q_indices] += np.square(
                            np.abs(np.dot(F, self.V[dh,dk,dl,:,rank]))) * \
                                         self.Winv[dh,dk,dl,rank]
        Id[~self.res_mask] = np.nan
        Id = np.real(Id)
        if outdir is not None:
            np.save(os.path.join(outdir, f"rank_{rank:05}.npy"), Id)
        return Id

class OnePhononBrillouin:

    def __init__(self, pdb_path, h, k, l, N,
                 group_by='asu', model='gnm',
                 gnm_cutoff=4., gamma_intra=1., gamma_inter=1.,
                 batch_size=10000, n_processes=8):
        self.phonon = OnePhonon(pdb_path,(h-1,h+1,N),(k-1,k+1,N), (l-1,l+1,N),
                                group_by=group_by, model=model,
                                gnm_cutoff=gnm_cutoff, gamma_intra=gamma_intra, gamma_inter=gamma_inter,
                                batch_size=batch_size, n_processes=n_processes)
        self.Id = self.phonon.apply_disorder().reshape(self.phonon.map_shape)[N//2+1:-N//2,N//2+1:-N//2,N//2+1:-N//2]

</file>
<file path="./eryx/reference.py" project="ptycho">
import numpy as np
from eryx.autotest.debug import debug

@debug
def structure_factors(q_grid, xyz, elements, U=None):
    """
    Compute the structure factors for an atomic model at 
    the given q-vectors. 
​
    Parameters
    ----------
    q_grid : numpy.ndarray, shape (n_points, 3)
        q-vectors in Angstrom
    xyz : numpy.ndarray, shape (n_atoms, 3)
        atomic xyz positions in Angstroms
    elements : list of gemmi.Element objects
        element objects, ordered as xyz
    U : numpy.ndarray, shape (n_atoms) or (n_atoms, 3, 3)
        (an)isotropic displacement parameters
        
    Returns
    -------
    A : numpy.ndarray, shape (n_points)
        complex structure factor amplitudes at q-vectors
    """
    
    A = np.zeros(q_grid.shape[0], dtype=np.complex128)
    stols2 = np.square(np.linalg.norm(q_grid, axis=1) / (4*np.pi)) 
    
    if U is None:
        U = np.zeros(xyz.shape[0])
    
    for i,q_vector in enumerate(q_grid):    
        
        for j in range(xyz.shape[0]):
            q_mag = np.linalg.norm(q_vector)
            fj = elements[j].it92.calculate_sf(stols2[i])
            rj = xyz[j,:]

            if len(U.shape) == 1:
                qUq = np.square(q_mag)*U[j]
            else:
                qUq = np.dot(np.dot(q_vector, U[j]), q_vector)

            A[i] +=      fj * np.cos( np.dot(q_vector, rj) ) * np.exp(- 0.5 * qUq)
            A[i] += 1j * fj * np.sin( np.dot(q_vector, rj) ) * np.exp(- 0.5 * qUq)

    return A

@debug
def diffuse_covmat(q_grid, xyz, elements, V):
    """
    Compute a diffuse scattering map for disorder in the harmonic 
    approximation, i.e. from a interatomic covariance matrix.
    
    Parameters
    ----------
    q_grid : numpy.ndarray, shape (n_points, 3)
        q-vectors in Angstrom
    xyz : numpy.ndarray, shape (n_atoms, 3)
        atomic xyz positions in Angstroms
    elements : list of gemmi.Element objects
        element objects, ordered as xyz
    V : numpy.ndarray, shape (n_atoms, n_atoms, 3, 3) or (n_atoms, n_atoms)
        covariance matrix of interatomic displacements
        
    Returns
    -------
    Id : numpy.ndarray, shape (n_points)
        diffuse scattering at q-vectors
    Ib : numpy.ndarray, shape (n_points)
        Bragg scattering at q-vectors
    """
    
    Id = np.zeros(q_grid.shape[0])  
    Ib = np.zeros(q_grid.shape[0])  
    stols2 = np.square(np.linalg.norm(q_grid, axis=1) / (4*np.pi)) 
    
    for i,q_vector in enumerate(q_grid):
        Fd, Fb = 0.0, 0.0
        q_mag = np.linalg.norm(q_vector)
        
        for j in range(xyz.shape[0]):
            for k in range(xyz.shape[0]):
                
                fj = elements[j].it92.calculate_sf(stols2[i])
                fk = elements[k].it92.calculate_sf(stols2[i])
                rjk = xyz[j] - xyz[k]
                
                if len(V.shape) == 2:
                    qVjjq = np.square(q_mag)*V[j][j]
                    qVkkq = np.square(q_mag)*V[k][k] 
                    qVjkq = np.square(q_mag)*V[j][k]
                    
                else:
                    qVjjq = np.dot(q_vector, np.dot(V[j][j], q_vector))
                    qVkkq = np.dot(q_vector, np.dot(V[k][k], q_vector))
                    qVjkq = np.dot(q_vector, np.dot(V[j][k], q_vector))
                
                Fb += fj * np.conj(fk) * np.exp(-1j * np.dot(q_vector, rjk)) * np.exp(-0.5 * qVjjq - 0.5 * qVkkq) 
                Fd += fj * np.conj(fk) * np.exp(-1j * np.dot(q_vector, rjk)) * np.exp(-0.5 * qVjjq - 0.5 * qVkkq) * (np.exp(qVjkq) - 1)
        
        Id[i], Ib[i] = Fd.real, Fb.real
        
    return Id, Ib

</file>
<file path="./eryx/logging_utils.py" project="ptycho">
</file>
<file path="./eryx/base.py" project="ptycho">
import numpy as np
import glob
import re
import os
from .pdb import AtomicModel
from .map_utils import *
from .scatter import structure_factors
from eryx.autotest.debug import debug

def natural_sort(l): 
    """
    Natural sort items in list. Helper function for guinier_reconstruct.
    
    Parameters
    ----------
    l : list of str
        list of strings to natural sort
    
    Returns
    -------
    naturally-sorted list of str
    """
    convert = lambda text: int(text) if text.isdigit() else text.lower()
    alphanum_key = lambda key: [convert(c) for c in re.split('([0-9]+)', key)]
    return sorted(l, key=alphanum_key)

def guinier_reconstruct(ensemble_dir, n_grid_points, res_mask, n_asu, weights=None):
    """
    Reconstruct a map from an ensemble of complex structure factors
    using Guinier's equation. The ensemble directory should contain
    a single file per asu / ensemble member, which are incoherently
    summed assuming that values at unmasked grid points were saved.
    
    Parameters
    ----------
    ensemble_dir : str
        path to directory containing ensemble's structure factors
    n_grid_points : int
        number of q-grid points, i.e. flattened map size
    res_mask : numpy.ndarray, shape (n_grid_points,)
        boolean resolution mask
    n_asu : int
        number of asymmetric units
    weights : numpy.ndarray, shape (n_conformations,)
        weights associated with each state; uniform if not provided
    """
    fnames = glob.glob(os.path.join(ensemble_dir, "*npy"))
    fnames = natural_sort(fnames)
    
    if weights is None:
        weights = np.ones(int(len(fnames) / n_asu)) / (len(fnames) / n_asu)
    
    Id = np.zeros(n_grid_points)
    for asu in range(n_asu):
        fnames_asu = fnames[asu::n_asu]
        fc = np.zeros(n_grid_points, dtype=complex)
        fc_square = np.zeros(n_grid_points)
        
        for i,fname in enumerate(fnames_asu):
            print(fname)
            A = np.load(fname)
            fc[res_mask] += A * weights[i]
            fc_square[res_mask] += np.square(np.abs(A)) * weights[i]
        Id += fc_square - np.square(np.abs(fc))
    
    Id[~res_mask] = np.nan
    return Id

@debug
def compute_crystal_transform(pdb_path, hsampling, ksampling, lsampling, U=None, expand_p1=True, 
                              res_limit=0, batch_size=5000, n_processes=8):
    """
    Compute the crystal transform as the coherent sum of the
    asymmetric units. If expand_p1 is False, it is assumed 
    that the pdb contains asymmetric units as separate frames.
    The crystal transform is only defined at integral Miller 
    indices, so grid points at fractional Miller indices or 
    beyond the resolution limit will be set to zero.
    
    Parameters
    ----------
    pdb_path : str
        path to coordinates file 
    hsampling : tuple, shape (3,)
        (hmin, hmax, oversampling) relative to Miller indices
    ksampling : tuple, shape (3,)
        (kmin, kmax, oversampling) relative to Miller indices
    lsampling : tuple, shape (3,)
        (lmin, lmax, oversampling) relative to Miller indices
    expand_p1 : bool
        if True, expand PDB (asymmetric unit) to unit cell
    U : numpy.ndarray, shape (n_atoms,)
        isotropic displacement parameters, applied to each asymmetric unit
    res_limit : float
        high resolution limit
    batch_size : int
        number of q-vectors to evaluate per batch 
    n_processes : int
        number of processors over which to parallelize the calculation
        
    Returns
    -------
    q_grid : numpy.ndarray, (n_points, 3)
        q-vectors corresponding to flattened intensity map
    I : numpy.ndarray, 3d
        intensity map of the crystal transform
    """
    model = AtomicModel(pdb_path, expand_p1=expand_p1, frame=-1)
    model.flatten_model()
    hkl_grid, map_shape = generate_grid(model.A_inv, 
                                        hsampling,
                                        ksampling, 
                                        lsampling, 
                                        return_hkl=True)
    q_grid = 2*np.pi*np.inner(model.A_inv.T, hkl_grid).T
    mask, res_map = get_resolution_mask(model.cell, hkl_grid, res_limit)
    dq_map = np.around(get_dq_map(model.A_inv, hkl_grid), 5)
    dq_map[~mask] = -1
    
    I = np.zeros(q_grid.shape[0])
    I[dq_map==0] = np.square(np.abs(structure_factors(q_grid[dq_map==0],
                                                      model.xyz, 
                                                      model.ff_a,
                                                      model.ff_b,
                                                      model.ff_c,
                                                      U=U, 
                                                      batch_size=batch_size,
                                                      n_processes=n_processes)))
    return q_grid, I.reshape(map_shape)

@debug
def compute_molecular_transform(pdb_path, hsampling, ksampling, lsampling, U=None, expand_p1=True,
                                expand_friedel=True, res_limit=0, batch_size=10000, n_processes=8):
    """
    Compute the molecular transform as the incoherent sum of the 
    asymmetric units. If expand_p1 is False, the pdb is assumed 
    to contain the asymmetric units as separate frames / models.
    The calculation is accelerated by leveraging symmetry in one
    of two ways, one of which will maintain the input grid extents
    (expand_friedel=False), while the other will output a map that 
    includes the volume of reciprocal space related by Friedel's law.
    If h/k/lsampling are symmetric about (0,0,0), these approaches 
    will yield identical maps. If expand_friedel is False and the
    space group is P1, the simple sum over asus will be performed
    to avoid wasting time on determining symmetry relationships.

    Parameters
    ----------
    pdb_path : str
        path to coordinates file 
    hsampling : tuple, shape (3,)
        (hmin, hmax, oversampling) relative to Miller indices
    ksampling : tuple, shape (3,)
        (kmin, kmax, oversampling) relative to Miller indices
    lsampling : tuple, shape (3,)
        (lmin, lmax, oversampling) relative to Miller indices
    U : numpy.ndarray, shape (n_atoms,)
        isotropic displacement parameters, applied to each asymmetric unit
    expand_p1 : bool
        if True, expand PDB (asymmetric unit) to unit cell
    expand_friedel : bool
        if True, expand to full sphere in reciprocal space
    res_limit : float
        high resolution limit
    batch_size : int
        number of q-vectors to evaluate per batch 
    n_processes : int
        number of processors over which to parallelize the calculation
        
    Returns
    -------
    q_grid : numpy.ndarray, (n_points, 3)
        q-vectors corresponding to flattened intensity map
    I : numpy.ndarray, 3d
        intensity map of the molecular transform
    """
    model = AtomicModel(pdb_path, expand_p1=expand_p1)
    hkl_grid, map_shape = generate_grid(model.A_inv, 
                                        hsampling,
                                        ksampling, 
                                        lsampling, 
                                        return_hkl=True)
    q_grid = 2*np.pi*np.inner(model.A_inv.T, hkl_grid).T
    mask, res_map = get_resolution_mask(model.cell, hkl_grid, res_limit)
    sampling = (hsampling[2], ksampling[2], lsampling[2])

    if model.space_group == 'P 1' and not expand_friedel:
        I = np.zeros(q_grid.shape[0])
        for asu in range(model.xyz.shape[0]):
            I[mask] += np.square(np.abs(structure_factors(q_grid[mask],
                                                          model.xyz[asu],
                                                          model.ff_a[asu], 
                                                          model.ff_b[asu], 
                                                          model.ff_c[asu],
                                                          U=U,
                                                          batch_size=batch_size,
                                                          n_processes=n_processes)))
        I = I.reshape(map_shape)
    else:
        if expand_friedel:
            I = incoherent_sum_real(model, hkl_grid, sampling, U, mask, batch_size, n_processes)
        else:
            I = incoherent_sum_reciprocal(model, hkl_grid, sampling, U, batch_size, n_processes)
            I = I.reshape(map_shape)
            I[~mask.reshape(map_shape)] = 0

    return q_grid, I

@debug
def incoherent_sum_real(model, hkl_grid, sampling, U=None, mask=None, batch_size=10000, n_processes=8):
    """
    Compute the incoherent sum of the scattering from all asus.
    The scattering for the unique reciprocal wedge is computed 
    by summing over all asymmetric units in real space, and then
    using symmetry to extend the calculation to the remainder of
    the map (including the portion of reciprocal space related by
    Friedel's law even if not spanned by the input hkl_grid).
    
    Parameters
    ----------
    model : AtomicModel
        instance of AtomicModel class expanded to p1
    hkl_grid : numpy.ndarray, shape (n_points, 3)
        hkl vectors of map grid points
    sampling : tuple
        sampling frequency along h,k,l axes
    U : numpy.ndarray, shape (n_atoms,)
        isotropic displacement parameters, applied to each asymmetric unit
    batch_size : int
        number of q-vectors to evaluate per batch
    mask : numpy.ndarray, shape (n_points,)
        boolean mask, where True indicates grid points to keep
    n_processes : int
        number of processors over which to parallelize the calculation
        
    Returns
    -------
    I : numpy.ndarray, 3d
        intensity map of the molecular transform
    """
    # generate asu mask and combine with resolution mask
    if mask is None:
        mask = np.ones(hkl_grid.shape[0]).astype(bool)
    mask *= get_asu_mask(model.space_group, hkl_grid)
    
    # sum over asus to compute scattering for unique reciprocal wedge
    q_grid = 2*np.pi*np.inner(model.A_inv.T, hkl_grid).T
    I_asu = np.zeros(q_grid.shape[0])
    for asu in range(model.xyz.shape[0]):
        I_asu[mask] += np.square(np.abs(structure_factors(q_grid[mask],
                                                          model.xyz[asu],
                                                          model.ff_a[asu], 
                                                          model.ff_b[asu], 
                                                          model.ff_c[asu], 
                                                          U=U, 
                                                          batch_size=batch_size,
                                                          n_processes=n_processes)))
        
    # get symmetry information for expanded map
    sym_ops = expand_sym_ops(model.sym_ops)
    hkl_sym = get_symmetry_equivalents(hkl_grid, sym_ops)
    ravel, map_shape_ravel = get_ravel_indices(hkl_sym, sampling)
    sampling_ravel = get_centered_sampling(map_shape_ravel, sampling)
    hkl_grid_mult, mult = compute_multiplicity(model, 
                                               sampling_ravel[0], 
                                               sampling_ravel[1], 
                                               sampling_ravel[2])

    # symmetrize and account for multiplicity
    I = np.zeros(map_shape_ravel).flatten()
    I[ravel[0]] = I_asu.copy()
    for asu in range(1, ravel.shape[0]):
        I[ravel[asu]] += I_asu.copy()
    I = I.reshape(map_shape_ravel)
    I /= (mult.max() / mult) 
    
    sampling_original = [(int(hkl_grid[:,i].min()),int(hkl_grid[:,i].max()),sampling[i]) for i in range(3)]
    I = resize_map(I, sampling_original, sampling_ravel)
    
    return I
    
@debug
def incoherent_sum_reciprocal(model, hkl_grid, sampling, U=None, batch_size=10000, n_processes=8):
    """
    Compute the incoherent sum of the scattering from all asus.
    For each grid point, the symmetry-equivalents are determined
    and mapped from 3d to 1d space by raveling. The intensities 
    for the first asu are computed and mapped to subsequent asus.
    Finally, intensities across symmetry-equivalent reflections 
    are summed (hence in reciprocal rather than real space). The 
    extents defined by hkl_grid are maintained.
    
    Parameters
    ----------
    model : AtomicModel
        instance of AtomicModel class expanded to p1
    hkl_grid : numpy.ndarray, shape (n_points, 3)
        hkl vectors of map grid points
    sampling : tuple
        sampling frequency along h,k,l axes
    U : numpy.ndarray, shape (n_atoms,)
        isotropic displacement parameters, applied to each asymmetric unit
    batch_size : int
        number of q-vectors to evaluate per batch
    n_processes : int
        number of processors over which to parallelize the calculation
        
    Returns
    -------
    I : numpy.ndarray, (n_points,)
        intensity map of the molecular transform
    """
    hkl_grid_sym = get_symmetry_equivalents(hkl_grid, model.sym_ops)
    ravel, map_shape_ravel = get_ravel_indices(hkl_grid_sym, sampling)
    
    I_sym = np.zeros(ravel.shape)
    for asu in range(I_sym.shape[0]):
        q_asu = 2*np.pi*np.inner(model.A_inv.T, hkl_grid_sym[asu]).T
        if asu == 0:
            I_sym[asu] = np.square(np.abs(structure_factors(q_asu,
                                                            model.xyz[0],
                                                            model.ff_a[0],
                                                            model.ff_b[0],
                                                            model.ff_c[0],
                                                            U=U,
                                                            batch_size=batch_size,
                                                            n_processes=n_processes)))
        else:
            intersect1d, comm1, comm2 = np.intersect1d(ravel[0], ravel[asu], return_indices=True)
            I_sym[asu][comm2] = I_sym[0][comm1]
            comm3 = np.arange(len(ravel[asu]))[~np.in1d(ravel[asu],ravel[0])]
            I_sym[asu][comm3] = np.square(np.abs(structure_factors(q_asu[comm3],
                                                                   model.xyz[0],
                                                                   model.ff_a[0],
                                                                   model.ff_b[0],
                                                                   model.ff_c[0],
                                                                   U=U,
                                                                   batch_size=batch_size)))
    I = np.sum(I_sym, axis=0)
    return I

"""
PyTorch implementation of transform calculations for diffuse scattering.

This module contains PyTorch versions of the transform calculation functions defined
in eryx/base.py. All implementations maintain the same API as the NumPy versions
but use PyTorch tensors and operations to enable gradient flow.

References:
    - Original NumPy implementation in eryx/base.py
"""

import numpy as np
import torch
from typing import Tuple, List, Dict, Optional, Union, Any

def compute_molecular_transform(pdb_path: str, 
                              hsampling: Tuple[float, float, float], 
                              ksampling: Tuple[float, float, float], 
                              lsampling: Tuple[float, float, float], 
                              U: Optional[torch.Tensor] = None, 
                              expand_p1: bool = True,
                              expand_friedel: bool = True, 
                              res_limit: float = 0, 
                              batch_size: int = 10000, 
                              n_processes: int = 8) -> Tuple[torch.Tensor, torch.Tensor]:
    """
    Compute the molecular transform as the incoherent sum of asymmetric units using PyTorch.
    
    Args:
        pdb_path: Path to coordinates file
        hsampling: Tuple (hmin, hmax, oversampling) for h dimension
        ksampling: Tuple (kmin, kmax, oversampling) for k dimension
        lsampling: Tuple (lmin, lmax, oversampling) for l dimension
        U: Optional tensor with isotropic displacement parameters
        expand_p1: If True, expand PDB to unit cell
        expand_friedel: If True, expand to full sphere in reciprocal space
        res_limit: High resolution limit in Angstrom
        batch_size: Number of q-vectors per batch
        n_processes: Number of processes (ignored in PyTorch implementation)
        
    Returns:
        Tuple containing:
            - PyTorch tensor of shape (n_points, 3) with q-vectors
            - PyTorch tensor of shape (dim_h, dim_k, dim_l) with intensity map
            
    References:
        - Original implementation: eryx/base.py:compute_molecular_transform
    """
    # TODO: Load model with adapter to convert to PyTorch tensors
    # TODO: Generate grid using map_utils_torch.generate_grid
    # TODO: Apply resolution mask
    # TODO: Choose calculation approach based on space group and expand_friedel
    # TODO: Call appropriate summation function
    
    raise NotImplementedError("compute_molecular_transform not implemented")

def compute_crystal_transform(pdb_path: str, 
                             hsampling: Tuple[float, float, float], 
                             ksampling: Tuple[float, float, float], 
                             lsampling: Tuple[float, float, float], 
                             U: Optional[torch.Tensor] = None, 
                             expand_p1: bool = True, 
                             res_limit: float = 0, 
                             batch_size: int = 5000, 
                             n_processes: int = 8) -> Tuple[torch.Tensor, torch.Tensor]:
    """
    Compute crystal transform as coherent sum of asymmetric units using PyTorch.
    
    Args:
        pdb_path: Path to coordinates file
        hsampling: Tuple (hmin, hmax, oversampling) for h dimension
        ksampling: Tuple (kmin, kmax, oversampling) for k dimension
        lsampling: Tuple (lmin, lmax, oversampling) for l dimension
        U: Optional tensor with isotropic displacement parameters
        expand_p1: If True, expand PDB to unit cell
        res_limit: High resolution limit in Angstrom
        batch_size: Number of q-vectors per batch
        n_processes: Number of processes (ignored in PyTorch implementation)
        
    Returns:
        Tuple containing:
            - PyTorch tensor of shape (n_points, 3) with q-vectors
            - PyTorch tensor of shape (dim_h, dim_k, dim_l) with intensity map
            
    References:
        - Original implementation: eryx/base.py:compute_crystal_transform
    """
    # TODO: Load model with adapter
    # TODO: Generate grid and q-vectors
    # TODO: Apply resolution and dq masks
    # TODO: Compute structure factors for Bragg reflections
    # TODO: Reshape results to 3D map
    
    raise NotImplementedError("compute_crystal_transform not implemented")

def incoherent_sum_real(model: Any, 
                       hkl_grid: torch.Tensor, 
                       sampling: Tuple[float, float, float], 
                       U: Optional[torch.Tensor] = None, 
                       mask: Optional[torch.Tensor] = None, 
                       batch_size: int = 10000, 
                       n_processes: int = 8) -> torch.Tensor:
    """
    Compute incoherent sum of scattering from all ASUs in real space using PyTorch.
    
    Args:
        model: AtomicModel instance (converted to use PyTorch)
        hkl_grid: PyTorch tensor of shape (n_points, 3) with hkl indices
        sampling: Tuple with sampling rates
        U: Optional tensor with displacement parameters
        mask: Optional tensor with boolean mask
        batch_size: Number of q-vectors per batch
        n_processes: Number of processes (ignored in PyTorch implementation)
        
    Returns:
        PyTorch tensor of shape (dim_h, dim_k, dim_l) with intensity map
        
    References:
        - Original implementation: eryx/base.py:incoherent_sum_real
    """
    # TODO: Generate ASU mask and combine with resolution mask
    # TODO: Compute scattering for unique reciprocal wedge
    # TODO: Expand symmetry operations and hkl indices
    # TODO: Get raveled indices
    # TODO: Perform symmetrization using PyTorch operations
    # TODO: Account for multiplicity
    # TODO: Resize map if needed
    
    raise NotImplementedError("incoherent_sum_real not implemented")

def incoherent_sum_reciprocal(model: Any, 
                            hkl_grid: torch.Tensor, 
                            sampling: Tuple[float, float, float], 
                            U: Optional[torch.Tensor] = None, 
                            batch_size: int = 10000, 
                            n_processes: int = 8) -> torch.Tensor:
    """
    Compute incoherent sum of scattering in reciprocal space using PyTorch.
    
    Args:
        model: AtomicModel instance (converted to use PyTorch)
        hkl_grid: PyTorch tensor of shape (n_points, 3) with hkl indices
        sampling: Tuple with sampling rates
        U: Optional tensor with displacement parameters
        batch_size: Number of q-vectors per batch
        n_processes: Number of processes (ignored in PyTorch implementation)
        
    Returns:
        PyTorch tensor of shape (n_points,) with intensity values
        
    References:
        - Original implementation: eryx/base.py:incoherent_sum_reciprocal
    """
    # TODO: Get symmetry equivalents of hkl indices
    # TODO: Compute raveled indices
    # TODO: Initialize tensors for symmetry-equivalent structure factors
    # TODO: Compute structure factors for the first ASU
    # TODO: Map to other ASUs and compute additional structure factors
    # TODO: Sum intensities over all symmetry-equivalents
    
    raise NotImplementedError("incoherent_sum_reciprocal not implemented")
</file>
<file path="./eryx/models_torch.py" project="ptycho">
"""
PyTorch implementation of disorder models for diffuse scattering calculations.

This module contains PyTorch versions of the disorder models defined in eryx/models.py.
All implementations maintain the same API as the NumPy versions but use PyTorch tensors
and operations to enable gradient flow.

References:
    - Original NumPy implementation in eryx/models.py
"""

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import List, Tuple, Dict, Optional, Union, Any

# Forward references for type hints
from eryx.pdb import AtomicModel, Crystal, GaussianNetworkModel

class OnePhonon:
    """
    PyTorch implementation of the OnePhonon model for diffuse scattering calculations.
    
    This class implements a lattice of interacting rigid bodies in the one-phonon
    approximation (a.k.a small-coupling regime) using PyTorch tensors and operations
    to enable gradient flow.
    
    References:
        - Original NumPy implementation in eryx/models.py:OnePhonon
    """
    
    def __init__(self, pdb_path: str, hsampling: Tuple[float, float, float], 
                 ksampling: Tuple[float, float, float], lsampling: Tuple[float, float, float],
                 expand_p1: bool = True, group_by: str = 'asu',
                 res_limit: float = 0., model: str = 'gnm',
                 gnm_cutoff: float = 4., gamma_intra: float = 1., gamma_inter: float = 1.,
                 batch_size: int = 10000, n_processes: int = 8):
        """
        Initialize the OnePhonon model with PyTorch tensors.
        
        Args:
            pdb_path: Path to coordinates file
            hsampling: (hmin, hmax, oversampling) for h dimension
            ksampling: (kmin, kmax, oversampling) for k dimension
            lsampling: (lmin, lmax, oversampling) for l dimension
            expand_p1: If True, expand to p1 (if PDB is asymmetric unit)
            group_by: Level of rigid-body assembly, 'asu' or None
            res_limit: High-resolution limit in Angstrom
            model: Chosen phonon model ('gnm' or 'rb')
            gnm_cutoff: Distance cutoff for GNM in Angstrom
            gamma_intra: Spring constant for atom pairs in same molecule
            gamma_inter: Spring constant for atom pairs in different molecules
            batch_size: Number of q-vectors to evaluate per batch
            n_processes: Number of processes for parallel computation
            
        References:
            - Original implementation: eryx/models.py:OnePhonon.__init__
        """
        # TODO: Initialize class attributes similar to the NumPy implementation
        # TODO: Convert sampling tuples to PyTorch compatible formats
        # TODO: Call self._setup() and self._setup_phonons() to initialize tensors
        
        self.hsampling = hsampling
        self.ksampling = ksampling
        self.lsampling = lsampling
        self.batch_size = batch_size
        self.n_processes = n_processes
        
        # These will be initialized in _setup() and _setup_phonons()
        self.model = None
        self.q_grid = None
        self.crystal = None
        self.res_mask = None
        self.group_by = group_by
        
        # Placeholder for a proper implementation
        raise NotImplementedError("OnePhonon.__init__ not implemented")
    
    def _setup(self, pdb_path: str, expand_p1: bool, res_limit: float, group_by: str):
        """
        Set up class, computing q-vectors and building the unit cell.
        
        Args:
            pdb_path: Path to coordinates file
            expand_p1: If True, expand to p1 (if PDB is asymmetric unit)
            res_limit: High-resolution limit in Angstrom
            group_by: Level of rigid-body assembly, 'asu' or None
            
        References:
            - Original implementation: eryx/models.py:OnePhonon._setup
        """
        # TODO: Create AtomicModel using adapter
        # TODO: Generate reciprocal space grid and convert to torch.Tensor
        # TODO: Calculate q vectors and q magnitudes as torch tensors
        # TODO: Set up Crystal object and compute necessary dimensions
        
        raise NotImplementedError("OnePhonon._setup not implemented")
    
    def _setup_phonons(self, pdb_path: str, model: str, 
                     gnm_cutoff: float, gamma_intra: float, gamma_inter: float):
        """
        Compute phonons from a Gaussian Network Model using PyTorch operations.
        
        Args:
            pdb_path: Path to coordinates file
            model: Chosen phonon model ('gnm' or 'rb')
            gnm_cutoff: Distance cutoff for GNM in Angstrom
            gamma_intra: Spring constant for atom pairs in same molecule
            gamma_inter: Spring constant for atom pairs in different molecules
            
        References:
            - Original implementation: eryx/models.py:OnePhonon._setup_phonons
        """
        # TODO: Initialize tensor arrays for phonon calculations
        # TODO: Build A and M matrices using PyTorch operations
        # TODO: Compute k-vectors in Brillouin zone as tensors
        # TODO: Setup GNM and compute phonon modes
        
        raise NotImplementedError("OnePhonon._setup_phonons not implemented")
    
    def _build_A(self):
        """
        Build the matrix A that projects small rigid-body displacements using PyTorch.
        
        References:
            - Original implementation: eryx/models.py:OnePhonon._build_A
        """
        # TODO: Implement tensor-based projection matrix construction
        # TODO: Handle the group_by='asu' case with PyTorch matrix operations
        # TODO: Ensure gradient flow through all operations
        
        raise NotImplementedError("OnePhonon._build_A not implemented")
    
    def _build_M(self):
        """
        Build the mass matrix M using PyTorch operations.
        
        References:
            - Original implementation: eryx/models.py:OnePhonon._build_M
        """
        # TODO: Get all-atoms mass matrix with _build_M_allatoms()
        # TODO: Project if needed based on group_by parameter
        # TODO: Implement Cholesky decomposition with PyTorch for Linv
        
        raise NotImplementedError("OnePhonon._build_M not implemented")
    
    def _build_M_allatoms(self) -> torch.Tensor:
        """
        Build all-atom mass matrix using PyTorch operations.
        
        Returns:
            torch.Tensor: Mass matrix for all atoms
            
        References:
            - Original implementation: eryx/models.py:OnePhonon._build_M_allatoms
        """
        # TODO: Convert mass array to tensor
        # TODO: Create block diagonal mass matrix
        # TODO: Reshape to the correct dimensions
        
        raise NotImplementedError("OnePhonon._build_M_allatoms not implemented")
    
    def _project_M(self, M_allatoms: torch.Tensor) -> torch.Tensor:
        """
        Project all-atom mass matrix using PyTorch tensor operations.
        
        Args:
            M_allatoms: All-atom mass matrix tensor
            
        Returns:
            torch.Tensor: Projected mass matrix
            
        References:
            - Original implementation: eryx/models.py:OnePhonon._project_M
        """
        # TODO: Initialize output tensor with correct shape
        # TODO: Implement matrix multiplication with PyTorch for projection
        # TODO: Ensure gradient flow through operations
        
        raise NotImplementedError("OnePhonon._project_M not implemented")
    
    def _build_kvec_Brillouin(self):
        """
        Compute k-vectors and their norm in the first Brillouin zone using PyTorch.
        
        References:
            - Original implementation: eryx/models.py:OnePhonon._build_kvec_Brillouin
        """
        # TODO: Generate k-vector grid using PyTorch's meshgrid
        # TODO: Compute k-vector norms with torch.norm
        # TODO: Store as tensors for differentiable computations
        
        raise NotImplementedError("OnePhonon._build_kvec_Brillouin not implemented")
    
    def _center_kvec(self, x: int, L: int) -> float:
        """
        Center k-vector components.
        
        Args:
            x: Index to center
            L: Length of periodic box
            
        Returns:
            float: Centered k-vector component
            
        References:
            - Original implementation: eryx/models.py:OnePhonon._center_kvec
        """
        # This function can remain the same as it's a simple calculation
        # that doesn't need tensor operations
        return int(((x - L / 2) % L) - L / 2) / L
    
    def _at_kvec_from_miller_points(self, hkl_kvec: Tuple[int, int, int]):
        """
        Return indices of q-vectors that are k-vector away from Miller indices.
        
        Args:
            hkl_kvec: Fractional Miller index tuple
            
        Returns:
            torch.Tensor: Indices of q-vectors
            
        References:
            - Original implementation: eryx/models.py:OnePhonon._at_kvec_from_miller_points
        """
        # TODO: Calculate index grid
        # TODO: Convert to PyTorch tensor for output
        # TODO: Handle ravel operation with PyTorch
        
        raise NotImplementedError("OnePhonon._at_kvec_from_miller_points not implemented")
    
    def compute_hessian(self) -> torch.Tensor:
        """
        Build the projected Hessian matrix using PyTorch operations.
        
        Returns:
            torch.Tensor: Hessian matrix tensor
            
        References:
            - Original implementation: eryx/models.py:OnePhonon.compute_hessian
        """
        # TODO: Initialize Hessian tensor with complex dtype
        # TODO: Initialize diagonal tensor
        # TODO: Compute off-diagonal and diagonal elements
        # TODO: Ensure proper gradient flow
        
        raise NotImplementedError("OnePhonon.compute_hessian not implemented")
    
    def compute_gnm_phonons(self):
        """
        Compute phonon modes and frequencies with PyTorch operations.
        
        References:
            - Original implementation: eryx/models.py:OnePhonon.compute_gnm_phonons
        """
        # TODO: Compute Hessian matrix
        # TODO: For each k-vector, compute dynamical matrix
        # TODO: Use torch.linalg.svd for eigendecomposition
        # TODO: Store eigenvalues and eigenvectors in tensors
        
        raise NotImplementedError("OnePhonon.compute_gnm_phonons not implemented")
    
    def compute_covariance_matrix(self):
        """
        Compute atomic displacement covariance matrix with PyTorch.
        
        References:
            - Original implementation: eryx/models.py:OnePhonon.compute_covariance_matrix
        """
        # TODO: Initialize covariance tensor with complex dtype
        # TODO: Compute for each k-vector with phase factors
        # TODO: Scale to match experimental ADPs
        # TODO: Compute ADP values from covariance
        
        raise NotImplementedError("OnePhonon.compute_covariance_matrix not implemented")
    
    def apply_disorder(self, rank: int = -1, outdir: Optional[str] = None, 
                     use_data_adp: bool = False):
        """
        Compute diffuse intensity map using PyTorch operations.
        
        Args:
            rank: If -1, sum across ranks; else use specific rank
            outdir: Directory to save results
            use_data_adp: If True, use ADPs from data instead of computed ones
            
        Returns:
            torch.Tensor: Diffuse intensity map
            
        References:
            - Original implementation: eryx/models.py:OnePhonon.apply_disorder
        """
        # TODO: Choose appropriate ADPs based on use_data_adp
        # TODO: Initialize output tensor with complex dtype
        # TODO: For each k-vector, compute structure factors
        # TODO: Apply phonon mode calculations and summation
        # TODO: Apply resolution mask
        # TODO: Save results if outdir is provided
        
        raise NotImplementedError("OnePhonon.apply_disorder not implemented")

# Add stubs for additional classes as well:

class RigidBodyTranslations:
    """
    PyTorch implementation of rigid body translation disorder model.
    
    References:
        - Original NumPy implementation in eryx/models.py:RigidBodyTranslations
    """
    # TODO: Implement initialization and methods with PyTorch operations
    pass

class LiquidLikeMotions:
    """
    PyTorch implementation of liquid-like motions disorder model.
    
    References:
        - Original NumPy implementation in eryx/models.py:LiquidLikeMotions
    """
    # TODO: Implement initialization and methods with PyTorch operations
    pass

class RigidBodyRotations:
    """
    PyTorch implementation of rigid body rotations disorder model.
    
    References:
        - Original NumPy implementation in eryx/models.py:RigidBodyRotations
    """
    # TODO: Implement initialization and methods with PyTorch operations
    pass
</file>
<file path="./eryx/scatter.py" project="ptycho">
import numpy as np
import multiprocess as mp
from functools import partial
from eryx.autotest.debug import debug

@debug
def compute_form_factors(q_grid, ff_a, ff_b, ff_c):
    """
    Evaluate atomic form factors at the input q-vectors.
    
    Parameters
    ----------
    q_grid : numpy.ndarray, shape (n_points, 3)
        q-vectors in Angstrom
    ff_a : numpy.ndarray, shape (n_atoms, 4)
        a coefficient of atomic form factors
    ff_b : numpy.ndarray, shape (n_atoms, 4)
        b coefficient of atomic form factors
    ff_c : numpy.ndarray, shape (n_atoms,)
        c coefficient of atomic form factors

    Returns
    -------
    fj : numpy.ndarray, shape (n_points, n_atoms)
        atomic form factors 
    """
    Q = np.square(np.linalg.norm(q_grid, axis=1) / (4*np.pi))
    fj = ff_a[:,:,np.newaxis] * np.exp(-1 * ff_b[:,:,None] * Q[:,np.newaxis].T)
    fj = np.sum(fj, axis=1) + ff_c[:,np.newaxis]
    return fj.T

@debug
def structure_factors_batch(q_grid, xyz, ff_a, ff_b, ff_c, U=None,
                            compute_qF=False, project_on_components=None,
                            sum_over_atoms=True):
    """
    Compute the structure factors for an atomic model at 
    the given q-vectors. 

    Parameters
    ----------
    q_grid : numpy.ndarray, shape (n_points, 3)
        q-vectors in Angstrom
    xyz : numpy.ndarray, shape (n_atoms, 3)
        atomic xyz positions in Angstroms
    ff_a : numpy.ndarray, shape (n_atoms, 4)
        a coefficient of atomic form factors
    ff_b : numpy.ndarray, shape (n_atoms, 4)
        b coefficient of atomic form factors
    ff_c : numpy.ndarray, shape (n_atoms,)
        c coefficient of atomic form factors
    U : numpy.ndarray, shape (n_atoms,) 
        isotropic displacement parameters
    compute_qF : boolean
        False (default).
        If true, return structure factors at q-vectors times q-vectors
    project_on_components : None (default) or numpy.ndarray, shape (n_atoms, n_components)
        Projection matrix to convert structure factors into component factors
    sum_over_atoms: boolean
        True (default) returns summed structure factor.
        
    Returns
    -------
    A : numpy.ndarray, shape (n_points) or (n_points, n_atoms) or (n_points, n_components)
        structure factors at q-vectors.
        If compute_qF, return [q_x A(q), q_y A(q), q_z A(q)] instead of A(q)
    """
    if U is None:
        U = np.zeros(xyz.shape[0])
    
    fj = compute_form_factors(q_grid, ff_a, ff_b, ff_c)
    qmags = np.linalg.norm(q_grid, axis=1)
    qUq = np.square(qmags[:,np.newaxis]) * U
    
    A = 1j * fj * np.sin(np.dot(q_grid, xyz.T)) * np.exp(-0.5 * qUq)
    A += fj * np.cos(np.dot(q_grid, xyz.T)) * np.exp(-0.5 * qUq)
    if compute_qF:
        A = A[:,:,None] * q_grid[:,None,:]
        A = A.reshape((A.shape[0],A.shape[1]*A.shape[2]))
    if project_on_components is not None:
        A = np.matmul(A, project_on_components)
    if sum_over_atoms:
        A = np.sum(A, axis=1)
    return A 

@debug
def structure_factors(q_grid, xyz, ff_a, ff_b, ff_c, U=None,
                      batch_size=100000, n_processes=8,
                      compute_qF=False, project_on_components=None,
                      sum_over_atoms=True):
    """
    Batched version of the structure factor calculation. See 
    docstring for structure_factors_batch for parameters and 
    returns, with the exception of n_processes, which refers 
    to the number of processors available. If greater than 1,
    multiprocessing will be used.
    """
    n_batches = q_grid.shape[0] // batch_size
    if n_batches == 0:
        n_batches = 1
    splits = np.append(np.arange(n_batches) * batch_size, np.array([q_grid.shape[0]]))

    if n_processes == 1:
        dim1_size = q_grid.shape[0]
        if sum_over_atoms:
            A_shape = dim1_size
        else:
            if project_on_components is None:
                dim2_size = xyz.shape[0]
            else:
                dim2_size = project_on_components.shape[1]
            A_shape = (dim1_size, dim2_size)
        A = np.zeros(A_shape, dtype=np.complex128)
        for batch in range(n_batches):
            q_sel = q_grid[splits[batch]: splits[batch+1]]
            A[splits[batch]: splits[batch+1]] = structure_factors_batch(q_sel, xyz, ff_a, ff_b, ff_c, U=U, compute_qF=compute_qF,
                                                                        project_on_components=project_on_components, sum_over_atoms=sum_over_atoms)
    else:
        q_sel = [q_grid[splits[batch]: splits[batch+1]] for batch in range(n_batches)]
        pool = mp.Pool(processes=n_processes)
        sf_partial = partial(structure_factors_batch, xyz=xyz, ff_a=ff_a, ff_b=ff_b, ff_c=ff_c, U=U,
                             compute_qF=compute_qF,
                             project_on_components=project_on_components, sum_over_atoms=sum_over_atoms)
        A = np.concatenate(pool.map(sf_partial, q_sel), axis=0)
       
    return A
</file>
<file path="./eryx/autotest/run_tests.py" project="ptycho">
#!/usr/bin/env python3
"""
Test runner for PyTorch implementation.

This script runs tests for the PyTorch implementation of diffuse scattering
calculations, comparing results with the NumPy implementation.
"""

import os
import sys
import argparse
import logging
import importlib
from typing import List, Optional

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s %(levelname)s: %(message)s"
)
logger = logging.getLogger(__name__)

def discover_tests(test_dir: str, pattern: str = "test_*.py") -> List[str]:
    """
    Discover test files in the given directory.
    
    Args:
        test_dir: Directory to search for tests
        pattern: Pattern to match test files
        
    Returns:
        List of test module names
    """
    test_modules = []
    for root, _, files in os.walk(test_dir):
        for file in files:
            if file.endswith(".py") and file.startswith("test_"):
                # Convert path to module name
                rel_path = os.path.relpath(os.path.join(root, file), os.path.dirname(test_dir))
                module_name = os.path.splitext(rel_path)[0].replace(os.path.sep, ".")
                test_modules.append(module_name)
    return test_modules

def run_test_module(module_name: str) -> bool:
    """
    Run tests in the given module.
    
    Args:
        module_name: Name of the test module
        
    Returns:
        True if all tests pass, False otherwise
    """
    try:
        logger.info(f"Running tests in {module_name}")
        module = importlib.import_module(module_name)
        
        # Find test functions in the module
        test_functions = [
            getattr(module, name) for name in dir(module)
            if name.startswith("test_") and callable(getattr(module, name))
        ]
        
        if not test_functions:
            logger.warning(f"No test functions found in {module_name}")
            return True
        
        # Run each test function
        passed = True
        for test_func in test_functions:
            try:
                logger.info(f"  Running {test_func.__name__}")
                result = test_func()
                if result is False:  # Explicitly check for False
                    logger.error(f"  {test_func.__name__} failed")
                    passed = False
                else:
                    logger.info(f"  {test_func.__name__} passed")
            except Exception as e:
                logger.error(f"  {test_func.__name__} raised exception: {e}")
                passed = False
        
        return passed
    except Exception as e:
        logger.error(f"Error running tests in {module_name}: {e}")
        return False

def run_tests(test_modules: List[str]) -> bool:
    """
    Run all tests in the given modules.
    
    Args:
        test_modules: List of test module names
        
    Returns:
        True if all tests pass, False otherwise
    """
    passed = True
    for module_name in test_modules:
        if not run_test_module(module_name):
            passed = False
    return passed

def main():
    """
    Main entry point for the test runner.
    """
    parser = argparse.ArgumentParser(description="Run tests for PyTorch implementation")
    parser.add_argument("--test-dir", default="tests", help="Directory containing tests")
    parser.add_argument("--pattern", default="test_*_torch.py", help="Pattern to match test files")
    parser.add_argument("--verbose", action="store_true", help="Enable verbose output")
    args = parser.parse_args()
    
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    logger.info("Discovering tests...")
    test_modules = discover_tests(args.test_dir, args.pattern)
    
    if not test_modules:
        logger.warning(f"No test modules found in {args.test_dir} matching {args.pattern}")
        return 1
    
    logger.info(f"Found {len(test_modules)} test modules")
    for module in test_modules:
        logger.info(f"  {module}")
    
    logger.info("Running tests...")
    if run_tests(test_modules):
        logger.info("All tests passed!")
        return 0
    else:
        logger.error("Some tests failed")
        return 1

if __name__ == "__main__":
    sys.exit(main())
</file>
<file path="./eryx/autotest/functionmapping.py" project="ptycho">
# spec
#    interface FunctionMapping {
#        """
#        Retrieves the log file path for a given function.
#
#        Preconditions:
#        - `func` must be a callable.
#        - `log_directory` must be a valid directory path.
#        - Expected JSON format: { "log_directory": "string" }
#
#        Postconditions:
#        - Returns the log file path for the given function, formatted as `prefix/module.fname<suffix>.log`.
#        - If `log_directory` is not provided or is an empty string, returns an empty string.
#        """
#        string getLogFilePath(Callable func, string log_directory);
#
#        """
#        Loads a function given its log file path or module path.
#
#        Preconditions:
#        - `log_file_path` must be a valid log file path or empty string.
#        - `module_path` must be a valid module path or empty string.
#        - Expected JSON format: { "log_file_path": "string", "module_path": "string" }
#
#        Postconditions:
#        - Returns the function object if successfully loaded.
#        - If the function cannot be found or imported, returns None.
#        """
#        Union[Callable, None] loadFunction(string log_file_path, string module_path);
#
#        """
#        Retrieves the module path for a given function.
#
#        Preconditions:
#        - `func` must be a callable.
#
#        Postconditions:
#        - Returns the module path for the given function, formatted as `module.fname`.
#        - If `func` is a built-in function or does not have a valid module path, returns an empty string.
#        """
#        string getModulePath(Callable func);
#    };

# implementation
import os
import shutil
import importlib
from typing import Callable, Optional

def dprint(*args):
    pass

class FunctionMapping:
    def __init__(self, log_directory: str = "logs"):
        self.log_directory = log_directory

    def get_log_file_path(self, func: Callable) -> str:
        """
        Retrieves the log file path for a given function.
        
        Preconditions:
        - `func` must be a callable.
        
        Postconditions:
        - Returns the log file path for the given function, formatted as `prefix/module.fname<suffix>.log`.
        >>> function_mapping = FunctionMapping(log_directory="test_logs")
        >>> def sample_function():
        ...     return "sample function executed"
        >>> function_mapping.get_log_file_path(sample_function)
        'test_logs/__main__.sample_function.log'
        """
        module_name = func.__module__
        func_name = func.__name__
        log_file_path = f"{self.log_directory}/{module_name}.{func_name}.log"
        return log_file_path

    def save_function(self, log_file_path: str, func: Callable) -> None:
        module_path, func_name = self.get_module_and_function_from_log_path(log_file_path)
        module = importlib.import_module(module_path)
        setattr(module, func_name, func)

    def load_function_from_path(self, log_file_path: str) -> Optional[Callable]:
        try:
            dprint(f"log_file_path: {log_file_path}")
            module_path, func_name = self.get_module_and_function_from_log_path(log_file_path)
            dprint(f"module_path: {module_path}")
            dprint(f"func_name: {func_name}")
            dprint(f"Importing module: {module_path}")
            module = importlib.import_module(module_path)
            dprint(f"Imported module: {module}")
            dprint(f"Retrieving function: {func_name}")
            func = getattr(module, func_name, None)
            dprint(f"Retrieved function: {func}")
            return func
        except Exception as e:
            dprint(f"Error loading function: {e}")
            return None

    def get_module_and_function_from_log_path(self, log_file_path: str) -> tuple:
        dprint(f"log_file_path: {log_file_path}")
        log_file_path = log_file_path.replace(f"{self.log_directory}/", "")
        dprint(f"log_file_path after removing log_directory: {log_file_path}")
        log_file_path = log_file_path.replace(".log", "")
        dprint(f"log_file_path after removing .log: {log_file_path}")
        parts = log_file_path.rsplit(".", 1)
        print(parts)
        dprint(f"parts: {parts}")
        module_path = parts[0]
        dprint(f"module_path: {module_path}")
        func_name = parts[1]
        dprint(f"func_name: {func_name}")
        return module_path, func_name

    def load_function(self, log_file_path: str) -> Optional[Callable]:
        """
        Loads a function given its log file path.
        
        Preconditions:
        - `log_file_path` must be valid.
        
        Postconditions:
        - Returns the function object if successfully loaded.
        - If the function cannot be found or imported, returns None.
        >>> function_mapping = FunctionMapping(log_directory="test_logs")
        >>> def sample_function():
        ...     return "sample function executed"
        >>> log_file_path = function_mapping.get_log_file_path(sample_function)
        >>> loaded_func = function_mapping.load_function(log_file_path)
        """
        return self.load_function_from_path(log_file_path)

    def get_module_path(self, func: Callable) -> str:
        """
        Retrieves the module path for a given function.
        
        Preconditions:
        - `func` must be a callable.
        
        Postconditions:
        - Returns the module path for the given function, formatted as `module.fname`.
        >>> function_mapping = FunctionMapping(log_directory="test_logs")
        >>> def sample_function():
        ...     return "sample function executed"
        >>> function_mapping.get_module_path(sample_function)
        '__main__.sample_function'
        """
        module_name = func.__module__
        func_name = func.__name__
        module_path = f"{module_name}.{func_name}"
        return module_path


if __name__ == "__main__":
    import doctest
    doctest.testmod(verbose=True)

def sample_function():
    return "sample function executed"

def another_function():
    return "another function executed"

def test_get_log_file_path():
    function_mapping = FunctionMapping(log_directory="test_logs")
    path = function_mapping.get_log_file_path(sample_function)
    assert path == 'test_logs/__main__.sample_function.log', f"Expected 'test_logs/__main__.sample_function.log', got '{path}'"

def test_load_function():
    function_mapping = FunctionMapping(log_directory="test_logs")
    log_file_path = function_mapping.get_log_file_path(sample_function)
    
    loaded_func = function_mapping.load_function(log_file_path=log_file_path)
    assert loaded_func is not None, "Expected function to be loaded, but got None"
    assert loaded_func.__name__ == 'sample_function', f"Expected 'sample_function', got '{loaded_func.__name__}'"

def test_get_module_path():
    function_mapping = FunctionMapping(log_directory="test_logs")
    path = function_mapping.get_module_path(sample_function)
    assert path == '__main__.sample_function', f"Expected '__main__.sample_function', got '{path}'"

if __name__ == "__main__":
    test_get_log_file_path()
    test_load_function()
    test_get_module_path()
    print("All tests passed!")
</file>
<file path="./eryx/autotest/testing.py" project="ptycho">
from .logger import Logger
from .functionmapping import FunctionMapping
from .configuration import Configuration
import unittest

from typing import List, Tuple, Any, Optional, Callable, Union

class TestSummary:
    def __init__(self):
        self.passed = 0
        self.failed = 0
        self.skipped = 0

    def increment_passed(self):
        self.passed += 1

    def increment_failed(self):
        self.failed += 1

    def increment_skipped(self):
        self.skipped += 1

    def __repr__(self):
        return f"TestSummary(passed={self.passed}, failed={self.failed}, skipped={self.skipped})"

class Testing:
    def __init__(self, logger: Logger, function_mapping: FunctionMapping):
        self.logger = logger
        self.function_mapping = function_mapping

class Testing:
    def __init__(self, logger: Logger, function_mapping: FunctionMapping):
        self.logger = logger
        self.function_mapping = function_mapping

    def testCallable(self, log_path_prefix: str, func: Callable) -> bool:
        print(f"Debug: testCallable called with log_path_prefix: {log_path_prefix}")
        log_files = self.logger.searchLogDirectory(log_path_prefix)
        print(f"Debug: Found log files: {log_files}")
        for log_file in log_files:
            logs = self.logger.loadLog(log_file)
            #print(f"Debug: Loaded logs: {logs}")
            for i in range(len(logs) // 2):
                args = logs[2 * i]['args']
                kwargs = logs[2 * i]['kwargs']
                expected_output = logs[2 * i + 1]['result']
                try:
                    deserialized_args = self.logger.serializer.deserialize(args)
                    deserialized_kwargs = self.logger.serializer.deserialize(kwargs)
                    deserialized_expected_output = self.logger.serializer.deserialize(expected_output)
                    actual_output = func(*deserialized_args, **deserialized_kwargs)
                    #print(f"Debug: Actual output: {actual_output}")
                    if actual_output != deserialized_expected_output:
                        print("Debug: Test failed")
                        return False
                except Exception as e:
                    print(f"Error testing function: {e}")
                    return False
        print("Debug: Test passed")
        return True

    def createTestCase(self, log_path_prefix: str) -> Union[tuple, None]:
        print(f"Debug: createTestCase called with log_path_prefix: {log_path_prefix}")
        log_files = self.logger.searchLogDirectory(log_path_prefix)
        print(f"Debug: Found log files: {log_files}")
        for log_file in log_files:
            logs = self.logger.loadLog(log_file)
            #print(f"Debug: Loaded logs: {logs}")
            if logs:
                log = logs[0]
                inputs = log['args']
                expected_output = log['result']
                func = self.function_mapping.load_function(log_file)
                print(f"Debug: Loaded function: {func}")
                if func is not None:
                    return (inputs, expected_output, func)
        print("Debug: No test case found")
        return None

    def runTestSuite(self, log_path_prefix: str) -> TestSummary:
        print(f"Debug: runTestSuite called with log_path_prefix: {log_path_prefix}")
        summary = TestSummary()
        log_files = self.logger.searchLogDirectory(log_path_prefix)
        print(f"Debug: Found log files: {log_files}")
        for log_file in log_files:
            test_case = self.createTestCase(log_path_prefix)
            if test_case is not None:
                inputs, expected_output, func = test_case
                if self.testCallable(log_path_prefix, func):
                    summary.increment_passed()
                else:
                    summary.increment_failed()
            else:
                summary.increment_skipped()
        print(f"Debug: Test summary: {summary}")
        return summary

class TestSummary:
    def __init__(self):
        self.passed = 0
        self.failed = 0
        self.skipped = 0

    def increment_passed(self):
        self.passed += 1

    def increment_failed(self):
        self.failed += 1

    def increment_skipped(self):
        self.skipped += 1

    def __repr__(self):
        return f"TestSummary(passed={self.passed}, failed={self.failed}, skipped={self.skipped})"


def add(x, y):
    return x + y

def multiply(x, y):
    return x * y

def divide(x, y):
    return x / y

class TestTesting(unittest.TestCase):
    def setUp(self):
        self.logger = Logger()
        self.function_mapping = FunctionMapping()
        self.testing = Testing(self.logger, self.function_mapping)

    def test_testCallable(self):
        log_path_prefix = 'test_logs'
        self.logger.logReturn(log_path_prefix + '/add', (3, 4), 7)
        self.assertTrue(self.testing.testCallable(log_path_prefix, add))

    def test_createTestCase(self):
        log_path_prefix = 'test_logs'
        self.logger.logReturn(log_path_prefix + '/add', (3, 4), 7)
        self.function_mapping.save_function(log_path_prefix + '/add', add)
        test_case = self.testing.createTestCase(log_path_prefix)
        self.assertIsNotNone(test_case)
        inputs, expected_output, func = test_case
        self.assertEqual(self.logger.serializer.deserialize(inputs), (3, 4))
        self.assertEqual(self.logger.serializer.deserialize(expected_output), 7)
        self.assertEqual(func, add)

    def test_runTestSuite(self):
        log_path_prefix = 'test_logs'
        self.logger.logReturn(log_path_prefix + '/add', (3, 4), 7)
        self.logger.logReturn(log_path_prefix + '/multiply', (3, 4), 12)
        self.function_mapping.save_function(log_path_prefix + '/add', add)
        self.function_mapping.save_function(log_path_prefix + '/multiply', multiply)
        summary = self.testing.runTestSuite(log_path_prefix)
        self.assertIsInstance(summary, TestSummary)
        self.assertEqual(summary.passed, 2)
        self.assertEqual(summary.failed, 0)
        self.assertEqual(summary.skipped, 0)

if __name__ == '__main__':
    unittest.main(argv=[''], verbosity=2, exit=False)
"""
Testing utilities for PyTorch implementation.

This module extends the autotest framework with PyTorch-specific testing
capabilities, including tensor comparison, gradient checking, and
PyTorch-NumPy conversion for testing.
"""

import numpy as np
import torch
from typing import Any, Callable, Dict, List, Optional, Tuple, Union
from eryx.autotest.testing import Testing
from eryx.autotest.logger import Logger
from eryx.autotest.functionmapping import FunctionMapping

class TorchTesting(Testing):
    """
    Extended testing framework for PyTorch implementations.
    
    This class extends the Testing class with PyTorch-specific utilities,
    including tensor comparison and gradient checking.
    """
    
    def __init__(self, logger: Logger, function_mapping: FunctionMapping, 
                rtol: float = 1e-5, atol: float = 1e-8):
        """
        Initialize the PyTorch testing framework.
        
        Args:
            logger: Logger instance for test logging
            function_mapping: FunctionMapping instance for function lookup
            rtol: Relative tolerance for tensor comparison
            atol: Absolute tolerance for tensor comparison
        """
        super().__init__(logger, function_mapping)
        self.rtol = rtol
        self.atol = atol
    
    def testTorchCallable(self, log_path_prefix: str, torch_func: Callable) -> bool:
        """
        Test a PyTorch function against NumPy implementation.
        
        Args:
            log_path_prefix: Path prefix for log files
            torch_func: PyTorch function to test
            
        Returns:
            True if test passes, False otherwise
        """
        log_files = self.logger.searchLogDirectory(log_path_prefix)
        for log_file in log_files:
            logs = self.logger.loadLog(log_file)
            for i in range(len(logs) // 2):
                args = logs[2 * i]['args']
                kwargs = logs[2 * i]['kwargs']
                expected_output = logs[2 * i + 1]['result']
                try:
                    deserialized_args = self._numpy_to_torch(self.logger.serializer.deserialize(args))
                    deserialized_kwargs = self._numpy_to_torch(self.logger.serializer.deserialize(kwargs))
                    deserialized_expected_output = self.logger.serializer.deserialize(expected_output)
                    
                    actual_output = torch_func(*deserialized_args, **deserialized_kwargs)
                    
                    numpy_actual_output = self._torch_to_numpy(actual_output)
                    
                    if not self._compare_outputs(numpy_actual_output, deserialized_expected_output):
                        print(f"Test failed for {log_file}")
                        return False
                except Exception as e:
                    print(f"Error testing PyTorch function: {e}")
                    return False
        return True
    
    def _numpy_to_torch(self, obj: Any) -> Any:
        """
        Convert NumPy arrays to PyTorch tensors recursively.
        
        Args:
            obj: Input object potentially containing NumPy arrays
            
        Returns:
            Object with NumPy arrays converted to PyTorch tensors
        """
        if isinstance(obj, np.ndarray):
            return torch.from_numpy(obj.copy())
        elif isinstance(obj, list):
            return [self._numpy_to_torch(item) for item in obj]
        elif isinstance(obj, tuple):
            return tuple(self._numpy_to_torch(item) for item in obj)
        elif isinstance(obj, dict):
            return {k: self._numpy_to_torch(v) for k, v in obj.items()}
        else:
            return obj
    
    def _torch_to_numpy(self, obj: Any) -> Any:
        """
        Convert PyTorch tensors to NumPy arrays recursively.
        
        Args:
            obj: Input object potentially containing PyTorch tensors
            
        Returns:
            Object with PyTorch tensors converted to NumPy arrays
        """
        if isinstance(obj, torch.Tensor):
            return obj.detach().cpu().numpy()
        elif isinstance(obj, list):
            return [self._torch_to_numpy(item) for item in obj]
        elif isinstance(obj, tuple):
            return tuple(self._torch_to_numpy(item) for item in obj)
        elif isinstance(obj, dict):
            return {k: self._torch_to_numpy(v) for k, v in obj.items()}
        else:
            return obj
    
    def _compare_outputs(self, actual: Any, expected: Any) -> bool:
        """
        Compare outputs with tolerance for numerical differences.
        
        Args:
            actual: Actual output
            expected: Expected output
            
        Returns:
            True if outputs match within tolerance, False otherwise
        """
        if isinstance(actual, np.ndarray) and isinstance(expected, np.ndarray):
            if actual.shape != expected.shape:
                print(f"Shape mismatch: {actual.shape} vs {expected.shape}")
                return False
            
            nan_mask_actual = np.isnan(actual)
            nan_mask_expected = np.isnan(expected)
            if not np.array_equal(nan_mask_actual, nan_mask_expected):
                print("NaN pattern mismatch")
                return False
            
            non_nan_mask = ~nan_mask_actual
            if np.any(non_nan_mask):
                return np.allclose(actual[non_nan_mask], expected[non_nan_mask], 
                                 rtol=self.rtol, atol=self.atol)
            return True
        
        elif isinstance(actual, list) and isinstance(expected, list):
            if len(actual) != len(expected):
                print(f"Length mismatch: {len(actual)} vs {len(expected)}")
                return False
            return all(self._compare_outputs(a, e) for a, e in zip(actual, expected))
        
        elif isinstance(actual, tuple) and isinstance(expected, tuple):
            if len(actual) != len(expected):
                print(f"Length mismatch: {len(actual)} vs {len(expected)}")
                return False
            return all(self._compare_outputs(a, e) for a, e in zip(actual, expected))
        
        elif isinstance(actual, dict) and isinstance(expected, dict):
            if set(actual.keys()) != set(expected.keys()):
                print(f"Key mismatch: {set(actual.keys())} vs {set(expected.keys())}")
                return False
            return all(self._compare_outputs(actual[k], expected[k]) for k in actual)
        
        else:
            return actual == expected
    
    def check_gradients(self, torch_func: Callable, inputs: List[torch.Tensor], 
                       eps: float = 1e-6) -> Tuple[bool, Dict[str, float]]:
        """
        Check gradients of PyTorch function using finite differences.
        
        Args:
            torch_func: PyTorch function to check
            inputs: List of input tensors
            eps: Step size for finite differences
            
        Returns:
            Tuple containing:
                - True if gradients match, False otherwise
                - Dictionary with gradient statistics
        """
        # TODO: Clone inputs, set requires_grad=True, perform forward and backward passes,
        # calculate numerical gradients, and compare.
        raise NotImplementedError("check_gradients not implemented")
    
    def create_tensor_test_case(self, log_path_prefix: str, 
                              torch_func: Callable, numpy_func: Callable) -> Callable:
        """
        Create a test case for comparing PyTorch and NumPy implementations.
        
        Args:
            log_path_prefix: Path prefix for log files
            torch_func: PyTorch function to test
            numpy_func: NumPy function to compare against
            
        Returns:
            Test function that can be called directly
        """
        def test_case():
            # TODO: Generate test inputs, run both functions, and compare outputs.
            raise NotImplementedError("test_case not implemented")
        
        return test_case
</file>
<file path="./eryx/autotest/configuration.py" project="ptycho">
import os

class Configuration:
    def __init__(self, debug: bool = False, log_file_prefix: str = "logs"):
        self.debug = debug
        self.log_file_prefix = log_file_prefix

    def getDebugFlag(self) -> bool:
        return self.debug

    def getLogFilePrefix(self) -> str:
        return self.log_file_prefix

</file>
<file path="./eryx/autotest/serializer.py" project="ptycho">
# spec
#module DebuggingSystem {
#
#    interface Serializer {
#        """
#        Serializes Python objects to a binary format using pickle.
#
#        Preconditions:
#        - `input_data` must be a picklable Python object.
#
#        Postconditions:
#        - Returns the serialized binary data of the input object.
#        - Raises ValueError if the input data is not picklable.
#        """
#        bytes serialize(Any input_data);
#
#        """
#        Deserializes Python objects from a binary format using pickle.
#
#        Preconditions:
#        - `serialized_data` must be a valid pickle-serialized binary string.
#
#        Postconditions:
#        - Returns the deserialized Python object.
#        - Raises ValueError if the binary data could not be deserialized.
#        """
#        Any deserialize(bytes serialized_data);
#    };

import doctest
import pickle
from typing import Any, List

class Serializer:
    def serialize(self, input_data: Any) -> bytes:
        """
        Serializes Python objects to a binary format using pickle.

        Preconditions:
        - `input_data` must be a picklable Python object.

        Postconditions:
        - Returns the serialized binary data of the input object.
        - Raises ValueError if the input data is not picklable.

        >>> s = Serializer()
        >>> data = {'key': 'value'}
        >>> serialized_data = s.serialize(data)
        >>> type(serialized_data)
        <class 'bytes'>
        >>> deserialized_data = s.deserialize(serialized_data)
        >>> deserialized_data == data
        True
        >>> s.serialize(lambda x: x)  # doctest: +IGNORE_EXCEPTION_DETAIL
        Traceback (most recent call last):
        ValueError: Input data is not picklable
        >>> s.deserialize(b'not a pickle')  # doctest: +IGNORE_EXCEPTION_DETAIL
        Traceback (most recent call last):
        ValueError: Could not deserialize the binary data
        """
        try:
            return pickle.dumps(input_data)
        except (pickle.PicklingError, AttributeError, TypeError):
            raise ValueError("Input data is not picklable")

    def deserialize(self, serialized_data: bytes) -> Any:
        """
        Deserializes Python objects from a binary format using pickle.

        Preconditions:
        - `serialized_data` must be a valid pickle-serialized binary string.

        Postconditions:
        - Returns the deserialized Python object.
        - Raises ValueError if the binary data could not be deserialized.

        >>> s = Serializer()
        >>> data = {'key': 'value'}
        >>> serialized_data = s.serialize(data)
        >>> deserialized_data = s.deserialize(serialized_data)
        >>> deserialized_data == data
        True
        >>> s.deserialize(b'not a pickle')  # doctest: +IGNORE_EXCEPTION_DETAIL
        Traceback (most recent call last):
        ValueError: Could not deserialize the binary data
        """
        try:
            return pickle.loads(serialized_data)
        except (pickle.UnpicklingError, EOFError, AttributeError, ImportError, IndexError):
            raise ValueError("Could not deserialize the binary data")
doctest.testmod(verbose=True)

</file>
<file path="./eryx/autotest/logger.py" project="ptycho">
from .serializer import Serializer
# spec
#    @depends_on(Serializer)
#    interface Logger {
#        """
#        Logs function call details to a specified log file.
#
#        Preconditions:
#        - `args` and `kwargs` are serialized using pickle.
#        - `log_file_path` must be a valid file path with write permissions.
#          The directory containing the file must exist.
#
#        Postconditions:
#        - The serialized function arguments and keyword arguments are written to the log file.
#          The log entry is formatted as a single line JSON string.
#        - If there is an error during logging, an error message is printed to stderr.
#        """
#        void logCall(bytes args, bytes kwargs, string log_file_path);
#
#        """
#        Logs function return details to the specified log file.
#
#        Preconditions:
#        - `result` is serialized using pickle.
#        - `log_file_path` must be a valid file path with write permissions.
#          The directory containing the file must exist.
#
#        Postconditions:
#        - The serialized `result` and `execution_time` are appended to the log file.
#          The log entry is formatted as a single line JSON string.
#        - If there is an error during logging, an error message is printed to stderr.
#        """
#        void logReturn(bytes result, float execution_time, string log_file_path);
#
#        """
#        Logs an error message to the specified log file.
#
#        Preconditions:
#        - `log_file_path` must be a valid file path with write permissions.
#          The directory containing the file must exist.
#
#        Postconditions:
#        - The `error` message is written to the log file.
#          The log entry is formatted as a single line JSON string.
#        - If there is an error during logging, an error message is printed to stderr.
#        """
#        void logError(string error, string log_file_path);
#
#        """
#        Loads a logged dataset from a log file.
#
#        Preconditions:
#        - `log_file_path` must be a valid file path with read permissions.
#          The file must contain valid JSON-formatted log entries.
#
#        Postconditions:
#        - Returns a list or tuple containing the logged inputs and output.
#        - If there is an error during loading, returns an empty list or tuple.
#        """
#        Union[list, tuple] loadLog(Configuration configuration);
#
#        """
#        Searches the log directory and returns all valid log file paths.
#
#        Preconditions:
#        - `log_directory` must be a valid directory path with read permissions.
#
#        Postconditions:
#        - Returns a list of valid log file paths adhering to the format ^(?P<log_path_prefix>[a-z0-9]+)/(?P<python_namespace_path>([a-z0-9]+\.)+)log$.?
#        - Invalid log file paths are filtered out using the validateLogFilePath method.
#        - If there are no valid log files or an error occurs during searching, returns an empty list.
#        """
#        list[str] searchLogDirectory(string log_directory);
#
#        """
#        Validates a log file path against the expected format.
#
#        Preconditions:
#        - `log_file_path` must be a string representing a file path.
#
#        Postconditions:
#        - Returns True if the `log_file_path` adheres to the format '^(?P<log_path_prefix>[a-z0-9]+)/(?P<python_namespace_path>([a-z0-9]+\.)+)log$.', False otherwise.
#        """
#        bool validateLogFilePath(string log_file_path);
#    };

import json
import os
import sys
import pickle
from typing import Any, Union, List
import re

class Logger:
    def __init__(self):
        self.serializer = Serializer()

    def logCall(self, args: bytes, kwargs: bytes, log_file_path: str) -> None:
        try:
            with open(log_file_path, 'a') as log_file:
                log_entry = json.dumps({
                    "args": args.hex(),
                    "kwargs": kwargs.hex()
                })
                log_file.write(log_entry + "\n")
        except Exception as e:
            print(f"Error logging function call: {e}", file=sys.stderr)

    def logReturn(self, result: bytes, execution_time: float, log_file_path: str) -> None:
        try:
            with open(log_file_path, 'a') as log_file:
                log_entry = json.dumps({
                    "result": result.hex(),
                    "execution_time": execution_time
                })
                log_file.write(log_entry + "\n")
        except Exception as e:
            print(f"Error logging function return: {e}", file=sys.stderr)

    def logError(self, error: str, log_file_path: str) -> None:
        pass
#        try:
#            with open(log_file_path, 'a') as log_file:
#                log_entry = json.dumps({
#                    "error": error
#                })
#                log_file.write(log_entry + "\n")
#        except Exception as e:
#            print(f"Error logging error: {e}", file=sys.stderr)

    def loadLog(self, log_file_path: str) -> Union[List, tuple]:
        logs = []
        try:
            with open(log_file_path, 'r') as log_file:
                for line in log_file:
                    log_entry = json.loads(line)
                    if "args" in log_entry:
                        log_entry["args"] = bytes.fromhex(log_entry["args"])
                    if "kwargs" in log_entry:
                        log_entry["kwargs"] = bytes.fromhex(log_entry["kwargs"])
                    if "result" in log_entry:
                        log_entry["result"] = bytes.fromhex(log_entry["result"])
                    logs.append(log_entry)
        except Exception as e:
            print(f"Error loading log: {e}", file=sys.stderr)
        return logs

    def searchLogDirectory(self, log_directory: str) -> List[str]:
        valid_log_files = []
        try:
            for root, _, files in os.walk(log_directory):
                for file in files:
                    file_path = os.path.relpath(os.path.join(root, file), start=log_directory)
                    if self.validateLogFilePath(file_path):
                        valid_log_files.append(os.path.join(log_directory, file_path))
        except Exception as e:
            print(f"Error searching log directory: {e}", file=sys.stderr)
        return valid_log_files

    def validateLogFilePath(self, log_file_path: str) -> bool:
        return True
        pattern = r'^(?P<log_path_prefix>[a-z0-9]+)/(?P<python_namespace_path>([a-z0-9]+\.)+)log$'
        return re.match(pattern, log_file_path) is not None

import unittest
import tempfile

class TestLogger(unittest.TestCase):
    def setUp(self):
        self.logger = Logger()
        self.test_dir = tempfile.TemporaryDirectory()
        self.test_file = os.path.join(self.test_dir.name, 'test.log')
        
    def tearDown(self):
        self.test_dir.cleanup()

    def test_logCall(self):
        args = self.logger.serializer.serialize(('arg1', 'arg2'))
        kwargs = self.logger.serializer.serialize({'key': 'value'})
        self.logger.logCall(args, kwargs, self.test_file)
        
        with open(self.test_file, 'r') as log_file:
            log_entry = json.loads(log_file.readline())
            self.assertEqual(log_entry["args"], args.hex())
            self.assertEqual(log_entry["kwargs"], kwargs.hex())

    def test_logReturn(self):
        result = self.logger.serializer.serialize('result')
        execution_time = 0.123
        self.logger.logReturn(result, execution_time, self.test_file)
        
        with open(self.test_file, 'r') as log_file:
            log_entry = json.loads(log_file.readline())
            self.assertEqual(log_entry["result"], result.hex())
            self.assertEqual(log_entry["execution_time"], execution_time)

    def test_logError(self):
        error = "Test error message"
        self.logger.logError(error, self.test_file)
        
        with open(self.test_file, 'r') as log_file:
            log_entry = json.loads(log_file.readline())
            self.assertEqual(log_entry["error"], error)

    def test_loadLog(self):
        args = self.logger.serializer.serialize(('arg1', 'arg2'))
        kwargs = self.logger.serializer.serialize({'key': 'value'})
        result = self.logger.serializer.serialize('result')
        execution_time = 0.123
        
        self.logger.logCall(args, kwargs, self.test_file)
        self.logger.logReturn(result, execution_time, self.test_file)
        
        logs = self.logger.loadLog(self.test_file)
        self.assertEqual(len(logs), 2)
        self.assertEqual(logs[0]["args"], args)
        self.assertEqual(logs[0]["kwargs"], kwargs)
        self.assertEqual(logs[1]["result"], result)
        self.assertEqual(logs[1]["execution_time"], execution_time)

    def test_searchLogDirectory(self):
        valid_file = os.path.join(self.test_dir.name, 'logs/module.samplefunc.log')
        invalid_file = os.path.join(self.test_dir.name, 'invalid.log')
        
        os.makedirs(os.path.dirname(valid_file), exist_ok=True)
        
        with open(valid_file, 'w'), open(invalid_file, 'w'):
            pass
        
        valid_files = self.logger.searchLogDirectory(self.test_dir.name)
        self.assertIn(valid_file, valid_files)
        self.assertNotIn(invalid_file, valid_files)

    def test_validateLogFilePath(self):
        valid_path = 'logs/module.samplefunc.log'
        invalid_path = 'invalid.log'
        
        self.assertTrue(self.logger.validateLogFilePath(valid_path))
        self.assertFalse(self.logger.validateLogFilePath(invalid_path))

if __name__ == '__main__':
    unittest.main(argv=[''], verbosity=2, exit=False)
</file>
<file path="./eryx/autotest/debug.py" project="ptycho">
import os
import time
import pickle
import json
from typing import Callable, Any, List, Union, Optional
import re

# spec
#    @depends_on(Logger, Configuration, FunctionMapping)
#    interface Debug {
#        """
#        Applies the debugging process to the function.
#
#        Preconditions:
#        - `func` must be a callable.
#        - Configuration must allow debugging.
#
#        Postconditions:
#        - If debugging is allowed by the Configuration:
#          - Returns a new function that wraps the original function with debugging functionality.
#          - The returned function, when called, performs two forms of logging:
#            1. Prints function call and return information to the console, surrounded by XML tags
#               containing the callable's module path and name. The console log messages are in the
#               format `<module.function>CALL/RETURN args/result</module.function>`. For all array
#               or tensor types (i.e., objects with a .shape and/or .dtype attribute), the shapes
#               and data types are also printed.
#            2. Serializes function inputs and outputs to a log file using the `logCall` and `logReturn`
#               methods of the Logger interface. The serialized data can be loaded using the `LoadLog`
#               method. If serialization fails, the console logging still occurs, but no log file is
#               generated for that invocation.
#          - Logs only the first two invocations of the function.
#        - If debugging is not allowed by the Configuration:
#          - Returns the original function unchanged, without any debugging functionality.
#        """
#        Callable decorate(Callable func);
#    };

## implementation
import time
import os
import pickle
import json
from typing import Callable, Any, List, Union, Optional
import re
from .configuration import Configuration
from .serializer import Serializer
from .logger import Logger
from .functionmapping import FunctionMapping

def make_invocation_counter():
    count = 0
    def increment():
        nonlocal count
        count += 1
        return count
    return increment

class Debug:
    def __init__(self):
        self.configuration = Configuration()
        self.serializer = Serializer()
        self.logger = Logger()
        self.function_mapping = FunctionMapping()

    def decorate(self, func: Callable) -> Callable:
        increment_count = make_invocation_counter()
        if not self.configuration.getDebugFlag():
            return func

        else:
            module_path = self.function_mapping.get_module_path(func)
            function_name = func.__name__

            def wrapper(*args: Any, **kwargs: Any) -> Any:
                invocation_count = increment_count()
                if invocation_count > 2:
                    return func(*args, **kwargs)
                
                log_file_path = self.function_mapping.get_log_file_path(func)
                os.makedirs(os.path.dirname(log_file_path), exist_ok=True)

                try:
                    serialized_args = self.serializer.serialize(args)
                    serialized_kwargs = self.serializer.serialize(kwargs)
                    self.logger.logCall(serialized_args, serialized_kwargs, log_file_path)
                except ValueError:
                    pass  # If serialization fails, just proceed with console logging

                console_log_start = f"<{module_path}.{function_name}>CALL"
                console_log_args = self._formatConsoleLog(args)
                console_log_kwargs = self._formatConsoleLog(kwargs)
                print(console_log_start)
                print(console_log_args)
                print(console_log_kwargs)

                start_time = time.time()

                result = func(*args, **kwargs)
                try:
                    serialized_result = self.serializer.serialize(result)
                    self.logger.logReturn(serialized_result, time.time() - start_time, log_file_path)

                    console_log_end = f"</{module_path}.{function_name}>RETURN"
                    console_log_result = self._formatConsoleLog(result)
                    print(console_log_end + " " + console_log_result)

                except Exception as e:
                    self.logger.logError(str(e), log_file_path)
                    print(f"<{module_path}.{function_name}>ERROR {str(e)}")
                return result

            return wrapper

    def _formatConsoleLog(self, data: Any) -> str:
        if not isinstance(data, tuple):
            data = (data,)

        formatted_data = []
        for item in data:
            if hasattr(item, 'shape') and hasattr(item, 'dtype'):
                formatted_data.append(f"type={type(item)}, shape={item.shape}, dtype={item.dtype}")
            elif isinstance(item, (int, float, str, bool)):
                formatted_data.append(f"type={type(item)}, {item}")
            else:
                formatted_data.append(f"type={type(item)}")
        return ", ".join(formatted_data)

if __name__ == "__main__":
    import doctest
    doctest.testmod(verbose=True)

import unittest

class TestDebug(unittest.TestCase):
    def setUp(self):
        self.configuration = Configuration()
        self.serializer = Serializer()
        self.logger = Logger()
        self.function_mapping = FunctionMapping()
        self.debug = Debug(self.configuration, self.serializer, self.logger, self.function_mapping)

    def test_decorate_call(self):
        @self.debug.decorate
        def add(x, y):
            return x + y

        result = add(3, 4)
        self.assertEqual(result, 7)

    def test_decorate_return(self):
        @self.debug.decorate
        def multiply(x, y):
            return x * y

        result = multiply(2, 3)
        self.assertEqual(result, 6)
        result = multiply(4, 5)
        self.assertEqual(result, 20)
        result = multiply(6, 7)  # This call should not be logged
        self.assertEqual(result, 42)

    def test_decorate_error(self):
        @self.debug.decorate
        def divide(x, y):
            return x / y

        with self.assertRaises(ZeroDivisionError):
            divide(1, 0)

#    def test_format_console_log(self):
#        data = (3, "hello")
#        formatted_log = self.debug._formatConsoleLog(data)
#        self.assertEqual(formatted_log, "3, hello")

# Import the global configuration
from eryx.autotest_config import config

# Create a global instance of Debug
def _create_debug_decorator():
    debug_obj = Debug()
    # Use the configuration from autotest_config
    debug_obj.configuration = config
    return debug_obj.decorate

# Create the decorator based on the configuration
if config.getDebugFlag():
    debug = _create_debug_decorator()
else:
    # Provide a no-op decorator when debugging is disabled
    def debug(func):
        return func

if __name__ == '__main__':
    import unittest
    unittest.main(argv=[''], verbosity=2, exit=False)

</file>
<file path="./eryx/autotest/torch_testing.py" project="ptycho">
"""
Testing utilities for PyTorch implementation.

This module extends the autotest framework with PyTorch-specific testing
capabilities, including tensor comparison, gradient checking, and
PyTorch-NumPy conversion for testing.
"""

import numpy as np
import torch
from typing import Any, Callable, Dict, List, Optional, Tuple, Union
from .testing import Testing
from .logger import Logger
from .functionmapping import FunctionMapping

class TorchTesting(Testing):
    """
    Extended testing framework for PyTorch implementations.
    
    This class extends the Testing class with PyTorch-specific utilities,
    including tensor comparison and gradient checking.
    """
    
    def __init__(self, logger: Logger, function_mapping: FunctionMapping, 
                rtol: float = 1e-5, atol: float = 1e-8):
        """
        Initialize the PyTorch testing framework.
        
        Args:
            logger: Logger instance for test logging
            function_mapping: FunctionMapping instance for function lookup
            rtol: Relative tolerance for tensor comparison
            atol: Absolute tolerance for tensor comparison
        """
        super().__init__(logger, function_mapping)
        self.rtol = rtol
        self.atol = atol
    
    def testTorchCallable(self, log_path_prefix: str, torch_func: Callable) -> bool:
        """
        Test a PyTorch function against NumPy implementation.
        
        Args:
            log_path_prefix: Path prefix for log files
            torch_func: PyTorch function to test
            
        Returns:
            True if test passes, False otherwise
        """
        log_files = self.logger.searchLogDirectory(log_path_prefix)
        for log_file in log_files:
            logs = self.logger.loadLog(log_file)
            for i in range(len(logs) // 2):
                args = logs[2 * i]['args']
                kwargs = logs[2 * i]['kwargs']
                expected_output = logs[2 * i + 1]['result']
                try:
                    # Deserialize and convert to PyTorch
                    deserialized_args = self._numpy_to_torch(self.logger.serializer.deserialize(args))
                    deserialized_kwargs = self._numpy_to_torch(self.logger.serializer.deserialize(kwargs))
                    deserialized_expected_output = self.logger.serializer.deserialize(expected_output)
                    
                    # Run PyTorch function
                    actual_output = torch_func(*deserialized_args, **deserialized_kwargs)
                    
                    # Convert PyTorch output to NumPy for comparison
                    numpy_actual_output = self._torch_to_numpy(actual_output)
                    
                    # Compare outputs
                    if not self._compare_outputs(numpy_actual_output, deserialized_expected_output):
                        print(f"Test failed for {log_file}")
                        return False
                except Exception as e:
                    print(f"Error testing PyTorch function: {e}")
                    return False
        return True
    
    def _numpy_to_torch(self, obj: Any) -> Any:
        """
        Convert NumPy arrays to PyTorch tensors recursively.
        
        Args:
            obj: Input object potentially containing NumPy arrays
            
        Returns:
            Object with NumPy arrays converted to PyTorch tensors
        """
        if isinstance(obj, np.ndarray):
            return torch.from_numpy(obj.copy())
        elif isinstance(obj, list):
            return [self._numpy_to_torch(item) for item in obj]
        elif isinstance(obj, tuple):
            return tuple(self._numpy_to_torch(item) for item in obj)
        elif isinstance(obj, dict):
            return {k: self._numpy_to_torch(v) for k, v in obj.items()}
        else:
            return obj
    
    def _torch_to_numpy(self, obj: Any) -> Any:
        """
        Convert PyTorch tensors to NumPy arrays recursively.
        
        Args:
            obj: Input object potentially containing PyTorch tensors
            
        Returns:
            Object with PyTorch tensors converted to NumPy arrays
        """
        if isinstance(obj, torch.Tensor):
            return obj.detach().cpu().numpy()
        elif isinstance(obj, list):
            return [self._torch_to_numpy(item) for item in obj]
        elif isinstance(obj, tuple):
            return tuple(self._torch_to_numpy(item) for item in obj)
        elif isinstance(obj, dict):
            return {k: self._torch_to_numpy(v) for k, v in obj.items()}
        else:
            return obj
    
    def _compare_outputs(self, actual: Any, expected: Any) -> bool:
        """
        Compare outputs with tolerance for numerical differences.
        
        Args:
            actual: Actual output
            expected: Expected output
            
        Returns:
            True if outputs match within tolerance, False otherwise
        """
        if isinstance(actual, np.ndarray) and isinstance(expected, np.ndarray):
            # Compare arrays with tolerance
            if actual.shape != expected.shape:
                print(f"Shape mismatch: {actual.shape} vs {expected.shape}")
                return False
            
            # Handle NaN values
            nan_mask_actual = np.isnan(actual)
            nan_mask_expected = np.isnan(expected)
            if not np.array_equal(nan_mask_actual, nan_mask_expected):
                print("NaN pattern mismatch")
                return False
            
            # Compare non-NaN values
            non_nan_mask = ~nan_mask_actual
            if np.any(non_nan_mask):
                return np.allclose(actual[non_nan_mask], expected[non_nan_mask], 
                                 rtol=self.rtol, atol=self.atol)
            return True
        
        elif isinstance(actual, list) and isinstance(expected, list):
            if len(actual) != len(expected):
                print(f"Length mismatch: {len(actual)} vs {len(expected)}")
                return False
            return all(self._compare_outputs(a, e) for a, e in zip(actual, expected))
        
        elif isinstance(actual, tuple) and isinstance(expected, tuple):
            if len(actual) != len(expected):
                print(f"Length mismatch: {len(actual)} vs {len(expected)}")
                return False
            return all(self._compare_outputs(a, e) for a, e in zip(actual, expected))
        
        elif isinstance(actual, dict) and isinstance(expected, dict):
            if set(actual.keys()) != set(expected.keys()):
                print(f"Key mismatch: {set(actual.keys())} vs {set(expected.keys())}")
                return False
            return all(self._compare_outputs(actual[k], expected[k]) for k in actual)
        
        else:
            # Direct comparison for other types
            return actual == expected
    
    def check_gradients(self, torch_func: Callable, inputs: List[torch.Tensor], 
                       eps: float = 1e-6) -> Tuple[bool, Dict[str, float]]:
        """
        Check gradients of PyTorch function using finite differences.
        
        Args:
            torch_func: PyTorch function to check
            inputs: List of input tensors
            eps: Step size for finite differences
            
        Returns:
            Tuple containing:
                - True if gradients match, False otherwise
                - Dictionary with gradient statistics
        """
        # Clone inputs and set requires_grad=True
        grad_inputs = []
        for inp in inputs:
            grad_input = inp.clone().detach()
            grad_input.requires_grad_(True)
            grad_inputs.append(grad_input)
        
        # Compute forward pass
        output = torch_func(*grad_inputs)
        
        # If output is not a scalar, sum it to get a scalar
        if output.numel() > 1:
            output = output.sum()
        
        # Compute backward pass
        output.backward()
        
        # Get analytical gradients
        analytical_grads = [inp.grad.clone() for inp in grad_inputs]
        
        # Compute numerical gradients with finite differences
        numerical_grads = []
        for i, inp in enumerate(grad_inputs):
            numerical_grad = torch.zeros_like(inp)
            
            # Flatten the input for easier iteration
            flat_input = inp.flatten()
            flat_grad = numerical_grad.flatten()
            
            # Compute gradient for each element
            for j in range(flat_input.numel()):
                # Forward difference
                flat_input[j] += eps
                forward_output = torch_func(*grad_inputs).sum().item()
                
                # Backward difference
                flat_input[j] -= 2 * eps
                backward_output = torch_func(*grad_inputs).sum().item()
                
                # Central difference
                flat_grad[j] = (forward_output - backward_output) / (2 * eps)
                
                # Restore original value
                flat_input[j] += eps
            
            numerical_grads.append(numerical_grad)
        
        # Compare analytical and numerical gradients
        stats = {}
        all_match = True
        
        for i, (analytical, numerical) in enumerate(zip(analytical_grads, numerical_grads)):
            # Compute relative error
            abs_diff = torch.abs(analytical - numerical)
            abs_analytical = torch.abs(analytical)
            abs_numerical = torch.abs(numerical)
            
            # Avoid division by zero
            max_abs = torch.max(abs_analytical, abs_numerical)
            max_abs = torch.where(max_abs > 0, max_abs, torch.ones_like(max_abs))
            
            rel_error = abs_diff / max_abs
            
            # Compute statistics
            max_rel_error = rel_error.max().item()
            mean_rel_error = rel_error.mean().item()
            
            # Check if gradients match within tolerance
            match = (rel_error < self.rtol).all().item()
            all_match = all_match and match
            
            # Store statistics
            stats[f"input_{i}_max_rel_error"] = max_rel_error
            stats[f"input_{i}_mean_rel_error"] = mean_rel_error
            stats[f"input_{i}_match"] = match
        
        stats["all_match"] = all_match
        
        return all_match, stats
    
    def create_tensor_test_case(self, log_path_prefix: str, 
                              torch_func: Callable, numpy_func: Callable) -> Callable:
        """
        Create a test case for comparing PyTorch and NumPy implementations.
        
        Args:
            log_path_prefix: Path prefix for log files
            torch_func: PyTorch function to test
            numpy_func: NumPy function to compare against
            
        Returns:
            Test function that can be called directly
        """
        def test_case():
            # Find log files
            log_files = self.logger.searchLogDirectory(log_path_prefix)
            if not log_files:
                print(f"No log files found for {log_path_prefix}")
                return False
            
            # Process each log file
            for log_file in log_files:
                logs = self.logger.loadLog(log_file)
                for i in range(len(logs) // 2):
                    # Get inputs and expected output
                    args_bytes = logs[2 * i]['args']
                    kwargs_bytes = logs[2 * i]['kwargs']
                    
                    # Deserialize inputs
                    args = self.logger.serializer.deserialize(args_bytes)
                    kwargs = self.logger.serializer.deserialize(kwargs_bytes)
                    
                    # Run NumPy function to get expected output
                    expected_output = numpy_func(*args, **kwargs)
                    
                    # Convert inputs to PyTorch tensors
                    torch_args = self._numpy_to_torch(args)
                    torch_kwargs = self._numpy_to_torch(kwargs)
                    
                    # Run PyTorch function
                    torch_output = torch_func(*torch_args, **torch_kwargs)
                    
                    # Convert PyTorch output to NumPy
                    numpy_output = self._torch_to_numpy(torch_output)
                    
                    # Compare outputs
                    if not self._compare_outputs(numpy_output, expected_output):
                        print(f"Test failed for {log_file}")
                        return False
            
            return True
        
        return test_case
</file>
<file path="./eryx/autotest/__init__.py" project="ptycho">
</file>
<file path="./eryx/pdb.py" project="ptycho">
import numpy as np
import gemmi
from scipy.spatial import KDTree
import os

# Import debug conditionally to avoid circular imports
if os.environ.get("DEBUG_MODE") == "1":
    from eryx.autotest.debug import debug
else:
    # Provide a no-op decorator when debugging is disabled
    def debug(func):
        return func

@debug
def sym_str_as_matrix(sym_str):
    """
    Comvert a symmetry operation from string to matrix format,
    only retaining the rotational component.
    
    Parameters
    ----------
    sym_str : str
        symmetry operation in string format, e.g. '-Y,X-Y,Z+1/3'
    
    Returns
    -------
    sym_matrix : numpy.ndarray, shape (3,3)
        rotation portion of symmetry operation in matrix format
    """
    sym_matrix = np.zeros((3,3))
    for i,item in enumerate(sym_str.split(",")):
        if '-X' in item:
            sym_matrix[i][0] = -1
        if 'X' in item and '-X' not in item:
            sym_matrix[i][0] = 1
        if 'Y' in item and '-Y' not in item:
            sym_matrix[i][1] = 1
        if '-Y' in item:
            sym_matrix[i][1] = -1
        if 'Z' in item and '-Z' not in item:
            sym_matrix[i][2] = 1
        if '-Z' in item:
            sym_matrix[i][2] = -1           
    return sym_matrix

def extract_sym_ops(pdb_file):
    """
    Extract the rotational component of the symmetry operations from 
    the PDB header (REMARK 290).

    Parameters
    ----------
    pdb_file : str
        path to coordinates file
    
    Returns
    -------
    sym_ops : dict
        dictionary of rotational component of symmetry operations
    """
    sym_ops = {}
    counter = 0
    with open(pdb_file, "r") as f:
        for line in f:
            if "REMARK 290" in line:
                if "555" in line:
                    sym_ops[counter] = sym_str_as_matrix(line.split()[-1])
                    counter += 1
            if "REMARK 300" in line:
                break
                
    return sym_ops

def extract_transformations(pdb_file):
    """
    Extract the transformations from the PDB header (REMARK 290).
    The rotational component of the matrices contain information
    about cell orthogonalization, import for non-orthogonal cells.
    The translations are in Angstroms rather than fractional cells.

    Parameters
    ----------
    pdb_file : str
        path to coordinates file
    
    Returns
    -------
    transformations : dict
        dictionary of symmetry operations as 3x4 matrices
    """
    transformations = {}
    with open(pdb_file, "r") as f:
        for line in f:
            if "REMARK 290" in line:
                if "SMTRY" in line:
                    sym_info = line.split()[3:]
                    op_num = int(sym_info[0]) - 1
                    op_line = np.array(sym_info[1:]).astype(float)
                    if op_num not in transformations:
                        transformations[op_num] = op_line
                    else:
                        transformations[op_num] = np.vstack((transformations[op_num], op_line))
            if "REMARK 300" in line:
                break
                
    return transformations

def get_unit_cell_axes(cell):
    """
    Compute axes for the unit cell.
    
    Parameters
    ----------
    cell : numpy.ndarray, shape (6,)
        unit cell parameters in Angstrom / degrees
        
    Returns
    -------
    axes : numpy.ndarray, shape (3,3)
        matrix of unit cell axes
    """
    a,b,c = cell[:3]
    alpha, beta, gamma = np.deg2rad(cell[3:])
    factor_21 = (np.cos(alpha) - np.cos(beta) * np.cos(gamma))/np.sin(gamma)
    factor_22 = np.sqrt(1 - np.square(np.cos(beta)) - np.square(factor_21))
    axes = np.array([[a, 0, 0],
                     [b * np.cos(gamma), b * np.sin(gamma), 0],
                     [c * np.cos(beta), c * factor_21, c*factor_22]])
    return axes

class AtomicModel:
    
    def __init__(self, pdb_file, expand_p1=False, frame=0, clean_pdb=True):
        self._get_gemmi_structure(pdb_file, clean_pdb)
        self._extract_cell()
        self.sym_ops, self.transformations = self._get_sym_ops(pdb_file)
        self.extract_frame(frame=frame, expand_p1=expand_p1)

    def _get_gemmi_structure(self, pdb_file, clean_pdb):
        """
        Retrieve Gemmi structure from PDB file.
        Optionally clean up water molecules, hydrogen, ...
        """
        self.structure = gemmi.read_pdb(pdb_file)
        if clean_pdb:
            self.structure.remove_alternative_conformations()
            self.structure.remove_hydrogens()
            self.structure.remove_waters()
            try:
                self.structure.remove_ligands_and_waters()
            except RuntimeError as e:
                print(e)
            self.structure.remove_empty_chains()

    def _extract_cell(self):
        """
        Extract unit cell information.
        """
        self.cell = np.array(self.structure.cell.parameters)
        self.A_inv = np.array(self.structure.cell.fractionalization_matrix)
        self.space_group = self.structure.spacegroup_hm
        self.unit_cell_axes = get_unit_cell_axes(self.cell)
        
    def _get_sym_ops(self, pdb_file):
        """
        Extract symmetry operations, preferably from the PDB
        header or alternatively from Gemmi. Since the latter
        has rotation operators that are unity, this may not 
        work correctly for non-orthongal cells.
    
        Parameters
        ----------
        pdb_file : str
            path to coordinates file in PDB format
        """
        sym_ops = extract_sym_ops(pdb_file)
        transformations = extract_transformations(pdb_file)
        
        if len(sym_ops) == 0:
            print(""""Warning: gathering symmetry operations from
            Gemmi rather than the PDB header. This may be incorrect
            for non-orthogonal unit cells.""")
            sym_ops = {}
            sg = gemmi.SpaceGroup(self.space_group) 
            for i,op in enumerate(sg.operations()):
                r = np.array(op.rot) / op.DEN 
                t = np.array(op.tran) / op.DEN
                sym_ops[i] = np.hstack((r, t[:,np.newaxis]))

        if len(transformations) == 0:
            transformations = sym_ops

        self.n_asu = len(transformations)
        return sym_ops, transformations
    
    def extract_frame(self, expand_p1=False, frame=0):
        """
        Extract the form factors and atomic coordinates for the
        given frame/model in the PDB, or if frame==-1, then for 
        all frames/models in the PDB. 
        
        Parameters
        ----------
        expand_p1 : bool
            if True, expand to all asymmetric units
        frame : int
            pdb frame to extract; if -1, extract all frames
        """
        self.ff_a, self.ff_b, self.ff_c = None, None, None
        self.adp = None
        self.elements = []
        self.xyz = None

        if frame == -1:
            frange = range(len(self.structure))
        else:
            frange = [frame]
        self.n_conf = len(frange)
            
        for fr in frange:
            model = self.structure[fr]
            residues = [res for ch in model for res in ch]
            self._extract_xyz(residues, expand_p1)
            if not expand_p1:
                self._extract_adp(residues, expand_p1)
                self._extract_ff_coefs(residues, expand_p1)
        if expand_p1:
            self._extract_adp(residues, expand_p1)
            self._extract_ff_coefs(residues, expand_p1)
        
    @debug
    def _get_xyz_asus(self, xyz):
        """
        Apply symmetry operations to get xyz coordinates of all 
        asymmetric units and pack them into the unit cell.
        
        Parameters
        ----------
        xyz : numpy.ndarray, shape (n_atoms, 3)
            atomic coordinates for a single asymmetric unit
            
        Returns
        -------
        xyz_asus : numpy.ndarray, shape (n_asu, n_atoms, 3)
            atomic coordinates for all asus in the unit cell
        """
        xyz_asus = []      
        for i,op in self.transformations.items():
            rot, trans = op[:3,:3], op[:,3]
            xyz_asu = np.inner(rot, xyz).T + trans
            com = np.mean(xyz_asu.T, axis=1)
            shift = np.sum(np.dot(self.unit_cell_axes.T, self.sym_ops[i]), axis=1)
            xyz_asu += np.abs(shift) * np.array(com < 0).astype(int)
            xyz_asu -= np.abs(shift) * np.array(com > self.cell[:3]).astype(int)
            xyz_asus.append(xyz_asu)
        
        return np.array(xyz_asus)

    def _extract_xyz(self, residues, expand_p1=False):
        """
        Extract atomic coordinates, optionally getting coordinates
        for all asymmetric units in the unit cell.
        
        Parameters
        ----------
        residues : list of gemmi.Residue objects
            residue objects containing atomic position information
        expand_p1 : bool
            if True, retrieve coordinates for all asus
            
        Returns
        -------
        xyz : numpy.ndarray, shape (n_atoms, 3) or (n_asu, n_atoms, 3)
            atomic coordinates in Angstroms
        """
        xyz = np.array([[atom.pos.tolist() for res in residues for atom in res]])
        if expand_p1:
            xyz = self._get_xyz_asus(xyz[0])
        
        if self.xyz is None:
            self.xyz = xyz
        else:
            self.xyz = np.vstack((self.xyz, xyz))

    def _extract_adp(self, residues, expand_p1):
        """
        Retrieve atomic displacement parameters.

        Parameters
        ----------
        residues : list of gemmi.Residue objects
            residue objects containing atomic position information
        expand_p1 : bool
            if True, expand to all asymmetric units
        """
        adp = np.array([[atom.b_iso for res in residues for atom in res]])

        if self.adp is None:
            self.adp = adp
        else:
            self.adp = np.vstack((self.adp, adp))

        if expand_p1:
            n_asu = self.xyz.shape[0]
            self.adp = np.tile(self.adp, (n_asu, 1))
    
    def _extract_ff_coefs(self, residues, expand_p1):
        """
        Retrieve atomic form factor coefficients, specifically
        a, b, and c of the following equation:
        f(qo) = sum(a_i * exp(-b_i * (qo/(4*pi))^2)) + c
        and the sum is over the element's coefficients. Also 
        store the gemmi element objects, an alternative way of
        computing the atomic form factors.
        
        Parameters
        ----------
        residues : list of gemmi.Residue objects
            residue objects containing atomic position information
        expand_p1 : bool
            if True, expand to all asymmetric units
        """
        ff_a = np.array([[atom.element.it92.a for res in residues for atom in res]])
        ff_b = np.array([[atom.element.it92.b for res in residues for atom in res]])
        ff_c = np.array([[atom.element.it92.c for res in residues for atom in res]])
        elements = [atom.element for res in residues for atom in res]
        
        if self.ff_a is None:
            self.ff_a, self.ff_b, self.ff_c = ff_a, ff_b, ff_c
        else:
            self.ff_a = np.vstack((self.ff_a, ff_a))
            self.ff_b = np.vstack((self.ff_b, ff_b))
            self.ff_c = np.vstack((self.ff_c, ff_c))
        self.elements.append(elements)
        
        if expand_p1:
            self._tile_form_factors()
    
    @debug
    def flatten_model(self):
        """
        Set self variables to correspond to the given frame,
        for instance flattening the atomic coordinates from 
        (n_frames, n_atoms, 3) to (n_atoms, 3). 
        """
        n_asu = self.xyz.shape[0]
        self.xyz = self.xyz.reshape(-1, self.xyz.shape[-1])
        self.ff_a = self.ff_a.reshape(-1, self.ff_a.shape[-1])
        self.ff_b = self.ff_b.reshape(-1, self.ff_b.shape[-1])
        self.ff_c = self.ff_c.flatten()
        self.elements = [item for sublist in self.elements for item in sublist]
        
    def _tile_form_factors(self):
        """
        Tile the form factor coefficients and gemmi elements
        to match the number of asymmetric units.
        """
        n_asu = self.xyz.shape[0]
        self.ff_a = np.tile(self.ff_a, (n_asu, 1, 1))
        self.ff_b = np.tile(self.ff_b, (n_asu, 1, 1))
        self.ff_c = np.tile(self.ff_c, (n_asu, 1))
        self.elements = n_asu * self.elements


class Crystal:

    def __init__(self, atomic_model):
        """
        Parameters
        ----------
        atomic_model : an eryx.models.AtomicModel object
        """
        self.model = atomic_model
        self.supercell_extent()

    def supercell_extent(self, nx=0, ny=0, nz=0):
        """
        Define supercell dimensions. There will be nx cells on
        each +X and -X side of the reference cell, for a total
        of 2*nx + 1 cells along the X dimension. Same logic
        follows for the two other dimensions.

        Parameters
        ----------
        nx : integer
        ny : integer
        nz : integer
        """
        self.nx = nx
        self.ny = ny
        self.nz = nz
        self.xrange = np.arange(-self.nx, self.nx + 1, 1)
        self.yrange = np.arange(-self.ny, self.ny + 1, 1)
        self.zrange = np.arange(-self.nz, self.nz + 1, 1)
        self.n_cell = (2 * nx + 1) * (2 * ny + 1) * (2 * nz + 1)

    def get_unitcell_origin(self, unit_cell=None):
        """
        Convert unit cell indices to spatial location of its origin.

        Parameters
        ----------
        unit_cell : list of 3 integer indices. Default: [0,0,0].
            index of the unit cell along the 3 dimensions.

        Returns
        -------
        origin : numpy.ndarray, shape (3,)
            location of the unit cell origin (in Angstrom)
        """
        if unit_cell is None:
            unit_cell = [0, 0, 0]
        origin = np.zeros(3)
        for i in range(3):
            origin += unit_cell[i] * self.model.unit_cell_axes[i]
        return origin

    def hkl_to_id(self, unit_cell=None):
        """
        Return unit cell index given its indices in the supercell.

        Parameters
        ----------
        unit_cell : list of 3 integer indices. Default: [0,0,0].
            index of the unit cell along the 3 dimensions.

        Returns
        -------
        cell_id : integer
            index of the unit cell in the supercell
        """
        if unit_cell is None:
            unit_cell = [0, 0, 0]
        icell = 0
        for h in self.xrange:
            for k in self.yrange:
                for l in self.zrange:
                    if h == unit_cell[0] and k == unit_cell[1] and l == unit_cell[2]:
                        cell_id = icell
                    icell += 1
        return cell_id

    def id_to_hkl(self, cell_id=0):
        """
        Return unit cell indices in supercell given its index.

        Parameters
        ----------
        cell_id : integer. Default: 0.

        Returns
        -------
        unit_cell : list of 3 integer indices.
            Index of the unit cell along the 3 dimensions.

        """
        icell = 0
        for h in self.xrange:
            for k in self.yrange:
                for l in self.zrange:
                    if icell == cell_id:
                        unit_cell = [h, k, l]
                    icell += 1
        return unit_cell

    @debug
    def get_asu_xyz(self, asu_id=0, unit_cell=None):
        """

        Parameters
        ----------
        asu_id : integer. Default: 0.
            Asymmetric unit index.
        unit_cell : list of 3 integer indices.
            Index of the unit cell along the 3 dimensions.

        Returns
        -------
        xyz : numpy.ndarray, shape (natoms, 3)
            atomic coordinate for this asu in given unit cell.

        """
        if unit_cell is None:
            unit_cell = [0, 0, 0]
        xyz = self.model._get_xyz_asus(self.model.xyz[0])[asu_id]  # get asu
        xyz += self.get_unitcell_origin(unit_cell)  # move to unit cell
        return xyz

class GaussianNetworkModel:
    @debug
    def __init__(self, pdb_path, enm_cutoff, gamma_intra, gamma_inter):
        self._setup_atomic_model(pdb_path)
        self.enm_cutoff = enm_cutoff
        self.gamma_inter = gamma_inter
        self.gamma_intra = gamma_intra
        self._setup_gaussian_network_model()

    @debug
    def _setup_atomic_model(self, pdb_path):
        """
        Build unit cell and its nearest neighbors while
        storing useful dimensions.

        Parameters
        ----------
        pdb_path : str
            path to coordinates file of asymmetric unit
        """
        atomic_model = AtomicModel(pdb_path, expand_p1=True)
        self.crystal = Crystal(atomic_model)
        self.crystal.supercell_extent(nx=1, ny=1, nz=1)
        self.id_cell_ref = self.crystal.hkl_to_id([0,0,0])
        self.n_cell = self.crystal.n_cell
        self.n_asu = self.crystal.model.n_asu
        self.n_atoms_per_asu = self.crystal.get_asu_xyz().shape[0]
        self.n_dof_per_asu_actual = self.n_atoms_per_asu * 3

    @debug
    def _setup_gaussian_network_model(self):
        """
        Build interaction pair list and spring constant.
        """
        self.build_gamma()
        self.build_neighbor_list()

    @debug
    def build_gamma(self):
        """
        The spring constant gamma dictates the interaction strength
        between pairs of atoms. It is defined across the main unit
        cell and its neighbors, for each asymmetric unit, resulting
        in an array of shape (n_cell, n_asu, n_asu).
        The spring constant between atoms belonging to different asus
        can be set to a different value than that for intra-asus atoms.
        """
        self.gamma = np.zeros((self.n_cell, self.n_asu, self.n_asu))
        for i_asu in range(self.n_asu):
            for i_cell in range(self.n_cell):
                for j_asu in range(self.n_asu):
                    self.gamma[i_cell, i_asu, j_asu] = self.gamma_inter
                    if (i_cell == self.id_cell_ref) and (j_asu == i_asu):
                        self.gamma[i_cell, i_asu, j_asu] = self.gamma_intra

    @debug
    def build_neighbor_list(self):
        """
        Returns the list asu_neighbors[i_asu][i_cell][j_asu]
        of atom pairs between the ASU i_asu from the reference cell
        and ASU j_asu from the cell i_cell. The length of the returned
        list is the number of atoms in one ASU.
        For each atom i in the ASU i_asu in the reference cell,
        asu_neighbors[i_asu][i_cell][j_asu][i] returns the indices of
        its neighbors, if any, in the ASU j_asu in cell i_cell.
        """
        self.asu_neighbors = []

        for i_asu in range(self.n_asu):
            self.asu_neighbors.append([])
            kd_tree1 = KDTree(self.crystal.get_asu_xyz(i_asu, self.crystal.id_to_hkl(self.id_cell_ref)))

            for i_cell in range(self.n_cell):
                self.asu_neighbors[i_asu].append([])

                for j_asu in range(self.n_asu):
                    self.asu_neighbors[i_asu][i_cell].append([])
                    kd_tree2 = KDTree(self.crystal.get_asu_xyz(j_asu, self.crystal.id_to_hkl(i_cell)))

                    self.asu_neighbors[i_asu][i_cell][j_asu] = kd_tree1.query_ball_tree(kd_tree2, r=self.enm_cutoff)

    @debug
    def compute_hessian(self):
        """
        For a pair of atoms the Hessian in a GNM is defined as:
        1. i not j and dij =< cutoff: -gamma_ij
        2. i not j and dij > cutoff: 0
        3. i=j: -sum_{j not i} hessian_ij

        Returns
        -------
        hessian: numpy.ndarray,
                 shape (n_asu, n_atoms_per_asu,
                        n_cell, n_asu, n_atoms_per_asu)
                 type 'complex'
            - dimension 0: index ASUs in reference cell
            - dimension 1: index their atoms
            - dimension 2: index neighbor cells
            - dimension 3: index ASUs in neighbor cell
            - dimension 4: index atoms in neighbor ASU
        """
        hessian = np.zeros((self.n_asu, self.n_atoms_per_asu,
                            self.n_cell, self.n_asu, self.n_atoms_per_asu),
                           dtype='complex')
        hessian_diagonal = np.zeros((self.n_asu, self.n_atoms_per_asu),
                                    dtype='complex')

        # off-diagonal
        for i_asu in range(self.n_asu):
            for i_cell in range(self.n_cell):
                for j_asu in range(self.n_asu):
                    for i_at in range(self.n_atoms_per_asu):
                        iat_neighbors = self.asu_neighbors[i_asu][i_cell][j_asu][i_at]
                        if len(iat_neighbors) > 0:
                            hessian[i_asu, i_at, i_cell, j_asu, iat_neighbors] = -self.gamma[i_cell, i_asu, j_asu]
                            hessian_diagonal[i_asu, i_at] -= self.gamma[i_cell, i_asu, j_asu] * len(iat_neighbors)

        # diagonal (also correct for over-counted self term)
        for i_asu in range(self.n_asu):
            for i_at in range(self.n_atoms_per_asu):
                hessian[i_asu, i_at, self.id_cell_ref, i_asu, i_at] = -hessian_diagonal[i_asu, i_at] - self.gamma[
                    self.id_cell_ref, i_asu, i_asu]

        return hessian

    @debug
    def compute_K(self, hessian, kvec=None):
        """
        Noting H(d) the block of the hessian matrix
        corresponding the the d-th reference cell
        whose origin is located at r_d, then:
        K(kvec) = \sum_d H(d) exp(i kvec. r_d)

        Parameters
        ----------
        hessian : numpy.ndarray, see compute_hessian()
        kvec : numpy.ndarray, shape (3,)
            phonon wavevector, default array([0.,0.,0.])

        Returns
        -------
        Kmat : numpy.ndarray,
               shape (n_asu, n_atoms_per_asu,
                      n_asu, n_atoms_per_asu)
               type 'complex'
        """
        if kvec is None:
            kvec = np.zeros(3)
        Kmat = np.copy(hessian[:, :, self.id_cell_ref, :, :])

        for j_cell in range(self.n_cell):
            if j_cell == self.id_cell_ref:
                continue
            r_cell = self.crystal.get_unitcell_origin(self.crystal.id_to_hkl(j_cell))
            phase = np.dot(kvec, r_cell)
            eikr = np.cos(phase) + 1j * np.sin(phase)
            for i_asu in range(self.n_asu):
                for j_asu in range(self.n_asu):
                    Kmat[i_asu, :, j_asu, :] += hessian[i_asu, :, j_cell, j_asu, :] * eikr
        return Kmat

    @debug
    def compute_Kinv(self, hessian, kvec=None, reshape=True):
        """
        Compute the inverse of K(kvec)
        (see compute_K() for the relationship between K and the hessian).

        Parameters
        ----------
        hessian : numpy.ndarray, see compute_hessian()
        kvec : numpy.ndarray, shape (3,)
            phonon wavevector, default array([0.,0.,0.])

        Returns
        -------
        Kinv : numpy.ndarray,
               shape (n_asu, n_atoms_per_asu,
                      n_asu, n_atoms_per_asu)
               type 'complex'
        """
        if kvec is None:
            kvec = np.zeros(3)
        Kmat = self.compute_K(hessian, kvec=kvec)
        Kshape = Kmat.shape
        Kinv = np.linalg.pinv(Kmat.reshape(Kshape[0] * Kshape[1],
                                           Kshape[2] * Kshape[3]))
        if reshape:
            Kinv = Kinv.reshape((Kshape[0], Kshape[1],
                                 Kshape[2], Kshape[3]))
        return Kinv
</file>
<file path="./eryx/stats.py" project="ptycho">
import numpy as np

def compute_cc(arr1, arr2, mask=None):
    """
    Compute the Pearson correlation coefficient between the input arrays.
    Voxels that should be ignored are assumed to have a value of NaN.
    
    Parameters
    ----------
    arr1 : numpy.ndarray, shape (n_samples, n_points)
        input array
    arr2 : numpy.ndarray, shape (n_samples, n_points) or (1, n_points)
        input array to compute CC with
    mask : numpy.ndarray, shape (map_shape) or (n_points,)
        e.g. to select asu/resolution. True values indicate retained grid points
    
    Returns
    -------
    ccs : numpy.ndarray, shape (n_samples)
        correlation coefficient between paired sample arrays, or if
        arr2.shape[0] == 1, then between each sample of arr1 to arr2
    """
    if len(arr1.shape) == 1:
        arr1 = np.array([arr1])
    if len(arr2.shape) == 1:
        arr2 = np.array([arr2])
    
    valid = ~np.isnan(np.sum(np.vstack((arr1, arr2)), axis=0))
    if mask is not None:
        valid *= mask.flatten()
    arr1_m, arr2_m = arr1[:,valid], arr2[:,valid]
    vx = arr1_m - arr1_m.mean(axis=-1)[:,None]
    vy = arr2_m - arr2_m.mean(axis=-1)[:,None]
    numerator = np.sum(vx * vy, axis=1)
    denom = np.sqrt(np.sum(vx**2, axis=1)) * np.sqrt(np.sum(vy**2, axis=1))
    return numerator / denom

def compute_cc_by_shell(arr1, arr2, res_map, mask=None, n_shells=10):
    """
    Compute the Pearson correlation coefficient by resolution shell between the 
    input arrays. The bin widths of resolutions shells are uniform in d^-3. 
    
    Parameters
    ----------
    arr1 : numpy.ndarray, shape (n_points,)
        input array
    arr2 : numpy.ndarray, shape (n_points,) 
        input array to compute CC with
    res_map : numpy.ndarray, shape (n_points,) 
        resolution in Angstrom of each reciprocal grid point
    mask : numpy.ndarray, shape (map_shape) or (n_points,)
        e.g. to select asu/resolution. True values indicate retained grid points
    n_shells : int
        number of resolution bins
    
    Returns
    -------
    res_shell : numpy.ndarray, shape (n_shells,)
        median resolution of each resolution shell
    cc_shell : numpy.ndarray, shape (n_shells,)
        correlation coefficient by resolution shell
    """
    arr1, arr2 = arr1.flatten(), arr2.flatten()
    if mask is None:
        mask = np.ones(res_map.shape).astype(bool)
    mask *= ~np.isnan(np.sum(np.vstack((arr1, arr2)), axis=0))
        
    inv_dcubed = 1.0 / (res_map ** 3.0)
    res_limit = res_map[mask].min()
    hist, bin_edges = np.histogram(inv_dcubed[res_map>res_limit], bins=n_shells)
    ind = np.digitize(inv_dcubed, bin_edges)
    
    cc_shell, res_shell = np.zeros(n_shells), np.zeros(n_shells)
    for i in range(1, n_shells+1):
        arr1_sel, arr2_sel, mask_sel = arr1[ind==i], arr2[ind==i], mask[ind==i]
        cc_shell[i-1] = compute_cc(arr1_sel, arr2_sel, mask=mask_sel)[0]
        res_shell[i-1] = np.median(res_map[ind==i])
        
    return res_shell, cc_shell

def compute_cc_by_dq(arr1, arr2, dq_map, mask=None):
    """
    Compute the Pearson correlation coefficient as a function of dq, the
    distance between reciprocal grid points and the nearest Bragg peak.
    
    Parameters
    ----------
    arr1 : numpy.ndarray, shape (n_points,)
        input array
    arr2 : numpy.ndarray, shape (n_points,) 
        input array to compute CC with
    dq_map : numpy.ndarray, shape (n_points,)
        distance of each grid point to 
    mask : numpy.ndarray, shape (map_shape) or (n_points,)
        e.g. to select asu/resolution. True values indicate retained grid points
    
    Returns
    -------
    dq_vals : numpy.ndarray, shape (n_unique_dq,)
        unique distances from the nearest Bragg peak
    cc_dq : numpy.ndarray, shape (n_shells,)
        correlation coefficient by resolution shell
    """
    arr1, arr2 = arr1.flatten(), arr2.flatten()
    if mask is None:
        mask = np.ones(dq_map.shape).astype(bool)
    mask *= ~np.isnan(np.sum(np.vstack((arr1, arr2)), axis=0))
    
    dq_vals = np.unique(dq_map)
    cc_dq = np.zeros(len(dq_vals))
    for i,dq in enumerate(dq_vals):
        arr1_sel, arr2_sel, mask_sel = arr1[dq_map==dq], arr2[dq_map==dq], mask[dq_map==dq]
        cc_dq[i] = compute_cc(arr1_sel, arr2_sel, mask=mask_sel)[0]
    
    return dq_vals, cc_dq
</file>
<file path="./eryx/scatter_torch.py" project="ptycho">
"""
PyTorch implementation of structure factor calculations for diffuse scattering.

This module contains PyTorch versions of the structure factor calculations defined
in eryx/scatter.py. All implementations maintain the same API as the NumPy versions
but use PyTorch tensors and operations to enable gradient flow.

References:
    - Original NumPy implementation in eryx/scatter.py
"""

import numpy as np
import torch
import torch.nn.functional as F
from typing import Tuple, List, Optional, Union

def compute_form_factors(q_grid: torch.Tensor, ff_a: torch.Tensor, 
                         ff_b: torch.Tensor, ff_c: torch.Tensor) -> torch.Tensor:
    """
    Calculate atomic form factors at the input q-vectors using PyTorch.
    
    Args:
        q_grid: PyTorch tensor of shape (n_points, 3) containing q-vectors in Angstrom
        ff_a: PyTorch tensor of shape (n_atoms, 4) with a coefficients of atomic form factors
        ff_b: PyTorch tensor of shape (n_atoms, 4) with b coefficients of atomic form factors
        ff_c: PyTorch tensor of shape (n_atoms,) with c coefficients of atomic form factors
        
    Returns:
        PyTorch tensor of shape (n_points, n_atoms) with atomic form factors
        
    References:
        - Original implementation: eryx/scatter.py:compute_form_factors
    """
    # TODO: Convert the NumPy implementation to PyTorch operations:
    # 1. Compute Q = ||q_grid||^2 / (4*pi)^2 using torch operations
    # 2. Compute exponential terms using torch.exp()
    # 3. Perform tensor multiplications and summations
    # 4. Return the transposed form factors tensor
    
    raise NotImplementedError("compute_form_factors not implemented")

def structure_factors_batch(q_grid: torch.Tensor, xyz: torch.Tensor, 
                           ff_a: torch.Tensor, ff_b: torch.Tensor, ff_c: torch.Tensor, 
                           U: Optional[torch.Tensor] = None,
                           compute_qF: bool = False, 
                           project_on_components: Optional[torch.Tensor] = None,
                           sum_over_atoms: bool = True) -> torch.Tensor:
    """
    Compute structure factors for an atomic model at the given q-vectors using PyTorch.
    
    Args:
        q_grid: PyTorch tensor of shape (n_points, 3) with q-vectors in Angstrom
        xyz: PyTorch tensor of shape (n_atoms, 3) with atomic positions in Angstrom
        ff_a: PyTorch tensor of shape (n_atoms, 4) with a coefficients
        ff_b: PyTorch tensor of shape (n_atoms, 4) with b coefficients 
        ff_c: PyTorch tensor of shape (n_atoms,) with c coefficients
        U: Optional PyTorch tensor of shape (n_atoms,) with isotropic displacement parameters
        compute_qF: If True, return structure factors times q-vectors
        project_on_components: Optional projection matrix
        sum_over_atoms: If True, sum over atoms; otherwise return per-atom values
        
    Returns:
        PyTorch tensor containing structure factors
        
    References:
        - Original implementation: eryx/scatter.py:structure_factors_batch
    """
    # TODO: Convert the NumPy implementation to PyTorch operations:
    # 1. Initialize U as zeros tensor if None
    # 2. Compute form factors using compute_form_factors()
    # 3. Calculate q magnitudes using torch.norm()
    # 4. Compute qUq with broadcasting
    # 5. Calculate sine and cosine components using torch operations
    # 6. Apply Debye-Waller factor with torch.exp()
    # 7. Handle compute_qF, project_on_components, and sum_over_atoms options
    
    raise NotImplementedError("structure_factors_batch not implemented")

def structure_factors(q_grid: torch.Tensor, xyz: torch.Tensor, 
                     ff_a: torch.Tensor, ff_b: torch.Tensor, ff_c: torch.Tensor, 
                     U: Optional[torch.Tensor] = None,
                     batch_size: int = 100000, n_processes: int = 1,
                     compute_qF: bool = False, 
                     project_on_components: Optional[torch.Tensor] = None,
                     sum_over_atoms: bool = True) -> torch.Tensor:
    """
    Batched version of structure factor calculation using PyTorch.
    
    Args:
        q_grid: PyTorch tensor of shape (n_points, 3) with q-vectors in Angstrom
        xyz: PyTorch tensor of shape (n_atoms, 3) with atomic positions in Angstrom
        ff_a: PyTorch tensor of shape (n_atoms, 4) with a coefficients
        ff_b: PyTorch tensor of shape (n_atoms, 4) with b coefficients 
        ff_c: PyTorch tensor of shape (n_atoms,) with c coefficients
        U: Optional PyTorch tensor of shape (n_atoms,) with isotropic displacement parameters
        batch_size: Number of q-vectors to evaluate per batch
        n_processes: Number of processes (ignored in PyTorch implementation)
        compute_qF: If True, return structure factors times q-vectors
        project_on_components: Optional projection matrix
        sum_over_atoms: If True, sum over atoms; otherwise return per-atom values
        
    Returns:
        PyTorch tensor containing structure factors
        
    References:
        - Original implementation: eryx/scatter.py:structure_factors
    """
    # TODO: Convert the NumPy implementation to PyTorch operations:
    # 1. Compute number of batches based on q_grid size and batch_size
    # 2. Create output tensor of appropriate shape
    # 3. Process each batch with structure_factors_batch
    # 4. Handle device placement for efficient GPU usage
    # Note: multiprocessing (n_processes) should be ignored in PyTorch implementation as
    # PyTorch has its own parallelization mechanisms
    
    raise NotImplementedError("structure_factors not implemented")
</file>
<file path="./eryx/map_utils.py" project="ptycho">
import numpy as np
import gemmi
from eryx.autotest.debug import debug

@debug
def generate_grid(A_inv, hsampling, ksampling, lsampling, return_hkl=False):
    """
    Generate a grid of q-vectors based on the desired extents 
    and spacing in hkl space.
    
    Parameters
    ----------
    A_inv : numpy.ndarray, shape (3,3)
        fractional cell orthogonalization matrix
    hsampling : tuple, shape (3,)
        (hmin, hmax, oversampling relative to Miller indices)
    ksampling : tuple, shape (3,)
        (kmin, kmax, oversampling relative to Miller indices)
    lsampling : tuple, shape (3,)
        (lmin, lmax, oversampling relative to Miller indices)
    return_hkl : bool
        if True, return hkl indices rather than q-vectors
    
    Returns
    -------
    q_grid or hkl_grid : numpy.ndarray, shape (n_points, 3)
        grid of q-vectors or hkl indices
    map_shape : tuple, shape (3,)
        shape of 3d map
    """
    hsteps = int(hsampling[2]*(hsampling[1]-hsampling[0])+1)
    ksteps = int(ksampling[2]*(ksampling[1]-ksampling[0])+1)
    lsteps = int(lsampling[2]*(lsampling[1]-lsampling[0])+1)
    
    hkl_grid = np.mgrid[lsampling[0]:lsampling[1]:lsteps*1j,
                        ksampling[0]:ksampling[1]:ksteps*1j,
                        hsampling[0]:hsampling[1]:hsteps*1j]
    map_shape = hkl_grid.shape[1:][::-1]
    hkl_grid = hkl_grid.T.reshape(-1,3)
    hkl_grid = hkl_grid[:, [2,1,0]]
    
    if return_hkl:
        return hkl_grid, map_shape
    else:
        q_grid = 2*np.pi*np.inner(A_inv.T, hkl_grid).T
        return q_grid, map_shape

@debug
def get_symmetry_equivalents(hkl_grid, sym_ops):
    """
    Get symmetry equivalent Miller indices of input hkl_grid.
    The symmetry-equivalents are stacked horizontally, so that
    the first dimension of the output array corresponds to the
    nth asymmetric unit.
    
    Parameters
    ----------
    hkl_grid : numpy.ndarray, shape (n_points, 3)
        hkl indices corresponding to flattened intensity map
    sym_ops : dict
        rotational symmetry operations as 3x3 arrays
        
    Returns
    -------
    hkl_grid_sym : numpy.ndarray, shape (n_asu, n_points, 3)
        stacked hkl indices of symmetry-equivalents
    """
    hkl_grid_sym = np.empty(3)
    for i,rot in sym_ops.items():
        hkl_grid_rot = np.matmul(hkl_grid, rot)
        hkl_grid_sym = np.vstack((hkl_grid_sym, hkl_grid_rot))
    hkl_grid_sym = hkl_grid_sym[1:]
    return hkl_grid_sym.reshape(len(sym_ops), hkl_grid.shape[0], 3)
    
@debug
def get_ravel_indices(hkl_grid_sym, sampling):
    """
    Map 3d hkl indices to corresponding 1d indices after raveling.
    
    Parameters
    ----------
    hkl_grid_sym : numpy.ndarray, shape (n_asu, n_points, 3)
        stacked hkl indices of symmetry-equivalents
    sampling : tuple, shape (3,)
        sampling rate relative to integral Millers along (h,k,l)
    
    Returns
    -------
    ravel : numpy.ndarray, shape (n_asu, n_points)
        indices in raveled space for hkl_grid_sym
    map_shape_ravel : tuple, shape (3,)  
        shape of expanded / raveled map
    """
    hkl_grid_stacked = hkl_grid_sym.reshape(-1, hkl_grid_sym.shape[-1])
    hkl_grid_int = np.around(hkl_grid_stacked * np.array(sampling)).astype(int)
    lbounds = np.min(hkl_grid_int, axis=0)
    ubounds = np.max(hkl_grid_int, axis=0)
    map_shape_ravel = tuple((ubounds - lbounds + 1)) 
    hkl_grid_int = hkl_grid_int.reshape(hkl_grid_sym.shape)
    
    ravel = np.zeros(hkl_grid_sym.shape[:2]).astype(int)
    for i in range(ravel.shape[0]):
        ravel[i] = np.ravel_multi_index((hkl_grid_int[i] - lbounds).T, map_shape_ravel)

    return ravel, map_shape_ravel

def cos_sq(angles):
    """ Compute cosine squared of input angles in radians. """
    return np.square(np.cos(angles))

def sin_sq(angles):
    """ Compute sine squared of input angles in radianss. """
    return np.square(np.sin(angles))

@debug
def compute_resolution(cell, hkl):
    """
    Compute reflections' resolution in 1/Angstrom. To check, see: 
    https://www.ruppweb.org/new_comp/reciprocal_cell.htm.
        
    Parameters
    ----------
    cell : numpy.ndarray, shape (6,)
        unit cell parameters (a,b,c,alpha,beta,gamma) in Ang/deg
    hkl : numpy.ndarray, shape (n_refl, 3)
        Miller indices of reflections
            
    Returns
    -------
    resolution : numpy.ndarray, shape (n_refl)
        resolution associated with each reflection in Angstrom
    """

    a,b,c = [cell[i] for i in range(3)] 
    alpha,beta,gamma = [np.radians(cell[i]) for i in range(3,6)] 
    h,k,l = [hkl[:,i] for i in range(3)]

    pf = 1.0 - cos_sq(alpha) - cos_sq(beta) - cos_sq(gamma) + 2.0*np.cos(alpha)*np.cos(beta)*np.cos(gamma)
    n1 = np.square(h)*sin_sq(alpha)/np.square(a) + np.square(k)*sin_sq(beta)/np.square(b) + np.square(l)*sin_sq(gamma)/np.square(c)
    n2a = 2.0*k*l*(np.cos(beta)*np.cos(gamma) - np.cos(alpha))/(b*c)
    n2b = 2.0*l*h*(np.cos(gamma)*np.cos(alpha) - np.cos(beta))/(c*a)
    n2c = 2.0*h*k*(np.cos(alpha)*np.cos(beta) - np.cos(gamma))/(a*b)

    with np.errstate(divide='ignore'):
        return 1.0 / np.sqrt((n1 + n2a + n2b + n2c) / pf)

def get_hkl_extents(cell, resolution, oversampling=1):
    """
    Determine the min/max hkl for the given cell and resolution.
    
    Parameters
    ----------
    cell : numpy.ndarray, shape (6,)
        unit cell parameters in Angstrom / degrees
    resolution : float
        high-resolution limit
    oversampling : int or tuple of shape (3,)
        oversampling rate relative to integral Miller indices
    
    Returns
    -------
    hsampling : tuple, shape (3,)
        (min, max, interval) along h axis
    ksampling : tuple, shape (3,)
        (min, max, interval) along k axis
    lsampling : tuple, shape (3,)
        (min, max, interval) along l axis        
    """
    if type(oversampling) == int:
        oversampling = 3 * [oversampling]
    g_cell = gemmi.UnitCell(*cell)
    h,k,l = g_cell.get_hkl_limits(resolution)
    return (-h,h,oversampling[0]), (-k,k,oversampling[1]), (-l,l,oversampling[2])

def expand_sym_ops(sym_ops):
    """
    Expand symmetry operations to include Friedel equivalents.
    
    Parameters
    ----------
    sym_ops : dict
        rotational symmetry operations as 3x3 matrices
    
    Returns
    -------
    sym_ops_exp : dict
        sym_ops, expanded to account for Friedel symmetry
    """
    sym_ops_exp = dict(sym_ops)
    for key in sym_ops:
        sym_ops_exp[key + len(sym_ops)] = -1 * sym_ops[key]
    return sym_ops_exp

def compute_multiplicity(model, hsampling, ksampling, lsampling):
    """
    Compute the multiplicity of each voxel in the map.
    
    Parameters
    ----------
    model : AtomicModel 
        instance of AtomicModel class
    hsampling : tuple, shape (3,)
        (min, max, interval) along h axis
    ksampling : tuple, shape (3,)
        (min, max, interval) along k axis
    lsampling : tuple, shape (3,)
        (min, max, interval) along l axis        
    
    Returns
    -------
    hkl_grid : numpy.ndarray, shape (n_points, 3)
        grid of hkl vectors
    multiplicity : numpy.ndarray, 3d
        multiplicity of each grid point in the map
    """
    sym_ops_exp = expand_sym_ops(model.sym_ops)
    hkl_grid, map_shape = generate_grid(model.A_inv, hsampling, ksampling, lsampling, return_hkl=True)
    hkl_sym = get_symmetry_equivalents(hkl_grid, sym_ops_exp)
    ravel, map_shape_ravel = get_ravel_indices(hkl_sym, (hsampling[2], ksampling[2], lsampling[2]))
    multiplicity = (np.diff(np.sort(ravel.T,axis=1),axis=1)!=0).sum(axis=1)+1
    return hkl_grid, multiplicity.reshape(map_shape)

def parse_asu_condition(asu_condition):
    """
    Parse Gemmi's string describing which reflections belong to the 
    asymmetric unit into a string compatible with python's eval.
    
    Parameters
    ----------
    asu_condition : str
        Gemmi-style string describing asu
        
    Returns
    -------
    asu_condition : str
        eval-compatible string describing asu
    """
    # convert to numpy boolean operators
    find = ["=", "and", "or", ">==", "<=="] 
    replace = ["==", "&", "|", ">=", "<="]
    for i,j in zip(find, replace):
        asu_condition = asu_condition.replace(i,j)
        
    # add missing parenthesis around individual conditions
    alphas = [idx for idx in range(len(asu_condition)) if (asu_condition[idx].isalpha() and asu_condition[idx+1] not in [" ", ")", "("])]
    counter = 0
    for start in alphas:
        asu_condition = asu_condition[:start+counter] + "(" + asu_condition[start+counter:]
        counter += 1
        end = asu_condition.find(" ", start+counter)
        if end == -1:
            asu_condition = asu_condition + ")"
        else:
            asu_condition = asu_condition[:end] + ")" + asu_condition[end:]
        counter += 1
    
    return asu_condition

def get_asu_mask(space_group, hkl_grid):
    """
    Generate a boolean mask that indicates which hkl indices belong
    to the asymmetric unit.
    
    Parameters
    ----------
    space_group : int or str
        crystal's space group
    hkl_grid : numpy.ndarray, shape (n_points, 3)
        hkl indices 
        
    Returns
    -------
    asu_mask : numpy.ndarray, shape (n_points,)
        True indicates hkls that belong to the asymmetric unit
    """
    sg = gemmi.SpaceGroup(space_group)
    asu_condition = gemmi.ReciprocalAsu(sg).condition_str()
    asu_condition = parse_asu_condition(asu_condition)
    h, k, l = [hkl_grid[:,i] for i in range(3)]
    asu_mask = eval(asu_condition)
    return asu_mask

@debug
def get_resolution_mask(cell, hkl_grid, res_limit):
    """
    Generate a boolean mask that indicates which hkl indices belong
    to the asymmetric unit.
    
    Parameters
    ----------
    space_group : int or str
        crystal's space group
    hkl_grid : numpy.ndarray, shape (n_points, 3)
        hkl indices 
    res_limit : float
        high resolution limit in Angstrom
        
    Returns
    -------
    res_mask : numpy.ndarray, shape (n_points,)
        True indicates hkls that are within high resolution limit
    res_map : numpy.ndarray, shape (n_points,)
        resolution in Angstrom for each point in the grid
    """
    res_map = compute_resolution(cell, hkl_grid)
    res_mask = res_map > res_limit
    return res_mask, res_map

@debug
def get_dq_map(A_inv, hkl_grid):
    """
    Compute dq, the distance to the nearest Bragg peak, for 
    each reciprocal grid point.
    
    Parameters
    ----------
    A_inv : numpy.ndarray, shape (3,3)
        fractional cell orthogonalization matrix
    hkl_grid : numpy.ndarray, shape (n_points, 3)
        grid of hkl indices
    
    Returns
    -------
    dq : numpy.ndarray, shape (n_points,)
        distance to the nearest Bragg peak
    """
    hkl_closest = np.around(hkl_grid)
    q_closest = 2*np.pi*np.inner(A_inv.T, hkl_closest).T 
    q_grid = 2*np.pi*np.inner(A_inv.T, hkl_grid).T 
    dq = np.linalg.norm(np.abs(q_closest - q_grid), axis=1)
    return np.around(dq, decimals=8)

@debug
def get_centered_sampling(map_shape, sampling):
    """
    Get the hsampling, ksampling, and lsampling tuples for the input 
    map shape, assuming that the map is centered about the origin in 
    reciprocal space, i.e. h,k,l=(0,0,0).
    
    Parameters
    ----------
    map_shape : tuple
        map dimensions
    sampling : tuple
        fractional sampling rate along h,k,l axes
    
    Returns
    -------
    list of tuples, [hsampling, lsampling, ksampling]
        in which each tuple corresponds to (min, max, fractional sampling rate)
    """
    extents = [((map_shape[i]-1) / sampling[i] / 2.0) for i in range(3)]
    return [(-extents[i], extents[i], sampling[i]) for i in range(3)]

@debug
def resize_map(new_map, old_sampling, new_sampling):
    """
    Resize map if symmetrization has resulted in the inclusion of 
    out-of-bounds regions, but not those valid by Friedel's law.
    
    Parameters
    ----------
    new_map : numpy.ndarray, 3d
        map to potentially crop
    old_sampling : tuple of tuples
        (hsampling, ksampling, lsampling) for original hkl_grid
    new_sampling : tuple of tuples 
        (hsampling, ksampling, lsampling) for new_map
        
    Returns
    -------
    new_map : numpy.ndarray, 3d
        potentially cropped map
    """
    tol = 1e-6
    if np.abs(new_sampling[0][1] - old_sampling[0][1]) > tol:
        excise = int(np.around(2*(new_sampling[0][1] - old_sampling[0][1])))
        new_map = new_map[excise:-excise,:,:]
    if np.abs(new_sampling[1][1] - old_sampling[1][1]) > tol:
        excise = int(np.around(2*(new_sampling[1][1] - old_sampling[1][1])))
        new_map = new_map[:,excise:-excise,:]
    if np.abs(new_sampling[2][1] - old_sampling[2][1]) > tol:
        excise = int(np.around(2*(new_sampling[2][1] - old_sampling[2][1])))
        new_map = new_map[:,:,excise:-excise]
    return new_map
</file>
<file path="./eryx/scripts/generate_ground_truth.py" project="ptycho">
#!/usr/bin/env python3
"""
Ground truth data generation script for PyTorch port.

This script runs the NumPy implementation with different parameter sets
to generate ground truth data for testing the PyTorch implementation.
"""

import os
import sys
import logging
import time
import numpy as np

# Import the configuration to ensure debug mode is enabled
from eryx.autotest_config import config

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s %(levelname)s: %(message)s",
    handlers=[
        logging.FileHandler("ground_truth_generation.log"),
        logging.StreamHandler(sys.stdout)
    ]
)

# Import after setting DEBUG_MODE
# Add the project root to the path so we can import run_debug.py
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '../..')))
from run_debug import run_np
from eryx.autotest_config import config

def main():
    """
    Main function to generate ground truth data.
    """
    start_time = time.time()
    logging.info("Starting ground truth data generation")
    
    # Create output directory if it doesn't exist
    os.makedirs("output", exist_ok=True)
    
    # Ensure log directory exists
    log_dir = "logs"  # Use the default log directory
    os.makedirs(log_dir, exist_ok=True)
    logging.info(f"Log files will be saved to: {log_dir}")
    
    # Run the NumPy implementation once
    run_start = time.time()
    logging.info("Running NumPy implementation")
    try:
        run_np()
        logging.info(f"Completed run in {time.time() - run_start:.2f} seconds")
    except Exception as e:
        logging.error(f"Error in run: {e}")
    
    # Verify log files were created
    log_files = []
    for root, _, files in os.walk(log_dir):
        for file in files:
            if file.endswith(".log"):
                log_files.append(os.path.join(root, file))
    
    logging.info(f"Found {len(log_files)} log files")
    
    total_time = time.time() - start_time
    logging.info(f"Ground truth data generation completed in {total_time:.2f} seconds")

if __name__ == "__main__":
    main()
</file>
<file path="./eryx/scripts/verify_ground_truth.py" project="ptycho">
#!/usr/bin/env python3
"""
Verification script for ground truth data.

This script checks that all required functions have been captured
in the ground truth data logs.
"""

import os
import sys
import json
import logging
import re
from collections import defaultdict

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s %(levelname)s: %(message)s",
    handlers=[
        logging.FileHandler("ground_truth_verification.log"),
        logging.StreamHandler(sys.stdout)
    ]
)

def load_to_convert_json():
    """Load the to_convert.json file."""
    try:
        with open("to_convert.json", "r") as f:
            return json.load(f)
    except Exception as e:
        logging.error(f"Error loading to_convert.json: {e}")
        return {}

def get_function_list(to_convert):
    """Extract a flat list of all functions to convert."""
    functions = []
    
    for module, items in to_convert.items():
        for item_name, item_value in items.items():
            if item_name.startswith("class "):
                class_name = item_name.replace("class ", "")
                for method_name in item_value.keys():
                    functions.append(f"{module}:{class_name}.{method_name}")
            else:
                functions.append(f"{module}:{item_name}")
    
    return functions

def search_log_files(log_dir):
    """Search for log files in the log directory."""
    log_files = []
    for root, _, files in os.walk(log_dir):
        for file in files:
            if file.endswith(".log"):
                log_files.append(os.path.join(root, file))
    return log_files

def extract_function_name(log_file):
    """Extract function name from log file path."""
    # Example: ground_truth_data/eryx.scatter.compute_form_factors.log
    match = re.search(r'([^/]+)\.log$', log_file)
    if match:
        return match.group(1)
    return None

def group_log_files(log_files):
    """Group log files by function name."""
    grouped = defaultdict(list)
    for log_file in log_files:
        func_name = extract_function_name(log_file)
        if func_name:
            grouped[func_name].append(log_file)
    return grouped

def count_function_calls(log_file):
    """Count the number of function calls in a log file."""
    try:
        with open(log_file, "r") as f:
            lines = f.readlines()
        # Each function call should have two log entries (args and result)
        return len(lines) // 2
    except Exception as e:
        logging.error(f"Error reading log file {log_file}: {e}")
        return 0

def main():
    """Main verification function."""
    logging.info("Starting ground truth data verification")
    
    # Load the to_convert.json file
    to_convert = load_to_convert_json()
    if not to_convert:
        logging.error("Failed to load to_convert.json")
        return
    
    # Get the list of functions to convert
    expected_functions = get_function_list(to_convert)
    logging.info(f"Found {len(expected_functions)} functions in to_convert.json")
    
    # Search for log files
    log_dir = "ground_truth_data"
    log_files = search_log_files(log_dir)
    logging.info(f"Found {len(log_files)} log files in {log_dir}")
    
    # Group log files by function
    grouped_logs = group_log_files(log_files)
    logging.info(f"Found logs for {len(grouped_logs)} unique functions")
    
    # Count function calls for each function
    function_counts = {}
    for func_name, files in grouped_logs.items():
        total_calls = sum(count_function_calls(file) for file in files)
        function_counts[func_name] = total_calls
    
    # Check for missing functions
    missing_functions = []
    for func in expected_functions:
        module, func_name = func.split(":")
        # Convert from to_convert.json format to log file format
        if "." in func_name:  # It's a class method
            class_name, method_name = func_name.split(".")
            log_func_name = f"{module}.{class_name}.{method_name}"
        else:
            log_func_name = f"{module}.{func_name}"
        
        if log_func_name not in function_counts:
            missing_functions.append(func)
    
    # Print summary
    logging.info("\n=== Ground Truth Data Summary ===")
    logging.info(f"Total functions to convert: {len(expected_functions)}")
    logging.info(f"Functions with log data: {len(function_counts)}")
    logging.info(f"Missing functions: {len(missing_functions)}")
    
    if missing_functions:
        logging.warning("The following functions are missing log data:")
        for func in missing_functions:
            logging.warning(f"  - {func}")
    
    # Print function call counts
    logging.info("\n=== Function Call Counts ===")
    for func_name, count in sorted(function_counts.items(), key=lambda x: x[0]):
        logging.info(f"{func_name}: {count} calls")
    
    logging.info("\nGround truth data verification completed")

if __name__ == "__main__":
    main()
</file>
<file path="./eryx/run_torch.py" project="ptycho">
#!/usr/bin/env python3
"""
PyTorch implementation of diffuse scattering simulation.

This script provides a PyTorch-based implementation of the diffuse scattering
simulation, parallel to the NumPy implementation in run_debug.py. It enables
gradient-based optimization of simulation parameters.
"""

import os
import logging
import numpy as np
import torch
from typing import Optional, Tuple, Dict, Any

# Set up logging
logging.basicConfig(
    level=logging.DEBUG,
    format="%(asctime)s %(levelname)s: %(message)s",
    filename="torch_debug_output.log",
    filemode="w"
)
console = logging.StreamHandler()
console.setLevel(logging.DEBUG)
console.setFormatter(logging.Formatter("%(asctime)s %(levelname)s: %(message)s"))
logging.getLogger("").addHandler(console)

def setup_logging():
    """
    Set up logging configuration.
    """
    # Remove any existing handlers.
    for handler in logging.root.handlers[:]:
        logging.root.removeHandler(handler)
    
    logging.basicConfig(
        level=logging.DEBUG,
        format="%(asctime)s %(levelname)s: %(message)s",
        filename="torch_debug_output.log",
        filemode="w"
    )
    # Also output to console
    console = logging.StreamHandler()
    console.setLevel(logging.DEBUG)
    formatter = logging.Formatter("%(asctime)s %(levelname)s: %(message)s")
    console.setFormatter(formatter)
    logging.getLogger("").addHandler(console)

def run_torch(device: Optional[torch.device] = None):
    """
    Run PyTorch version of the diffuse scattering simulation.
    
    Args:
        device: PyTorch device to use (default: CUDA if available, else CPU)
    """
    if device is None:
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    logging.info(f"Starting PyTorch branch computation on {device}")
    
    # Same parameters as NumPy version
    pdb_path = "tests/pdbs/5zck_p1.pdb"
    
    # TODO: Import the PyTorch OnePhonon implementation
    # TODO: Create OnePhonon instance with parameters
    # TODO: Apply disorder model
    # TODO: Extract and save results
    # TODO: Log debug information
    
    # Placeholder for actual implementation
    logging.info("PyTorch implementation not implemented yet")
    
    # When implemented, save results for comparison
    # torch.save(Id_torch, "torch_diffuse_intensity.pt")
    # np.save("torch_diffuse_intensity.npy", Id_torch.detach().cpu().numpy())

def run_np():
    """
    Run NumPy version of the diffuse scattering simulation.
    
    This is the same function as in run_debug.py, included for direct comparison.
    """
    logging.info("Starting NP branch computation")
    pdb_path = "tests/pdbs/5zck_p1.pdb"
    
    from eryx.models import OnePhonon
    
    onephonon_np = OnePhonon(
        pdb_path,
        [-4, 4, 3], [-17, 17, 3], [-29, 29, 3],
        expand_p1=True,
        res_limit=0.0,
        gnm_cutoff=4.0,
        gamma_intra=1.0,
        gamma_inter=1.0
    )
    Id_np = onephonon_np.apply_disorder(use_data_adp=True)
    logging.debug(f"NP: hkl_grid shape = {onephonon_np.hkl_grid.shape}")
    logging.debug("NP: hkl_grid coordinate ranges:")
    logging.debug(f"  Dimension 0: min = {onephonon_np.hkl_grid[:,0].min()}, max = {onephonon_np.hkl_grid[:,0].max()}")
    logging.debug(f"  Dimension 1: min = {onephonon_np.hkl_grid[:,1].min()}, max = {onephonon_np.hkl_grid[:,1].max()}")
    logging.debug(f"  Dimension 2: min = {onephonon_np.hkl_grid[:,2].min()}, max = {onephonon_np.hkl_grid[:,2].max()}")
    logging.debug(f"NP: q_grid range: min = {onephonon_np.q_grid.min()}, max = {onephonon_np.q_grid.max()}")
    logging.info("NP branch diffuse intensity stats: min=%s, max=%s", np.nanmin(Id_np), np.nanmax(Id_np))
    np.save("np_diffuse_intensity.npy", Id_np)

def compare_results():
    """
    Compare the results from NumPy and PyTorch implementations.
    """
    # TODO: Load NumPy and PyTorch results
    # TODO: Compute statistics (MSE, correlation)
    # TODO: Log comparison results
    
    raise NotImplementedError("compare_results not implemented")

if __name__ == "__main__":
    setup_logging()
    run_np()
    run_torch()
    logging.info("Completed debug run. Please check torch_debug_output.log, np_diffuse_intensity.npy and torch_diffuse_intensity.npy")
</file>
<file path="./eryx/torch_utils.py" project="ptycho">
"""
PyTorch-specific utilities for diffuse scattering calculations.

This module contains PyTorch-specific utilities and helper functions for
diffuse scattering calculations, including complex number operations,
differentiable eigendecomposition, and other tensor operations.
"""

import numpy as np
import torch
import torch.nn.functional as F
from typing import Tuple, List, Dict, Optional, Union, Any

class ComplexTensorOps:
    """
    Operations for complex tensors.
    
    PyTorch's complex tensor support is still evolving, so this class provides
    helper methods for complex tensor operations to ensure differentiability.
    """
    
    @staticmethod
    def complex_exp(phase: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Compute complex exponential e^(i*phase).
        
        Args:
            phase: PyTorch tensor with phase values
            
        Returns:
            Tuple of (real, imaginary) parts
        """
        return torch.cos(phase), torch.sin(phase)
    
    @staticmethod
    def complex_mul(a_real: torch.Tensor, a_imag: torch.Tensor, 
                   b_real: torch.Tensor, b_imag: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Multiply complex numbers in rectangular form.
        
        Args:
            a_real: Real part of first operand
            a_imag: Imaginary part of first operand
            b_real: Real part of second operand
            b_imag: Imaginary part of second operand
            
        Returns:
            Tuple of (real, imaginary) parts of the product
        """
        real = a_real * b_real - a_imag * b_imag
        imag = a_real * b_imag + a_imag * b_real
        return real, imag
    
    @staticmethod
    def complex_abs_squared(real: torch.Tensor, imag: torch.Tensor) -> torch.Tensor:
        """
        Compute squared magnitude of complex numbers.
        
        Args:
            real: Real part
            imag: Imaginary part
            
        Returns:
            Squared magnitude |z|^2
        """
        return real**2 + imag**2
    
    @staticmethod
    def complex_exp_dwf(q_vec: torch.Tensor, u_vec: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Compute complex exponential for Debye-Waller factor.
        
        Args:
            q_vec: Q-vector tensor
            u_vec: Displacement tensor
            
        Returns:
            Tuple of (real, imaginary) parts of e^(-0.5*qUq)
        """
        qUq = torch.sum(q_vec * u_vec * q_vec, dim=-1)
        dwf = torch.exp(-0.5 * qUq)
        return dwf, torch.zeros_like(dwf)

class FFTOps:
    """
    FFT operations for diffuse scattering calculations.
    
    This class provides FFT operations specifically tailored for diffuse
    scattering calculations, ensuring differentiability and proper handling
    of complex numbers.
    """
    
    @staticmethod
    def fft_convolve(signal: torch.Tensor, kernel: torch.Tensor) -> torch.Tensor:
        """
        Convolve signal with kernel using FFT.
        
        Args:
            signal: Input signal tensor
            kernel: Convolution kernel tensor
            
        Returns:
            Convolved signal tensor
        """
        # TODO: Normalize kernel
        # TODO: Compute FFTs
        # TODO: Multiply in frequency domain
        # TODO: Compute inverse FFT
        # TODO: Return real part
        
        raise NotImplementedError("fft_convolve not implemented")
    
    @staticmethod
    def fft_3d(input_tensor: torch.Tensor) -> torch.Tensor:
        """
        Compute 3D FFT of input tensor.
        
        Args:
            input_tensor: Input tensor
            
        Returns:
            Output tensor with FFT result
        """
        # TODO: Use torch.fft.fftn with appropriate normalization
        # TODO: Handle complex numbers properly
        
        raise NotImplementedError("fft_3d not implemented")
    
    @staticmethod
    def ifft_3d(input_tensor: torch.Tensor) -> torch.Tensor:
        """
        Compute 3D inverse FFT of input tensor.
        
        Args:
            input_tensor: Input tensor
            
        Returns:
            Output tensor with IFFT result
        """
        # TODO: Use torch.fft.ifftn with appropriate normalization
        # TODO: Handle complex numbers properly
        
        raise NotImplementedError("ifft_3d not implemented")

class EigenOps:
    """
    Differentiable eigendecomposition operations.
    
    This class provides differentiable implementations of eigenvalue
    decomposition and related operations for the diffuse scattering calculations.
    """
    
    @staticmethod
    def svd_decomposition(matrix: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        Compute SVD decomposition with gradient support.
        
        Args:
            matrix: Input matrix tensor
            
        Returns:
            Tuple of (U, S, V) tensors
        """
        # TODO: Use torch.linalg.svd
        # TODO: Ensure proper gradient flow
        
        raise NotImplementedError("svd_decomposition not implemented")
    
    @staticmethod
    def eigen_decomposition(matrix: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Compute eigendecomposition with gradient support.
        
        Args:
            matrix: Input matrix tensor
            
        Returns:
            Tuple of (eigenvalues, eigenvectors) tensors
        """
        # TODO: Use appropriate PyTorch function
        # TODO: Handle complex matrices if necessary
        # TODO: Ensure proper gradient flow
        
        raise NotImplementedError("eigen_decomposition not implemented")
    
    @staticmethod
    def solve_linear_system(A: torch.Tensor, b: torch.Tensor) -> torch.Tensor:
        """
        Solve linear system Ax = b with gradient support.
        
        Args:
            A: Coefficient matrix
            b: Right-hand side vector
            
        Returns:
            Solution vector x
        """
        # TODO: Use torch.linalg.solve
        # TODO: Handle edge cases (singular matrices)
        # TODO: Ensure proper gradient flow
        
        raise NotImplementedError("solve_linear_system not implemented")

class GradientUtils:
    """
    Utilities for gradient computation and manipulation.
    
    This class provides utilities for computing and manipulating gradients
    in the diffuse scattering calculations.
    """
    
    @staticmethod
    def finite_differences(func: callable, input_tensor: torch.Tensor, 
                          eps: float = 1e-6) -> torch.Tensor:
        """
        Compute gradients using finite differences for validation.
        
        Args:
            func: Function to differentiate
            input_tensor: Input tensor
            eps: Step size for finite differences
            
        Returns:
            Gradient tensor
        """
        # TODO: Implement central difference scheme
        # TODO: Handle multidimensional inputs
        
        raise NotImplementedError("finite_differences not implemented")
    
    @staticmethod
    def validate_gradients(analytical_grad: torch.Tensor, 
                          numerical_grad: torch.Tensor, 
                          rtol: float = 1e-4, 
                          atol: float = 1e-6) -> bool:
        """
        Validate analytical gradients against numerical gradients.
        
        Args:
            analytical_grad: Analytically computed gradients
            numerical_grad: Numerically computed gradients
            rtol: Relative tolerance
            atol: Absolute tolerance
            
        Returns:
            True if gradients match within tolerance
        """
        # TODO: Compute element-wise relative and absolute errors
        # TODO: Check if errors are within tolerance
        
        raise NotImplementedError("validate_gradients not implemented")
    
    @staticmethod
    def gradient_norm(gradient: torch.Tensor) -> torch.Tensor:
        """
        Compute L2 norm of gradient.
        
        Args:
            gradient: Gradient tensor
            
        Returns:
            Gradient norm
        """
        return torch.norm(gradient, p=2)
</file>
<file path="./eryx/map_utils_torch.py" project="ptycho">
"""
PyTorch implementation of map utility functions for diffuse scattering.

This module contains PyTorch versions of the map utility functions defined
in eryx/map_utils.py. All implementations maintain the same API as the NumPy versions
but use PyTorch tensors and operations to enable gradient flow.

References:
    - Original NumPy implementation in eryx/map_utils.py
"""

import numpy as np
import torch
import gemmi  # Keep gemmi imports for crystallographic data
from typing import Tuple, List, Dict, Optional, Union, Any

def generate_grid(A_inv: torch.Tensor, hsampling: Tuple[float, float, float], 
                 ksampling: Tuple[float, float, float], lsampling: Tuple[float, float, float], 
                 return_hkl: bool = False) -> Tuple[torch.Tensor, Tuple[int, int, int]]:
    """
    Generate a grid of q-vectors based on the desired extents and spacing in hkl space.
    
    Args:
        A_inv: PyTorch tensor of shape (3, 3) with fractional cell orthogonalization matrix
        hsampling: Tuple (hmin, hmax, oversampling) for h dimension
        ksampling: Tuple (kmin, kmax, oversampling) for k dimension
        lsampling: Tuple (lmin, lmax, oversampling) for l dimension
        return_hkl: If True, return hkl indices rather than q-vectors
        
    Returns:
        Tuple containing:
            - PyTorch tensor of shape (n_points, 3) with q-vectors or hkl indices
            - Tuple with shape of 3D map
            
    References:
        - Original implementation: eryx/map_utils.py:generate_grid
    """
    # Calculate steps for each dimension
    hsteps = int(hsampling[2] * (hsampling[1] - hsampling[0]) + 1)
    ksteps = int(ksampling[2] * (ksampling[1] - ksampling[0]) + 1)
    lsteps = int(lsampling[2] * (lsampling[1] - lsampling[0]) + 1)
    
    # Create linspace for each dimension
    l_grid = torch.linspace(lsampling[0], lsampling[1], lsteps)
    k_grid = torch.linspace(ksampling[0], ksampling[1], ksteps)
    h_grid = torch.linspace(hsampling[0], hsampling[1], hsteps)
    
    # Create meshgrid
    # Note: torch.meshgrid behavior changed in PyTorch 1.10
    # Using indexing='ij' to match NumPy's default behavior
    l_mesh, k_mesh, h_mesh = torch.meshgrid(l_grid, k_grid, h_grid, indexing='ij')
    
    # Get map shape
    map_shape = (h_mesh.size(2), k_mesh.size(1), l_mesh.size(0))
    
    # Reshape and reorder dimensions
    hkl_grid = torch.stack([h_mesh.flatten(), k_mesh.flatten(), l_mesh.flatten()], dim=1)
    
    if return_hkl:
        return hkl_grid, map_shape
    else:
        # Calculate q_grid using matrix multiplication
        # q_grid = 2π * A_inv^T * hkl_grid^T
        q_grid = 2 * torch.pi * torch.matmul(A_inv.T, hkl_grid.T).T
        return q_grid, map_shape

def get_symmetry_equivalents(hkl_grid: torch.Tensor, sym_ops: Dict[int, torch.Tensor]) -> torch.Tensor:
    """
    Get symmetry equivalent Miller indices of input hkl_grid.
    
    Args:
        hkl_grid: PyTorch tensor of shape (n_points, 3) with hkl indices
        sym_ops: Dictionary mapping integer keys to rotation matrices as PyTorch tensors
        
    Returns:
        PyTorch tensor of shape (n_asu, n_points, 3) with stacked hkl indices
        
    References:
        - Original implementation: eryx/map_utils.py:get_symmetry_equivalents
    """
    # Initialize list to collect rotated hkl indices
    hkl_grid_rotated_list = []
    
    # Apply each symmetry operation
    for i, rot in sym_ops.items():
        # Apply rotation matrix to hkl_grid
        hkl_grid_rot = torch.matmul(hkl_grid, rot)
        hkl_grid_rotated_list.append(hkl_grid_rot)
    
    # Stack all rotated grids
    hkl_grid_sym = torch.stack(hkl_grid_rotated_list, dim=0)
    
    # Shape should be (n_asu, n_points, 3)
    return hkl_grid_sym

def get_ravel_indices(hkl_grid_sym: torch.Tensor, 
                    sampling: Tuple[float, float, float]) -> Tuple[torch.Tensor, Tuple[int, int, int]]:
    """
    Map 3D hkl indices to corresponding 1D indices after raveling.
    
    Args:
        hkl_grid_sym: PyTorch tensor of shape (n_asu, n_points, 3) with symmetry equivalents
        sampling: Tuple with sampling rates along (h, k, l)
        
    Returns:
        Tuple containing:
            - PyTorch tensor of shape (n_asu, n_points) with raveled indices
            - Tuple with shape of expanded/raveled map
            
    References:
        - Original implementation: eryx/map_utils.py:get_ravel_indices
    """
    # Reshape for processing
    n_asu, n_points, _ = hkl_grid_sym.shape
    hkl_grid_stacked = hkl_grid_sym.reshape(-1, 3)
    
    # Convert to integer indices with scaling
    sampling_tensor = torch.tensor(sampling, device=hkl_grid_sym.device)
    hkl_grid_int = torch.round(hkl_grid_stacked * sampling_tensor).long()
    
    # Find bounds and calculate map shape
    lbounds, _ = torch.min(hkl_grid_int, dim=0)
    ubounds, _ = torch.max(hkl_grid_int, dim=0)
    map_shape_ravel = tuple((ubounds - lbounds + 1).tolist())
    
    # Reshape back to original dimensions
    hkl_grid_int = hkl_grid_int.reshape(n_asu, n_points, 3)
    
    # Initialize output tensor
    ravel = torch.zeros((n_asu, n_points), dtype=torch.long, device=hkl_grid_sym.device)
    
    # Implement ravel_multi_index equivalent
    for i in range(n_asu):
        # Shift indices to start from 0
        shifted = hkl_grid_int[i] - lbounds
        
        # Calculate raveled indices
        # Formula: index = x * (dim_y * dim_z) + y * dim_z + z
        strides = torch.tensor([map_shape_ravel[1] * map_shape_ravel[2], 
                               map_shape_ravel[2], 
                               1], device=hkl_grid_sym.device)
        
        ravel[i] = torch.sum(shifted * strides, dim=1)
    
    return ravel, map_shape_ravel

def cos_sq(angles: torch.Tensor) -> torch.Tensor:
    """
    Compute cosine squared of input angles in radians.
    
    Args:
        angles: PyTorch tensor with angles in radians
        
    Returns:
        PyTorch tensor with cos^2 of angles
    """
    return torch.square(torch.cos(angles))

def sin_sq(angles: torch.Tensor) -> torch.Tensor:
    """
    Compute sine squared of input angles in radians.
    
    Args:
        angles: PyTorch tensor with angles in radians
        
    Returns:
        PyTorch tensor with sin^2 of angles
    """
    return torch.square(torch.sin(angles))

def compute_resolution(cell: torch.Tensor, hkl: torch.Tensor) -> torch.Tensor:
    """
    Compute reflections' resolution in 1/Angstrom using PyTorch.
    
    Args:
        cell: PyTorch tensor of shape (6,) with unit cell parameters
        hkl: PyTorch tensor of shape (n_refl, 3) with Miller indices
        
    Returns:
        PyTorch tensor of shape (n_refl,) with resolution in Angstrom
        
    References:
        - Original implementation: eryx/map_utils.py:compute_resolution
    """
    # Extract cell parameters
    a, b, c = cell[0], cell[1], cell[2]
    alpha, beta, gamma = torch.deg2rad(cell[3]), torch.deg2rad(cell[4]), torch.deg2rad(cell[5])
    
    # Extract Miller indices
    h, k, l = hkl[:, 0], hkl[:, 1], hkl[:, 2]
    
    # Calculate terms
    pf = 1.0 - cos_sq(alpha) - cos_sq(beta) - cos_sq(gamma) + 2.0 * torch.cos(alpha) * torch.cos(beta) * torch.cos(gamma)
    
    n1 = torch.square(h) * sin_sq(alpha) / torch.square(a) + \
         torch.square(k) * sin_sq(beta) / torch.square(b) + \
         torch.square(l) * sin_sq(gamma) / torch.square(c)
    
    n2a = 2.0 * k * l * (torch.cos(beta) * torch.cos(gamma) - torch.cos(alpha)) / (b * c)
    n2b = 2.0 * l * h * (torch.cos(gamma) * torch.cos(alpha) - torch.cos(beta)) / (c * a)
    n2c = 2.0 * h * k * (torch.cos(alpha) * torch.cos(beta) - torch.cos(gamma)) / (a * b)
    
    # Calculate resolution with safe division
    denominator = (n1 + n2a + n2b + n2c) / pf
    # Handle potential divide by zero
    safe_denominator = torch.where(denominator > 0, denominator, torch.ones_like(denominator))
    resolution = 1.0 / torch.sqrt(safe_denominator)
    
    # Set resolution to infinity where denominator is zero or negative
    resolution = torch.where(denominator > 0, resolution, float('inf') * torch.ones_like(resolution))
    
    return resolution

def get_resolution_mask(cell: torch.Tensor, hkl_grid: torch.Tensor, 
                       res_limit: float) -> Tuple[torch.Tensor, torch.Tensor]:
    """
    Generate a boolean mask for resolution limits.
    
    Args:
        cell: PyTorch tensor of shape (6,) with cell parameters
        hkl_grid: PyTorch tensor of shape (n_points, 3) with hkl indices
        res_limit: High resolution limit in Angstrom
        
    Returns:
        Tuple containing:
            - PyTorch tensor of shape (n_points,) with boolean mask
            - PyTorch tensor of shape (n_points,) with resolution map
            
    References:
        - Original implementation: eryx/map_utils.py:get_resolution_mask
    """
    # Compute resolution map
    res_map = compute_resolution(cell, hkl_grid)
    
    # Create mask by comparing to res_limit
    res_mask = res_map > res_limit
    
    return res_mask, res_map

def get_dq_map(A_inv: torch.Tensor, hkl_grid: torch.Tensor) -> torch.Tensor:
    """
    Compute distance to nearest Bragg peak for grid points.
    
    Args:
        A_inv: PyTorch tensor of shape (3, 3) with cell orthogonalization matrix
        hkl_grid: PyTorch tensor of shape (n_points, 3) with hkl indices
        
    Returns:
        PyTorch tensor of shape (n_points,) with distances
        
    References:
        - Original implementation: eryx/map_utils.py:get_dq_map
    """
    # Find closest integral hkl points using torch.round
    hkl_closest = torch.round(hkl_grid)
    
    # Convert to q-vectors
    q_closest = 2 * torch.pi * torch.matmul(A_inv.T, hkl_closest.T).T
    q_grid = 2 * torch.pi * torch.matmul(A_inv.T, hkl_grid.T).T
    
    # Compute distances using torch.norm
    dq = torch.norm(torch.abs(q_closest - q_grid), dim=1)
    
    # Round to specified precision (8 decimal places)
    # PyTorch doesn't have a direct equivalent to np.around with decimals
    # We can multiply by 10^8, round, then divide by 10^8
    scale = 1e8
    dq = torch.round(dq * scale) / scale
    
    return dq

def get_centered_sampling(map_shape: Tuple[int, int, int], 
                         sampling: Tuple[float, float, float]) -> List[Tuple[float, float, float]]:
    """
    Get sampling tuples for map centered about the origin.
    
    Args:
        map_shape: Tuple with map dimensions
        sampling: Tuple with fractional sampling rates
        
    Returns:
        List of sampling tuples for h, k, l dimensions
        
    References:
        - Original implementation: eryx/map_utils.py:get_centered_sampling
    """
    # Calculate extent for each dimension
    # This is a pure calculation that doesn't need tensors
    extents = [((map_shape[i] - 1) / sampling[i] / 2.0) for i in range(3)]
    
    # Create tuples for min, max, sampling rate
    return [(-extents[i], extents[i], sampling[i]) for i in range(3)]

def resize_map(new_map: torch.Tensor, 
              old_sampling: List[Tuple[float, float, float]], 
              new_sampling: List[Tuple[float, float, float]]) -> torch.Tensor:
    """
    Resize map if symmetrization changed dimensions.
    
    Args:
        new_map: PyTorch tensor of shape (dim_h, dim_k, dim_l) with map data
        old_sampling: List of (min, max, rate) tuples for original grid
        new_sampling: List of (min, max, rate) tuples for new map
        
    Returns:
        PyTorch tensor with resized map
        
    References:
        - Original implementation: eryx/map_utils.py:resize_map
    """
    # Define tolerance
    tol = 1e-6
    
    # Check sampling differences and crop if needed
    resized_map = new_map
    
    # Check and crop h dimension
    if abs(new_sampling[0][1] - old_sampling[0][1]) > tol:
        excise = int(torch.round(torch.tensor(2 * (new_sampling[0][1] - old_sampling[0][1]))))
        resized_map = resized_map[excise:-excise, :, :]
    
    # Check and crop k dimension
    if abs(new_sampling[1][1] - old_sampling[1][1]) > tol:
        excise = int(torch.round(torch.tensor(2 * (new_sampling[1][1] - old_sampling[1][1]))))
        resized_map = resized_map[:, excise:-excise, :]
    
    # Check and crop l dimension
    if abs(new_sampling[2][1] - old_sampling[2][1]) > tol:
        excise = int(torch.round(torch.tensor(2 * (new_sampling[2][1] - old_sampling[2][1]))))
        resized_map = resized_map[:, :, excise:-excise]
    
    return resized_map
</file>
<file path="./eryx/autotest_config.py" project="ptycho">
import os
import logging
from eryx.autotest.configuration import Configuration

# Create configuration with debug mode enabled
config = Configuration(debug=True, log_file_prefix="logs")

# Ensure log directory exists
os.makedirs(config.getLogFilePrefix(), exist_ok=True)
</file>
<file path="./eryx/__init__.py" project="ptycho">
"""
Eryx: A package for diffuse scattering simulation and analysis.

This package provides tools for simulating diffuse scattering from protein crystals,
with both NumPy and PyTorch implementations. The PyTorch implementation enables
gradient-based optimization of simulation parameters.
"""

# NumPy implementation
from eryx.models import OnePhonon, RigidBodyTranslations, LiquidLikeMotions, RigidBodyRotations
from eryx.scatter import compute_form_factors, structure_factors, structure_factors_batch
from eryx.base import compute_molecular_transform, compute_crystal_transform

# Import PyTorch implementations if available
try:
    import torch
    HAS_TORCH = True
    
    # Import PyTorch implementations with _torch suffix to avoid naming conflicts
    from eryx.models_torch import OnePhonon as OnePhonon_torch
    from eryx.models_torch import RigidBodyTranslations as RigidBodyTranslations_torch
    from eryx.models_torch import LiquidLikeMotions as LiquidLikeMotions_torch
    from eryx.models_torch import RigidBodyRotations as RigidBodyRotations_torch
    
    from eryx.scatter_torch import compute_form_factors as compute_form_factors_torch
    from eryx.scatter_torch import structure_factors as structure_factors_torch
    from eryx.scatter_torch import structure_factors_batch as structure_factors_batch_torch
    
    from eryx.base_torch import compute_molecular_transform as compute_molecular_transform_torch
    from eryx.base_torch import compute_crystal_transform as compute_crystal_transform_torch
    
    # Import adapters for convenience
    from eryx.adapters import PDBToTensor, GridToTensor, TensorToNumpy, ModelAdapters
    
except ImportError:
    HAS_TORCH = False

__version__ = "0.1.0"
</file>
<file path="./eryx/adapters.py" project="ptycho">
"""
Adapter components to bridge NumPy and PyTorch implementations.

This module contains adapter classes to convert between NumPy arrays and PyTorch tensors,
as well as domain-specific adapters for the diffuse scattering calculations.
All adapters preserve the computational graph for gradient backpropagation.
"""

import numpy as np
import torch
from typing import Tuple, List, Dict, Optional, Union, Any
import gemmi  # For crystallographic data structures

class PDBToTensor:
    """
    Adapter to convert PDB data from NumPy arrays to PyTorch tensors.
    
    This class handles the conversion of AtomicModel and related classes from
    the NumPy implementation to PyTorch tensors suitable for gradient-based calculations.
    """
    
    def __init__(self, device: Optional[torch.device] = None):
        """
        Initialize the adapter.
        
        Args:
            device: The PyTorch device to place tensors on
        """
        self.device = device or torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    def convert_atomic_model(self, model: Any) -> Dict[str, Any]:
        """
        Convert an AtomicModel to PyTorch tensors.
        
        Args:
            model: AtomicModel instance from eryx.pdb
            
        Returns:
            Dictionary containing PyTorch tensor versions of the model attributes
        """
        # TODO: Convert all NumPy arrays to PyTorch tensors
        # TODO: Preserve crystallographic information
        # TODO: Handle complex attributes like symmetry operations
        # TODO: Return a dictionary with all the converted tensors
        
        raise NotImplementedError("convert_atomic_model not implemented")
    
    def convert_crystal(self, crystal: Any) -> Dict[str, Any]:
        """
        Convert a Crystal object to PyTorch tensors.
        
        Args:
            crystal: Crystal instance from eryx.pdb
            
        Returns:
            Dictionary containing PyTorch tensor versions of the crystal attributes
        """
        # TODO: Convert relevant attributes to PyTorch tensors
        # TODO: Handle unit cell information
        # TODO: Preserve crystallographic metadata
        
        raise NotImplementedError("convert_crystal not implemented")
    
    def convert_gnm(self, gnm: Any) -> Dict[str, Any]:
        """
        Convert a GaussianNetworkModel to PyTorch tensors.
        
        Args:
            gnm: GaussianNetworkModel instance from eryx.pdb
            
        Returns:
            Dictionary containing PyTorch tensor versions of the GNM attributes
        """
        # TODO: Convert gamma matrix to tensor
        # TODO: Convert neighbor lists to tensor format
        # TODO: Preserve parameter information
        
        raise NotImplementedError("convert_gnm not implemented")
    
    def array_to_tensor(self, array: np.ndarray, requires_grad: bool = True) -> torch.Tensor:
        """
        Convert a NumPy array to a PyTorch tensor.
        
        Args:
            array: NumPy array to convert
            requires_grad: Whether the tensor requires gradients
            
        Returns:
            PyTorch tensor with the same data
        """
        if array is None:
            return None
            
        tensor = torch.from_numpy(array).to(self.device)
        tensor.requires_grad = requires_grad
        return tensor
    
    def convert_dict_of_arrays(self, dict_arrays: Dict[Any, np.ndarray], 
                              requires_grad: bool = True) -> Dict[Any, torch.Tensor]:
        """
        Convert a dictionary of NumPy arrays to PyTorch tensors.
        
        Args:
            dict_arrays: Dictionary mapping keys to NumPy arrays
            requires_grad: Whether tensors require gradients
            
        Returns:
            Dictionary mapping the same keys to PyTorch tensors
        """
        return {k: self.array_to_tensor(v, requires_grad) for k, v in dict_arrays.items()}

class GridToTensor:
    """
    Adapter to convert grid data from NumPy arrays to PyTorch tensors.
    
    This class handles the conversion of reciprocal space grids and related data
    from the NumPy implementation to PyTorch tensors.
    """
    
    def __init__(self, device: Optional[torch.device] = None):
        """
        Initialize the adapter.
        
        Args:
            device: The PyTorch device to place tensors on
        """
        self.device = device or torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    def convert_grid(self, q_grid: np.ndarray, map_shape: Tuple[int, int, int]) -> Tuple[torch.Tensor, Tuple[int, int, int]]:
        """
        Convert a grid of q-vectors to PyTorch tensor.
        
        Args:
            q_grid: NumPy array of shape (n_points, 3) with q-vectors
            map_shape: Tuple with 3D map shape
            
        Returns:
            Tuple containing:
                - PyTorch tensor of q-vectors
                - Tuple with map shape (unchanged)
        """
        # TODO: Convert q_grid to PyTorch tensor
        # TODO: Set requires_grad to True
        # TODO: Return tensor and shape
        
        raise NotImplementedError("convert_grid not implemented")
    
    def convert_mask(self, mask: np.ndarray) -> torch.Tensor:
        """
        Convert a boolean mask to PyTorch tensor.
        
        Args:
            mask: NumPy boolean array
            
        Returns:
            PyTorch boolean tensor
        """
        # TODO: Convert mask to PyTorch tensor
        # TODO: Ensure boolean dtype
        
        raise NotImplementedError("convert_mask not implemented")
    
    def convert_symmetry_ops(self, sym_ops: Dict[int, np.ndarray]) -> Dict[int, torch.Tensor]:
        """
        Convert symmetry operations to PyTorch tensors.
        
        Args:
            sym_ops: Dictionary mapping IDs to rotation matrices
            
        Returns:
            Dictionary mapping IDs to tensor rotation matrices
        """
        # TODO: Convert each symmetry operation matrix to tensor
        # TODO: Maintain dictionary structure
        
        raise NotImplementedError("convert_symmetry_ops not implemented")

class TensorToNumpy:
    """
    Adapter to convert PyTorch tensors back to NumPy arrays.
    
    This class handles the conversion of PyTorch tensors to NumPy arrays
    for visualization, saving, or compatibility with existing code.
    """
    
    def __init__(self):
        """
        Initialize the adapter.
        """
        pass
    
    def tensor_to_array(self, tensor: torch.Tensor) -> np.ndarray:
        """
        Convert a PyTorch tensor to a NumPy array.
        
        Args:
            tensor: PyTorch tensor to convert
            
        Returns:
            NumPy array with the same data
        """
        if tensor is None:
            return None
            
        if tensor.requires_grad:
            tensor = tensor.detach()
        
        return tensor.cpu().numpy()
    
    def convert_dict_of_tensors(self, dict_tensors: Dict[Any, torch.Tensor]) -> Dict[Any, np.ndarray]:
        """
        Convert a dictionary of PyTorch tensors to NumPy arrays.
        
        Args:
            dict_tensors: Dictionary mapping keys to PyTorch tensors
            
        Returns:
            Dictionary mapping the same keys to NumPy arrays
        """
        return {k: self.tensor_to_array(v) for k, v in dict_tensors.items()}
    
    def convert_intensity_map(self, intensity: torch.Tensor, map_shape: Tuple[int, int, int]) -> np.ndarray:
        """
        Convert an intensity map tensor to a NumPy array.
        
        Args:
            intensity: PyTorch tensor with intensity values
            map_shape: Tuple with desired 3D shape
            
        Returns:
            NumPy array with intensity map reshaped to 3D
        """
        # TODO: Detach tensor if it requires gradients
        # TODO: Convert to CPU NumPy array
        # TODO: Reshape to 3D if necessary
        
        raise NotImplementedError("convert_intensity_map not implemented")

class ModelAdapters:
    """
    Adapters for the various model classes in eryx.
    
    This class contains methods to convert between the NumPy and PyTorch
    versions of the various model classes used in diffuse scattering calculations.
    """
    
    def __init__(self, device: Optional[torch.device] = None):
        """
        Initialize the adapters.
        
        Args:
            device: The PyTorch device to place tensors on
        """
        self.device = device or torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.pdb_to_tensor = PDBToTensor(device)
        self.grid_to_tensor = GridToTensor(device)
        self.tensor_to_numpy = TensorToNumpy()
    
    def adapt_one_phonon_inputs(self, np_model: Any) -> Dict[str, Any]:
        """
        Adapt inputs for the OnePhonon model from NumPy to PyTorch.
        
        Args:
            np_model: OnePhonon instance from eryx.models
            
        Returns:
            Dictionary with PyTorch tensor versions of inputs
        """
        # TODO: Extract necessary inputs
        # TODO: Convert to PyTorch tensors
        # TODO: Structure for easy passing to PyTorch implementation
        
        raise NotImplementedError("adapt_one_phonon_inputs not implemented")
    
    def adapt_one_phonon_outputs(self, torch_outputs: Dict[str, torch.Tensor]) -> np.ndarray:
        """
        Adapt outputs from the PyTorch OnePhonon model back to NumPy.
        
        Args:
            torch_outputs: Dictionary with PyTorch tensor outputs
            
        Returns:
            NumPy array with intensity map
        """
        # TODO: Extract intensity map
        # TODO: Convert to NumPy array
        # TODO: Reshape if necessary
        
        raise NotImplementedError("adapt_one_phonon_outputs not implemented")
    
    def adapt_rigid_body_translations_inputs(self, np_model: Any) -> Dict[str, Any]:
        """
        Adapt inputs for the RigidBodyTranslations model from NumPy to PyTorch.
        
        Args:
            np_model: RigidBodyTranslations instance from eryx.models
            
        Returns:
            Dictionary with PyTorch tensor versions of inputs
        """
        # TODO: Extract necessary inputs
        # TODO: Convert to PyTorch tensors
        # TODO: Structure for easy passing to PyTorch implementation
        
        raise NotImplementedError("adapt_rigid_body_translations_inputs not implemented")
    
    # Add similar methods for other model classes
</file>
