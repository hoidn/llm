<file path="./misc/errorspec.md" project="">
# Error Type Hierarchy and Handling Design

## Purpose
Define the core error types that enable control flow and task adaptation in the intelligent task execution system.

## Error Categories

### 1. Resource Exhaustion
- **Purpose**: Signal when system resource limits are exceeded
- **Characteristics**:
  - Parameterized resource type and limits (e.g., turns, context window)
  - Clear threshold for triggering
  - No partial results preserved
- **Control Flow Impact**: 
  - Signals that task requires more resources than available

### 2. Task Failure
- **Purpose**: Signal that a task cannot be completed as attempted
- **Characteristics**:
  - Generic failure mechanism
  - No internal categorization
  - No partial results preserved
- **Control Flow Impact**:
  - Task terminates
  - Control returns to parent task/evaluator

## Error Handling Principles

### 1. Separation of Concerns
- Errors purely signal failure conditions
- No recovery logic in error objects
- No state/progress tracking
- No partial results

### 2. Control Flow
- Resource Exhaustion → Task too large
- Task Failure → Termination
- No retry logic in components

### 3. Context Independence  
- Errors do not carry execution state.
- No partial results in errors.
- Clean separation from context management.

### Missing Argument Handling
When a task attempts to resolve an input, the evaluator first performs template substitution on any placeholders in the form `{{variable_name}}` within the task definition—using values from its current lexical environment. Additionally, if an `<input>` element specifies a `from` attribute, the evaluator binds that input using the value associated with that environment variable. If a required binding is missing, the evaluator returns a standard `TASK_FAILURE` error with a message such as "Missing required input: variable_name."

## Integration Points

All components in the unified task-execution system use the same generic error signaling.
In particular:
 - The Task System (and its unified execution component) detects resource exhaustion and signals a generic `TASK_FAILURE` with attached metadata.
 - The Memory System continues to provide context without carrying error state.

## Design Decisions & Rationale

1. Minimal Error Categories
   - Only essential control flow signals
   - Clear mapping to system behaviors
   - Simplified error handling

2. Stateless Error Design
   - Separates control flow from state
   - Clean component boundaries
   - Simplified recovery

3. No Complex Recovery
   - Decomposition as consequence not strategy
   - Simplified control flow
   - Clear system behavior

## Context Operation Failures

In scenarios where a task step calls `MemorySystem.getRelevantContextFor()` (or otherwise attempts context assembly), any failure is reported as a standard `TASK_FAILURE`.
Partial results are not preserved; on any failure, intermediate data is discarded.

In short, "context operation failures" are reported solely as `TASK_FAILURE`, and no partial sub-task outputs are retained.

## Dependencies
- Task system must detect resource limits

## Script Execution Errors
Script execution errors (e.g. non-zero exit codes) are captured and passed along to the evaluator for downstream decision-making rather than causing an immediate task failure.
- Evaluator must handle control flow
- Memory system must maintain context
</file>
<file path="./misc/operators.md" project="">
# Sequential and Reduce Operator Specification

## Purpose
Define the structure and semantics of Sequential and Reduce operators for task composition and execution, specifying XML schemas and execution behaviors.

## Memory Structure
```typescript
shortTermMemory: {
    files: Map<string, WorkingFile>;
    dataContext: string;
}
```

## Sequential Operator

### Purpose
Execute a series of tasks with explicit dependencies. Maintains execution order while allowing parallel execution of independent inputs.

### Structure
```xml
<task type="sequential">
    <description>Analyze {{dataset_name}} using provided configuration</description>
    <context_management>
        <inherit_context>none</inherit_context>
        <accumulate_data>true</accumulate_data>
        <accumulation_format>notes_only</accumulation_format>
    </context_management>
    <steps>
        <task>
            <description>Load initial data</description>
            <inputs>
                <input name="data" from="raw_data"/>
            </inputs>
        </task>
        <task>
            <description>Apply configuration from {{config_profile}}</description>
            <inputs>
                <input name="config" from="default_config"/>
            </inputs>
        </task>
    </steps>
</task>
```

### Context Management

#### Operator Default Settings

Each operator type has specific default context management settings that apply when no explicit configuration is provided:

| Operator Type | inherit_context | accumulate_data | accumulation_format | fresh_context |
|---------------|-----------------|-----------------|---------------------|---------------|
| atomic        | full            | false           | notes_only          | enabled       |
| sequential    | full            | true            | notes_only          | enabled       |
| reduce        | none            | true            | notes_only          | enabled       |
| script        | full            | false           | notes_only          | disabled      |
| director_evaluator_loop | none  | true            | notes_only          | enabled       |

#### Context Management Dimensions

The system uses a standardized three-dimensional context management model:

1. **inherit_context**: An enumeration with allowed values:
   - **full** – the full parent context is passed unchanged
   - **none** – no parent context is inherited
   - **subset** – only a subset (as determined by task-specific rules) is inherited

2. **accumulate_data**: A boolean controlling whether outputs from prior steps are accumulated:
   - **true** – previous step outputs are accumulated
   - **false** – no accumulation of step outputs

3. **accumulation_format**: When accumulating data, specifies the storage format:
   - **notes_only** – only summary information is preserved
   - **full_output** – complete step outputs are preserved

4. **fresh_context**: Controls whether new context is generated via associative matching:
   - **enabled** – fresh context is generated
   - **disabled** – no fresh context is generated

#### Context Management Override

These dimensions can be explicitly configured through a standardized XML structure:

```xml
<context_management>
    <inherit_context>full|none|subset</inherit_context>
    <accumulate_data>true|false</accumulate_data>
    <accumulation_format>notes_only|full_output</accumulation_format>
    <fresh_context>enabled|disabled</fresh_context>
</context_management>
```

When the `<context_management>` block is present, its settings override the operator defaults. When it's omitted, the operator-specific defaults apply. This hybrid approach provides both consistency and flexibility.

Settings are merged during template loading, with explicit settings taking precedence over defaults. This ensures that task authors can rely on sensible defaults while still having the ability to customize context behavior when needed.

**Note:** Partial results are now preserved when subtasks fail, with the format determined by the `accumulation_format` setting.

### Execution Semantics
- Tasks execute in specified order
- For tasks with multiple inputs:
  - All input tasks execute in parallel
  - Parent task executes after all inputs complete
- Execution fails if:
  - Required task structure is missing/invalid
  - Any task execution fails (with failure context indicating which task)

## Reduce Operator

### Purpose
Process a list of named inputs through repeated application of inner task and reduction operations.

### Structure
```xml
<task type="reduce">
    <description>Reduction operation description</description>
    <initial_value>
        <!-- Initial accumulator value -->
    </initial_value>
    <inputs>
        <input name="dataset1">Value 1</input>
        <input name="dataset2">Value 2</input>
        <input name="dataset3">Value 3</input>
    </inputs>
    <inner_task>
        <description>Processing for each input</description>
        <inputs>
            <input name="current_data">
                <!-- Current input being processed -->
            </input>
            <input name="metadata">
                <!-- Additional input needed for processing -->
                <task>
                    <description>Load metadata for processing</description>
                </task>
            </input>
        </inputs>
    </inner_task>
    <reduction_task>
        <description>Combine current result with accumulator</description>
        <inputs>
            <input name="current_result">
                <!-- Result from inner_task -->
            </input>
            <input name="accumulator">
                <!-- Current accumulated value -->
            </input>
            <input name="original_input">
                <!-- Original input being processed -->
            </input>
        </inputs>
    </reduction_task>
</task>
```

### Execution Semantics
- For each named input:
  1. Execute inner_task with:
     - Current input
     - Any additional specified inputs
     - When both inheritance and accumulation are enabled in a reduce operator, a basic inheritance model is used without merging accumulated outputs. Advanced dual-context tracking is deferred to future iterations.
  2. Execute reduction_task with:
     - Current inner_task result
     - Current accumulator value
     - Original input
  3. Result becomes new accumulator value
- Maintains strict ordering of input processing
- Context changes managed by memory system
- Execution fails if:
  - Required task structure is missing/invalid 
  - Any inner_task execution fails (with failure context indicating which input)
  - Any reduction_task execution fails (with failure context indicating current state)

## Integration Points

### With Memory System
- System maintains execution context via shortTermMemory
- Files and data context available to all tasks
- Context changes managed by memory system, not tasks

### With Task System
- Responsible for generating valid XML
- Manages task decomposition on failure
- Handles task library matching

## Dependencies
- Error types defined in errorspec.md
- Memory system must handle context
- Task system must support XML generation

## Constraints
- Tasks cannot modify context directly
- XML structure must encode all input dependencies
- All inputs must have unique names within their scope
- Inner tasks can specify multiple inputs

## Subtask Spawning Integration

The Task System supports dynamic subtask spawning through a standardized mechanism:

```typescript
interface SubtaskRequest {
  // Required fields
  type: TaskType;                      // Type of subtask to spawn
  description: string;                 // Description of the subtask
  inputs: Record<string, any>;         // Input parameters for the subtask
  
  // Optional fields
  template_hints?: string[];           // Hints for template selection
  context_management?: {               // Override default context settings
    inherit_context?: 'full' | 'none' | 'subset';
    accumulate_data?: boolean;
    accumulation_format?: 'notes_only' | 'full_output';
    fresh_context?: 'enabled' | 'disabled';
  };
  max_depth?: number;                  // Override default max nesting depth
  subtype?: string;                    // Optional subtype for atomic tasks
}
```

### Context Management Defaults

Subtasks have specific default context management settings:

| Setting | Default Value | Description |
|---------|---------------|-------------|
| inherit_context | subset | Inherits only relevant context from parent |
| accumulate_data | false | Does not accumulate previous step outputs |
| accumulation_format | notes_only | Stores only summary information |
| fresh_context | enabled | Generates new context via associative matching |

These defaults can be overridden through explicit configuration in the SubtaskRequest.

### Data Flow

Subtask spawning uses direct parameter passing rather than environment variables:

1. Parent task returns with `status: "CONTINUATION"` and a `subtask_request` in its notes
2. System validates the request and selects an appropriate template
3. Subtask executes with inputs from the request
4. Subtask result is passed back to the parent task when execution resumes

This approach ensures clear data dependencies and improves debug visibility.

### Depth Control

To prevent infinite recursion and resource exhaustion:

1. **Maximum Nesting Depth**: Default limit of 5 levels of nested subtasks
2. **Cycle Detection**: Prevention of tasks spawning identical subtasks
3. **Resource Tracking**: Monitoring of total resource usage across the subtask chain
4. **Timeout Enforcement**: Overall time limits for the complete subtask chain

These mechanisms ensure that subtask spawning remains controlled and resource-efficient.
</file>
<file path="./process.md" project="">
# Architecture Enhancement Process

This document outlines the process for addressing architectural gaps, ambiguities, or needed enhancements in the system. It captures the methodology we used for the Output Standardization feature and can serve as a template for future architectural work.

## 1. Gap Identification

**Input**: Prioritized list of architectural issues or enhancements
**Output**: Clear understanding of the gap and its impact

We began with Output Standardization identified as a high-priority architectural gap from our prioritized list. The gap was specific: the system lacked a standardized approach for tasks to return structured data (like lists or objects), creating issues with variable binding, type ambiguity, and task composition.

## 2. Initial Approach Exploration

**Input**: Architectural gap description
**Output**: First draft of potential solution approaches

In this phase, we created an initial approach document outlining potential solutions to the Output Standardization issue. This included:
- Key design decisions that needed to be made
- Potential XML syntax for structured outputs
- Basic requirements for JSON parsing and validation
- Integration points with existing architecture

## 3. Clarification of Design Decisions

**Input**: Initial approach document
**Output**: Specific questions requiring architectural decisions

Not all design decisions have obvious answers. For Output Standardization, several key questions emerged:
- Should all outputs use JSON or only those needing structured data?
- How should JSON outputs relate to the existing TaskResult structure?
- Should outputs have formal schema declarations?
- How should backward compatibility be handled?
- What error handling mechanism is appropriate?

## 4. Stakeholder Input on Design Decisions

**Input**: Design decision questions
**Output**: Clear direction on architectural choices

For Output Standardization, specific decisions were made:
- JSON would be optional, focused on tasks needing structured data
- Structured output would live in the existing content field
- Schema declaration would be kept lightweight to avoid over-complication
- Backward compatibility was required
- Error handling needed further exploration

## 5. Comprehensive Approach Documentation

**Input**: Design decisions and architectural requirements
**Output**: Detailed approach document

We created a detailed document outlining:
- Core approach and rationale
- XML template syntax extensions
- JSON detection and parsing algorithm
- Variable binding mechanism
- Integration with other features (e.g., function-based templates)
- Error handling strategy
- Implementation priorities
- Example usage patterns

This document served as a proposal rather than a final decision.

## 6. Approach Review and Documentation Strategy

**Input**: Detailed approach document
**Output**: Review feedback and documentation plan

The approach document underwent review focusing on:
- Alignment with architectural principles
- Technical feasibility
- Potential issues or gaps

For Output Standardization, we determined a two-phase documentation approach:
1. Create an Architecture Decision Record (ADR) to capture the decision
2. Later propagate changes to component-specific documentation

## 7. ADR Creation

**Input**: Reviewed approach and documentation strategy
**Output**: Draft Architecture Decision Record

We drafted a formal ADR following the project's standard ADR template:
- Context (problem statement)
- Decision (solution approach)
- Consequences (positive and negative impacts)
- Implementation guidance
- Related decisions
- Affected documentation

The ADR formalized the design decisions and provided technical implementation details.

## 8. ADR Review

**Input**: Draft ADR
**Output**: Review comments focusing on clarity and potential issues

The ADR underwent critical review focusing on:
- Clarity and freedom from ambiguity
- Potential over-engineering
- Premature design decisions
- Integration with existing architecture
- Implementation feasibility

For Output Standardization, the review identified areas of over-engineering in the schema validation system and unnecessary complexity in error handling.

## 9. Simplification Analysis

**Input**: ADR review comments
**Output**: Specific simplification recommendations

Based on the review, we identified specific areas for simplification:
- Schema validation could be reduced to basic type checking
- TaskResult extension could be minimized
- Error handling could be simplified
- XML syntax could be streamlined

This followed the principle: "Start simple, add complexity only when needed."

## 10. ADR Refinement

**Input**: Simplification recommendations
**Output**: Revised, simplified ADR

We revised the ADR to focus on core functionality:
- Reduced schema complexity to basic type validation
- Simplified TaskResult interface changes
- Streamlined error handling
- Maintained a clear path for future extensions
- Added an explicit "Future Extensions" section highlighting deferred complexity

The refined ADR maintained the core value proposition while reducing implementation complexity.

## 11. Final Review and Approval

**Input**: Revised ADR
**Output**: Approved architectural direction

The final step is reviewing the revised ADR and obtaining approval for the architectural direction. This includes:
- Confirming alignment with architectural principles
- Validating that simplifications maintain core functionality
- Ensuring integration points are well defined
- Approving implementation priority

## 12. Documentation Updates

**Input**: Approved ADR
**Output**: Updated component documentation

After ADR approval, changes must be propagated to affected documentation:
- Update contract documents with new interfaces
- Update component documentation with implementation guidance
- Update schema definitions with new elements
- Add examples demonstrating the new capability

## Key Principles

Throughout the Output Standardization process, several principles guided our work:

1. **Start Simple**: Begin with minimal implementations; add complexity only when needed.

2. **Question Complexity**: Regularly challenge whether proposed solutions are over-engineered.

3. **Maintain Compatibility**: Ensure changes don't break existing functionality.

4. **Look for Patterns**: Align new features with existing architectural patterns.

5. **Document Decisions**: Capture the "why" behind decisions, not just the "what."

6. **Defer When Uncertain**: If a feature isn't clearly needed now, defer it to future iterations.

7. **Review Critically**: Apply constructive criticism to your own work and encourage others to do the same.

## Process Variations

This process can be adapted based on the nature of the architectural enhancement:

- For smaller changes, some steps can be combined or streamlined
- For more complex changes, additional review cycles may be needed
- For urgent issues, temporary solutions might be documented separately from long-term architectural direction

## Conclusion

The iterative process outlined above allowed us to address the Output Standardization gap in a methodical way that balanced immediate needs with long-term architectural health. By starting with an exploration of approaches, getting stakeholder input, creating detailed documentation, conducting critical reviews, and simplifying where possible, we arrived at a clean solution that adds significant value without unnecessary complexity.
</file>
<file path="./plans/atomic_task_subtypes.md" project="">
1. Type Hierarchy Recommendation

mermaid
Copy
graph TD
    Task[Base Task Type]
    Task --> Atomic
    Task --> Sequential
    Task --> Reduce
    Atomic --> Director
    Atomic --> Evaluator
    classDef atomic fill:#cff,stroke:#099;
    class Director,Evaluator atomic
2. XML Implementation Strategy

xml
Copy
<!-- As atomic subtypes -->
<task type="atomic" subtype="director">
    <continuation_policy>latest-only</continuation_policy>
    <output_slot>last_eval_input</output_slot>
</task>

<task type="atomic" subtype="evaluator">
    <input_source>last_eval_input</input_source>
    <validation_rules>strict</validation_rules>
</task>

<!-- Example using template substitution for input binding -->
<task type="atomic">
    <description>Process {{input_data}} with parameters from {{config}}</description>
    <inputs>
        <input name="data" from="input_data"/>
        <input name="settings" from="config"/>
    </inputs>
</task>
Run HTML
3. Advantages of Atomic Subtyping

Aspect	Benefit
Execution Flow	Inherits standard atomic task lifecycle (init → execute → cleanup)
Error Handling	Uses existing atomic error recovery patterns
Resource Tracking	Leverages atomic task's turn counting & context management
Template Matching	Works with current associative matching system
4. Context Management Additions

typescript
Copy
interface AtomicTaskEnv {
    // Shared atomic properties
    turnsUsed: number;
    contextWindowUsage: number;
    
    // Director-Evaluator specific
    lastEvaluation?: {
        output: string;
        timestamp: Date;
        success: boolean;
    };
}
5. Implementation Requirements

a. Subtype Registration

typescript
Copy
// In task-system initialization
registerAtomicSubtype('director', {
    continuationHandler: handleDirectorContinuation,
    maxTurns: 10 // Special limit for directors
});

registerAtomicSubtype('evaluator', {
    outputValidation: strictJSONValidator,
    autoClearContext: true
});
b. Execution Flow Modifications

Copy
[Director Task]
1. Execute as normal atomic task
2. On CONTINUATION:
   a. Store output in lastEvaluation slot
   b. Clear other context variables
   c. Pass control to Evaluator subtype

[Evaluator Task]
1. Receive lastEvaluation data as input
2. Execute validation/processing
3. Return results to parent environment
4. Auto-clear lastEvaluation slot
6. XML Schema Changes (operators.md update)

diff
Copy
<xs:element name="task">
  <xs:complexType>
    <xs:attribute name="type" use="required">
      <xs:simpleType>
        <xs:restriction base="xs:string">
          <xs:enumeration value="atomic"/>
          <xs:enumeration value="sequential"/>
          <xs:enumeration value="reduce"/>
+         <xs:enumeration value="script"/>
        </xs:restriction>
      </xs:simpleType>
    </xs:attribute>
+   <xs:attribute name="subtype">
+     <xs:simpleType>
+       <xs:restriction base="xs:string">
+         <xs:enumeration value="director"/>
+         <xs:enumeration value="evaluator"/>
+       </xs:restriction>
+     </xs:simpleType>
+   </xs:attribute>
  </xs:complexType>
</xs:element>
7. Transition Plan

Phase 1: Add subtype support to atomic tasks

Phase 2: Migrate existing director/evaluator templates

Phase 3: Deprecate top-level director/evaluator types

Phase 4: Update documentation (errorspec.md, operators.md)

## Implementation Implications

The atomic subtyping approach provides precise technical advantages:

- **Execution Model Integration**: Subtypes inherit the established turn tracking, resource management, and cleanup protocols already defined for atomic tasks
- **Architecture Simplification**: Reduces duplicated code by keeping atomic behavior in a single class hierarchy rather than implementing separate task types
- **Integration Cost Reduction**: Requires only minimal changes to the XML schema (adding the `subtype` attribute) rather than introducing new top-level element types
- **Template System Compatibility**: Existing template matching logic continues to work without modification

This approach balances specialization needs with architectural consistency, while maintaining the simplified context model from previous planning.
</file>
<file path="./plans/general_improvements.md" project="">
# Architecture Analysis and Recommendations

## Key Inconsistencies and Gaps

### 1. Context Management Ambiguity - RESOLVED

#### ~~Current Issues~~
- ~~Unclear distinction between how `inherit_context` and `accumulate_data` interact in reduce operations~~
- ~~Inconsistent handling of dual-context tracking across different operators~~
- ~~Ambiguous mapping between XML schema and AST structure for context attributes~~

#### Resolution
This issue has been resolved in ADR 14 (Operator Context Configuration) by implementing a hybrid configuration approach with operator-specific defaults and explicit overrides:

1. Each operator type has specific default settings:
   | Operator Type | inherit_context | accumulate_data | accumulation_format | fresh_context |
   |---------------|-----------------|-----------------|---------------------|---------------|
   | atomic        | full            | false           | notes_only          | enabled       |
   | sequential    | full            | true            | notes_only          | enabled       |
   | reduce        | none            | true            | notes_only          | enabled       |
   | script        | full            | false           | notes_only          | disabled      |
   | director_evaluator_loop | none  | true            | notes_only          | enabled       |

2. These defaults apply when no explicit context_management block is provided
3. When present, explicit settings override the defaults
4. This hybrid approach provides both consistency and flexibility

#### Context Flow
The standardized context flow model remains valid:
   ```
   Parent Context -> Child Task
     │
     ├─ inherit_context: none  -> Fresh context only
     ├─ inherit_context: full  -> Complete parent context
     └─ inherit_context: subset -> Filtered via associative matching
   ```

### 2. Resource Management Gaps

#### Current Issues
- Unclear ownership of resource tracking between Handler and Memory System
- Inconsistent handling of partial results during resource exhaustion
- Ambiguous cleanup responsibilities

#### Recommendations
1. Define strict resource boundaries:
   - Handler: Turn counting, context window, execution resources
   - Memory System: File metadata only
   - Task System: Template and execution coordination

2. Standardize cleanup protocol:
   ```
   Task Completion/Failure
     ├─ Handler: Clean session state
     ├─ Memory System: Clear transient context
     └─ Task System: Release template resources
   ```

### 3. Error Handling Inconsistencies

#### Current Issues
- Mixed approaches to context generation failures
- Unclear recovery strategies for different error types
- Inconsistent error propagation across components

#### Recommendations
1. Standardize error taxonomy:
   ```
   Error Types
     ├─ RESOURCE_EXHAUSTION
     │   ├─ turns
     │   ├─ context
     │   └─ output
     ├─ TASK_FAILURE
     │   ├─ execution
     │   ├─ validation
     │   └─ progress
     └─ CONTEXT_FAILURE (new)
         ├─ generation
         └─ inheritance
   ```

2. Define clear recovery paths for each error type

### 4. Script Execution Integration

#### Current Issues
- Ambiguous handling of script execution in Director-Evaluator pattern
- Unclear error propagation from script execution
- Missing standardization of script execution environment

#### Recommendations
1. Formalize script execution model:
   ```xml
   <task type="script">
     <description>Execute target script</description>
     <inputs>
       <input name="script_input" from="director_output"/>
     </inputs>
     <execution>
       <command>{{script_path}}</command>
       <timeout>300</timeout>
       <environment inherit="none"/>
     </execution>
   </task>
   ```

2. Standardize script execution results:
   - Capture stdout, stderr, exit code
   - Define error handling strategy
   - Specify environment inheritance rules

### 5. Memory System Interface Ambiguity

#### Current Issues
- Inconsistent version references (2.0 vs 3.0)
- Unclear boundaries between file operations and metadata management
- Mixed approaches to context retrieval

#### Recommendations
1. Standardize Memory System interface:
   ```typescript
   interface MemorySystem {
     // Core operations
     getGlobalIndex(): Promise<GlobalIndex>;
     updateGlobalIndex(index: GlobalIndex): Promise<void>;
     
     // Context operations
     getRelevantContextFor(input: ContextGenerationInput): Promise<AssociativeMatchResult>;
     
     // No file operations - delegated to Handler tools
   }
   ```

2. Clear separation of responsibilities:
   - Memory System: Metadata and context management
   - Handler Tools: File operations
   - Task System: Execution coordination

## Implementation Priorities

### Phase 1: Core Architecture Alignment
1. Standardize Memory System interface (version 3.0)
2. Implement clear context management rules
3. Define strict resource boundaries
4. Update error taxonomy

### Phase 2: Feature Implementation
1. Enhanced context inheritance
2. Script execution integration
3. Error recovery paths
4. Resource cleanup protocols

### Phase 3: Documentation Updates
1. Update cross-references to Memory System 3.0
2. Document context inheritance rules
3. Define script execution guidelines
4. Standardize error handling documentation

## Recommendations for Open Questions

1. **Context Generation Failures**
   - Treat as distinct error type (CONTEXT_FAILURE)
   - Preserve partial results for potential recovery
   - Allow task-specific handling via template

2. **Operator Inheritance**
   - Extend `inherit_context` to all operators
   - Define operator-specific inheritance rules
   - Document interaction with accumulation

3. **Subtask Spawning**
   - Implement through CONTINUATION status
   - Use notes section for spawn parameters
   - Define clear parent-child relationship

## Additional Considerations

### 1. Environment Model
```typescript
interface Environment {
    bindings: Map<string, any>;
    context: Map<string, any>;
    taskLibrary: TaskLibrary;
    
    extend(bindings: Map<string, any>): Environment;
    find(name: string): any;
    cleanup(): void;
}
```

### 2. Task Library Organization
```typescript
interface TaskLibrary {
    // Core task types
    atomic: Map<string, TaskDefinition>;
    sequential: Map<string, TaskDefinition>;
    reduce: Map<string, TaskDefinition>;
    
    // Special types
    associativeMatch: Map<string, TaskDefinition>;
    scriptExecution: Map<string, TaskDefinition>;
    
    // Operations
    register(task: TaskDefinition): void;
    find(type: string, criteria: string): TaskDefinition[];
    get(id: string): TaskDefinition;
}
```

### 3. Context Management Rules
```typescript
interface ContextManagement {
    inheritContext: 'none' | 'full' | 'subset';
    accumulateData: boolean;
    accumulationFormat: 'notes_only' | 'full_output';
    
    // New fields
    clearOnContinuation: boolean;
    preserveOnError: boolean;
}
```

## Next Steps

1. **Immediate Actions**
   - Finalize Memory System 3.0 interface
   - Update all cross-references
   - Implement standardized context management
   - Define error taxonomy

2. **Short-term Goals**
   - Implement script execution integration
   - Update documentation
   - Add error recovery paths
   - Standardize cleanup protocols

3. **Long-term Considerations**
   - Agent features
   - Multi-LLM support
   - Enhanced error recovery
   - Performance optimization
</file>
<file path="./inconsistencies.md" project="">
# Inconsistencies Requiring Subjective Choices

Below are areas where the documents highlight open design questions or competing approaches. Each needs a decision to remove ambiguity.

## Partial-Results Policy for Failing Operators

Options:
- A: Discard partial outputs on sub‐task failure, treating the entire operator as a single unit of success/failure.
- B: Preserve partial outcomes (e.g. in TaskResult.notes.partialResults) so higher‐level tasks can decide how to handle them.

Recommendation: If your use cases frequently require partial data (e.g., some parallel tasks succeed while others fail), choose (B) and store partial results. If the system rarely benefits from partial data, keep (A) for simplicity.

## Context-Generation Failure as Its Own Error

Options:
- A: New error type (CONTEXT_GENERATION_FAILURE) in errorspec.md, distinctly recognized so that tasks can handle it differently than normal "task failures."
- B: Keep a single TASK_FAILURE category and rely on message or reason to identify context errors.

Recommendation: If you plan to handle context failures with specialized fallback or re-tries, choose (A). If context issues are not special in your system's eyes, keep (B) to avoid error-type proliferation.

## Inherited Context for Map and Reduce - RESOLVED

This issue has been resolved in ADR 14 (Operator Context Configuration). All operators now support the `<inherit_context>` setting with consistent semantics, following the hybrid configuration approach with operator-specific defaults:

| Operator Type | inherit_context | accumulate_data | accumulation_format | fresh_context |
|---------------|-----------------|-----------------|---------------------|---------------|
| atomic        | full            | false           | notes_only          | enabled       |
| sequential    | full            | true            | notes_only          | enabled       |
| reduce        | none            | true            | notes_only          | enabled       |
| script        | full            | false           | notes_only          | disabled      |
| director_evaluator_loop | none  | true            | notes_only          | enabled       |

These defaults apply when no explicit context_management block is provided. When present, explicit settings override the defaults, providing both consistency and flexibility.

## Handler Tools API (Read-Only vs. Read/Write)

Question: "Do tasks need to modify or delete files, or is read-only enough?"

Options:
- A: Provide readFile, writeFile, deleteFile from the start.
- B: Provide only readFile, and mention a future extension for writes.

Recommendation: Decide based on real usage. If you have no short-term need for writes, option (B) keeps it simpler, adding write methods later if necessary.

## Summary

To maintain consistency going forward:
1. Pick one approach for partial results behavior
2. Choose error taxonomy for context generation failures
3. Decide on context inheritance scope
4. Finalize Handler tools API requirements
5. Document these decisions clearly to prevent future ambiguity
</file>
<file path="./docstructure.md" project="">
# Documentation Structure

## Directory Layout
```
docs/
├── system/                         # System-level documentation
│   ├── README.md                   # System overview & development sequence
│   ├── docs-guide.md               # Documentation standards, map & navigation
│   ├── architecture/               # Core architecture
│   │   ├── overview.md            # High-level design
│   │   ├── decisions/             # Architecture Decision Records (ADRs)
│   │   └── patterns/              # Core patterns & principles
│   ├── protocols/                  # System-wide protocols
│   │   └── resources.md            # Protocol-level resource definitions
│   └── contracts/                  # System-wide contracts
│       ├── interfaces.md          # External interfaces
│       └── resources.md           # Resource management
│
└── components/                     # Component documentation
    └── [component]/               # Per component (can be nested)
        ├── README.md              # Component overview
        ├── api/                   # Public API documentation
        │   └── interfaces.md      # Public interface definitions
        ├── spec/                  # Formal specifications
        │   ├── requirements.md    # Component requirements
        │   ├── interfaces.md      # Internal interface definitions
        │   ├── types.md          # Type definitions
        │   └── behaviors.md       # Expected behaviors
        └── impl/                  # Implementation details
            ├── design.md         # Design decisions
            ├── protocols.md      # Protocol implementations
            └── examples.md       # Implementation examples
```

## Common Types
[Rest of common types section remains unchanged]

## Cross-Reference Updates
Ensure that all components referencing the Director-Evaluator pattern mention both the dynamic and static variants. In particular, update references to [Pattern:DirectorEvaluator:1.1] to include the static variant with script execution support.

## Document Standards

### System Level Documents

#### README.md
[README.md template remains unchanged]

#### docs-guide.md
```markdown
# Documentation Guide

## Documentation Map
Required:
- Core system specifications
- Component contracts & integration
- Core patterns & protocols
- Key data structures
- Documentation status
- Cross-reference guide

## Standards
Required:
- Writing style guidelines
- Document templates
- Cross-reference syntax
- Version control practices

## Documentation Principles
Required:
1. Single Responsibility
   - Each document covers one concern
   - Clear boundaries between concerns
   - Explicit dependencies
2. Template Consistency
   - All new task templates must use the unified template substitution mechanism.
   - Evaluator's lexically scoped variables are referenced using the `{{variable_name}}` syntax.
   - Input bindings can be explicitly declared using the optional `from` attribute on `<input>` elements.

2. Contract Completeness
   - All requirements stated
   - All guarantees explicit
   - All resources documented

3. Resource Clarity
   - Ownership explicit
   - Lifecycle documented
   - Cleanup requirements specified

## Writing Style
Required:
1. Active voice
2. One sentence per line
3. Explicit section numbering
4. Consistent terminology

## Documentation Map
Required:
- Core system specifications
- Component contracts & integration
- Core patterns & protocols
- Key data structures
- Documentation status

## Version Management
Required:
1. Version Format: MAJOR.MINOR.PATCH
2. Update Rules:
   - MAJOR: Breaking changes
   - MINOR: New features, backward compatible
   - PATCH: Bug fixes, backward compatible

## Navigation
Required:
1. Documentation Map Usage
   - Primary navigation aid
   - Definitive specification locations
   - Cross-reference resolution
   - Documentation status tracking

2. Map Maintenance
   - Regular updates with changes
   - Validation of references
   - Status accuracy verification
   - Gap identification

Note: The Documentation Map in this guide serves as the primary navigation aid
for finding specifications and understanding document relationships.

## Templates
[See individual document templates in this guide]
```

[Rest of document remains unchanged]
</file>
<file path="./progress.md" project="">
# Progress Report for ADRs

Below is the completion level (0-100%) for each ADR as reflected in the current system documentation:

- decisions/completed/010-evaluator-director.md: 100%
- decisions/removed.md: 100%
- decisions/14-operator-ctx-config.md: 100%
- decisions/12-function-based-templates.md: 95%
- decisions/13-json-output.md: 100%
- decisions/9-partial-results.md: 100%
- decisions/8-errors.md: 90%
- decisions/needs_update/003-memory-context-update.md: 100%
- decisions/needs_update/001-memory-system.md: 80%
- decisions/needs_update/004-sequential-context-management.md: 85%
- decisions/11-subtask-spawning.md: 100%
</file>
<file path="./system/contracts/resources.md" project="">
# Resource Management Contracts [Contract:Resources:1.0]

## 1. Resource Types

### 1.1 Turn Counter
**Implementation**: [Component:Handler:1.0]  
**Interface**: [Interface:Handler:ResourceMonitoring:1.0]

The turn counter implements:
- Atomic per-session counter incremented with each LLM interaction
- Non-shared counter state isolated to individual Handler instances
- Hard limits enforced at the Handler level (defaults to system-wide value)
- Warning threshold at 80% of maximum turn count
- Clear metrics reporting through the ResourceMetrics interface
- Automatic cleanup on session termination

### 1.2 Context Window
**Implementation**: [Component:Handler:1.0]  
**Interface**: [Interface:Handler:ContextManagement:1.0]

The context window implementation provides:
- Token-based size calculation for all content
- Fraction-based limits (percentage of model's full context)
- Peak usage tracking for performance optimization
- Warning thresholds at 80% utilization
- No automatic content optimization (delegated to task implementation)
- Clean termination when limits are reached

### 1.3 Memory Resources
**Implementation**: [Component:Memory:3.0]

Memory resources are managed with:
- Isolated memory contexts per session
- Explicit garbage collection triggers
- Size-based eviction policies for context management
- Relevance-based retention for associative memory
- Explicit versioning for all stored artifacts

## 2. Resource Management Protocols

Resource management follows these concrete protocols:

1. **Allocation**: Resources are allocated at Handler initialization with explicit limits
2. **Tracking**: Usage is tracked with each operation and reported via ResourceMetrics
3. **Warning**: Non-fatal warnings are issued at 80% of resource limits
4. **Termination**: Clean termination occurs at hard limits with detailed error information
5. **Release**: Resources are explicitly released when sessions end

## 3. Contract Validation

Implementation must satisfy these technical requirements:

1. **Isolation**: No resource sharing between Handler instances
2. **Atomicity**: Resource operations must be atomic to prevent race conditions
3. **Reporting**: All resource usage must be reported through standard interfaces
4. **Recovery**: Resource exhaustion must trigger clean termination with recovery options
5. **Efficiency**: Resource tracking overhead must be <1% of total operation time
</file>
<file path="./system/contracts/interfaces.md" project="">
# System Interface Contracts

> This document is the authoritative source for cross-component interfaces.

## 1. Component Integration Contracts

### 1.1 Compiler Integration [Contract:Integration:CompilerTask:1.0]
Defined in [Interface:Compiler:1.0]

The Compiler component provides AST generation and transformation services.

### 1.2 Evaluator Integration [Contract:Integration:EvaluatorTask:1.0]
Defined in [Interface:Evaluator:1.0]

The Evaluator component handles task execution, template variable substitution, and error recovery.

### 1.3 Task System Integration [Contract:Integration:TaskSystem:1.0]
Defined in [Interface:TaskSystem:1.0]

#### Interfaces
- Task Execution: [Interface:TaskSystem:1.0] 
- Template Management: [Interface:TaskSystem:Templates:1.0]
- XML Processing: [Contract:Tasks:TemplateSchema:1.0]

#### Component Responsibilities
- Evaluator: Responsible for all template variable substitution (resolving {{variable_name}} placeholders)
- Handler: Works with fully resolved content only, no template substitution

### Template Substitution Responsibility
- The Evaluator is exclusively responsible for all template variable substitution
- This includes resolving all {{variable_name}} placeholders and input bindings
- Handlers receive fully resolved content with no remaining template variables
- This separation ensures clean component boundaries and single responsibility

#### Resource Contracts
See [Contract:Resources:1.0]

### 1.4 Memory System Integration [Contract:Integration:TaskMemory:3.0]
Defined in [Interface:Memory:3.0]

#### Interfaces
  - Metadata Management: [Interface:Memory:3.0]
    - Task System uses metadata for associative matching
  - Index Management: [Interface:Memory:3.0]
    - Global index serves as the bootstrap for matching; updates occur in bulk

#### Responsibilities
Memory System:
 - Maintains global file metadata index
 - Provides bulk index updates
 - Supplies metadata for associative matching
 - NEVER performs file I/O operations (reading, writing, deletion)
 - Does NOT store or process file contents
 - Follows read-only context model (no updateContext capability)

Task System:
 - Uses context for task execution
 - Receives file references via associative matching
 - Delegates file access to Handler tools
 - Must not attempt to update context directly (removed in 3.0)

Handler:
 - Performs ALL file I/O operations
 - For Anthropic models: Configures computer use tools (optional)
 - For other models: Uses appropriate file access mechanisms
 - Manages all direct interaction with file system
 - Works with fully resolved content only, no template substitution

Evaluator:
 - Responsible for all template variable substitution (resolving {{variable_name}} placeholders)
 - Ensures all templates are fully resolved before passing to Handler
 - Applies different resolution rules for function vs. standard templates
 - Detects and handles variable resolution errors

#### Integration Points
 - Context flow from associative matching to task execution
 - File metadata index is used exclusively for matching; file access is handled by Handler tools

### 1.5 Delegation Boundaries

#### Handler Responsibilities
- Present unified tool interface to the LLM
- Execute direct tools synchronously
- Transform subtask tool calls into CONTINUATION requests
- Manage tool-specific resources
- Configure model-specific tools (e.g., Anthropic computer use)
- Direct execution without continuation mechanism
- Track resource usage for tool operations

#### Memory System Responsibilities
- Provide context for subtask execution
- Support context inheritance for LLM-to-LLM interaction
- Not involved in tool call execution
- Maintain metadata for associative matching

#### Task System Responsibilities
- Coordinate tool implementations (direct and subtask)
- Process CONTINUATION requests from subtask tools
- Manage execution flow between direct and subtask operations
- Handle template selection for subtask tools

## 2. Cross-Component Requirements

### 2.1 State Management
See [Pattern:Error:1.0] for error state handling.

Context Management:
- Task context maintained by Memory System
- Context operations provide immediate consistency
- No persistence guarantees for context data
- Context scope limited to current task execution

### 2.2 Error Propagation
Error handling defined in [Pattern:Error:1.0]

### 2.3 Resource Tracking
Resource contracts defined in [Contract:Resources:1.0]

Memory-Specific Resource Considerations:
- Context size limits defined by Handler
- Global index accessed in full only
- No partial index updates or queries
- File content handling delegated to Handler tools

## 3. Contract Validation 

### 3.1 Validation Requirements
- Context Management
  - Context updates must be atomic
  - Context retrieval must be consistent
  - No data persistence required
- Index Management
  - Index updates must be atomic
  - Index must persist across sessions
  - All file paths must be absolute
  - All metadata must be strings

### 3.2 Success Criteria
Context Management:
- Context updates visible to immediate subsequent reads
- No context data persists between sessions
- Context size within Handler limits

Index Management:
- Index survives system restarts
- Bulk updates atomic and consistent
- All paths resolvable by Handler tools

### 3.3 Verification Methods
- Unit tests for context operations
- Integration tests for index persistence
- Validation of file paths with Handler tools
- Context size limit compliance checks

## 4. Type References

For system-wide type definitions, see [Type:System:1.0] in `/system/contracts/types.md`.
</file>
<file path="./system/contracts/protocols.md" project="">
# System Protocols

## Task Template Schema [Contract:Tasks:TemplateSchema:1.0]

**Note:** This document is the authoritative specification for the XML schema used in task template definitions. All field definitions, allowed enumerations (such as for `<inherit_context>` and `<accumulation_format>`), and validation rules are defined here. For complete validation guidelines, please see Appendix A in [Contract:Resources:1.0].

The task template schema defines the structure for XML task template files and maps to the TaskTemplate interface.

### XML Schema Definition

```xml
<?xml version="1.0" encoding="UTF-8"?>
<xs:schema xmlns:xs="http://www.w3.org/2001/XMLSchema">
  <xs:complexType name="EvaluationResult">
    <xs:sequence>
      <xs:element name="success" type="xs:boolean"/>
      <xs:element name="feedback" type="xs:string" minOccurs="0"/>
    </xs:sequence>
  </xs:complexType>

  <xs:element name="task">
    <xs:complexType>
      <xs:sequence>
        <xs:element name="description" type="xs:string"/>
        <xs:element name="provider" type="xs:string" minOccurs="0"/>
        <xs:element name="output_slot" type="xs:string" minOccurs="0"/>
        <xs:element name="input_source" type="xs:string" minOccurs="0"/>
        <xs:element name="output_format" minOccurs="0">
          <xs:complexType>
            <xs:attribute name="type" use="required">
              <xs:simpleType>
                <xs:restriction base="xs:string">
                  <xs:enumeration value="json"/>
                  <xs:enumeration value="text"/>
                </xs:restriction>
              </xs:simpleType>
            </xs:attribute>
            <xs:attribute name="schema" type="xs:string" use="optional"/>
          </xs:complexType>
        </xs:element>
        <xs:element name="context_management">
          <xs:complexType>
            <xs:sequence>
              <xs:element name="inherit_context">
                <xs:simpleType>
                  <xs:restriction base="xs:string">
                    <xs:enumeration value="full"/>
                    <xs:enumeration value="none"/>
                    <xs:enumeration value="subset"/>
                  </xs:restriction>
                </xs:simpleType>
              </xs:element>
              <xs:element name="accumulate_data" type="xs:boolean"/>
              <xs:element name="accumulation_format">
                <xs:simpleType>
                  <xs:restriction base="xs:string">
                    <!-- When 'full' is specified, complete notes are preserved -->
                    <xs:enumeration value="full"/>
                    <!-- When 'minimal' is specified, only essential metadata is preserved -->
                    <xs:enumeration value="minimal"/>
                  </xs:restriction>
                </xs:simpleType>
              </xs:element>
              <xs:element name="fresh_context">
                <xs:simpleType>
                  <xs:restriction base="xs:string">
                    <xs:enumeration value="enabled"/>
                    <xs:enumeration value="disabled"/>
                  </xs:restriction>
                </xs:simpleType>
              </xs:element>
            </xs:sequence>
          </xs:complexType>
        </xs:element>
        <xs:element name="file_paths" minOccurs="0">
          <xs:complexType>
            <xs:sequence>
              <xs:element name="path" type="xs:string" maxOccurs="unbounded"/>
            </xs:sequence>
          </xs:complexType>
        </xs:element>
        <xs:element name="steps">
          <xs:complexType>
            <xs:sequence>
              <xs:element name="task" maxOccurs="unbounded">
                <xs:complexType>
                  <xs:sequence>
                    <xs:element name="description" type="xs:string"/>
                    <xs:element name="inputs" minOccurs="0">
                      <xs:complexType>
                        <xs:sequence>
                          <xs:element name="input" maxOccurs="unbounded">
                            <xs:complexType>
                              <xs:sequence>
                                <xs:element name="task">
                                  <xs:complexType>
                                    <xs:sequence>
                                      <xs:element name="description" type="xs:string"/>
                                    </xs:sequence>
                                  </xs:complexType>
                                </xs:element>
                              </xs:sequence>
                              <xs:attribute name="name" type="xs:string" use="required"/>
                            </xs:complexType>
                          </xs:element>
                        </xs:sequence>
                      </xs:complexType>
                    </xs:element>
                  </xs:sequence>
                </xs:complexType>
              </xs:element>
            </xs:sequence>
          </xs:complexType>
        </xs:element>
        <xs:element name="inputs" minOccurs="0">             <!-- Maps to inputs -->
          <xs:complexType>
            <xs:sequence>
              <xs:element name="input" maxOccurs="unbounded">
                <xs:complexType>
                  <xs:simpleContent>
                    <xs:extension base="xs:string">
                      <xs:attribute name="name" type="xs:string" use="required"/>
                      <xs:attribute name="from" type="xs:string" use="optional"/>
                    </xs:extension>
                  </xs:simpleContent>
                </xs:complexType>
              </xs:element>
            </xs:sequence>
          </xs:complexType>
        </xs:element>
        <xs:element name="manual_xml" type="xs:boolean" minOccurs="0" default="false"/>      <!-- Maps to isManualXML -->
        <xs:element name="disable_reparsing" type="xs:boolean" minOccurs="0" default="false"/> <!-- Maps to disableReparsing -->
      </xs:sequence>
      <xs:attribute name="ref" type="xs:string" use="optional"/>
      <xs:attribute name="subtype" type="xs:string" use="optional"/>
      <xs:attribute name="type" use="required">
        <xs:simpleType>
          <xs:restriction base="xs:string">
            <xs:enumeration value="atomic"/>
            <xs:enumeration value="sequential"/>
            <xs:enumeration value="reduce"/>
            <xs:enumeration value="script"/>
            <xs:enumeration value="director_evaluator_loop"/>
          </xs:restriction>
        </xs:simpleType>
      </xs:attribute>
    </xs:complexType>
  </xs:element>
  
  <xs:element name="template">
    <xs:complexType>
      <xs:sequence>
        <xs:element name="name" type="xs:string"/>
        <xs:element name="params" type="xs:string"/>
        <xs:element name="returns" type="xs:string" minOccurs="0"/>
        <xs:element name="task" type="TaskType"/>
      </xs:sequence>
    </xs:complexType>
  </xs:element>

  <xs:element name="call">
    <xs:complexType>
      <xs:sequence>
        <xs:element name="template" type="xs:string"/>
        <xs:element name="arg" type="xs:string" maxOccurs="unbounded"/>
      </xs:sequence>
    </xs:complexType>
  </xs:element>
  
  <xs:element name="cond">
    <xs:complexType>
      <xs:sequence>
        <xs:element name="case" maxOccurs="unbounded">
          <xs:complexType>
            <xs:attribute name="test" type="xs:string" use="required"/>
            <xs:sequence>
              <xs:element name="task" minOccurs="1" maxOccurs="1">
                <!-- Task definition inside case -->
              </xs:element>
            </xs:sequence>
          </xs:complexType>
        </xs:element>
      </xs:sequence>
    </xs:complexType>
  </xs:element>
<!-- Director-Evaluator Loop Task Definition -->
<xs:element name="director_evaluator_loop">
  <xs:complexType>
    <xs:sequence>
      <xs:element name="description" type="xs:string"/>
      <xs:element name="max_iterations" type="xs:integer" minOccurs="0"/>
      <xs:element ref="context_management"/>
      <xs:element name="director" type="TaskType"/>
      <xs:element name="evaluator" type="TaskType"/>
      <xs:element name="script_execution" minOccurs="0">
        <xs:complexType>
          <xs:sequence>
            <xs:element name="command" type="xs:string"/>
            <xs:element name="timeout" type="xs:integer" minOccurs="0"/>
            <xs:element name="inputs" type="InputsType"/>
          </xs:sequence>
        </xs:complexType>
      </xs:element>
      <xs:element name="termination_condition" minOccurs="0">
        <xs:complexType>
          <xs:sequence>
            <xs:element name="condition" type="xs:string"/>
          </xs:sequence>
        </xs:complexType>
      </xs:element>
    </xs:sequence>
  </xs:complexType>
</xs:element>
</xs:schema>
```

### Script Execution Support

The XML task template schema now supports defining tasks for script execution within sequential tasks. These tasks enable:
 - Command Specification: Defining external commands (e.g. bash scripts) to be executed.
 - Input/Output Contracts: Passing the director's output as input to the script task, and capturing the script's output for subsequent evaluation.
Script execution errors (e.g. non-zero exit codes) are treated as generic TASK_FAILURE conditions. The evaluator captures the script's stdout and stderr in a designated notes field for downstream decision-making.

Example:
```xml
<task type="sequential">
  <description>Static Director-Evaluator Pipeline</description>
  <context_management>
    <inherit_context>none</inherit_context>
    <accumulate_data>true</accumulate_data>
    <accumulation_format>notes_only</accumulation_format>
  </context_management>
  <steps>
    <task>
      <description>Generate Initial Output</description>
    </task>
    <task type="script">
      <description>Run Target Script</description>
      <inputs>
        <input name="director_output" from="last_director_output"/>
      </inputs>
    </task>
    <task>
      <description>Evaluate Script Output</description>
      <inputs>
        <input name="script_output">
          <task>
            <description>Process output from target script</description>
          </task>
        </input>
      </inputs>
    </task>
  </steps>
</task>
```

### Output Format Specification

Tasks can specify structured output format:

```xml
<task>
  <description>List files in directory</description>
  <output_format type="json" schema="string[]" />
</task>
```

The `schema` attribute provides basic type information:
- "object" - JSON object
- "array" or "[]" - JSON array
- "string[]" - Array of strings
- "number" - Numeric value
- "boolean" - Boolean value

Output validation ensures the result matches the specified type.

## Subtask Spawning Protocol [Protocol:SubtaskSpawning:1.0]

The subtask spawning protocol defines how tasks can dynamically create and execute subtasks.

## LLM Interaction Protocol [Protocol:LLMInteraction:1.0]

The Handler-LLM interaction follows a standardized protocol using the `HandlerPayload` structure:

```typescript
interface HandlerPayload {
  systemPrompt: string;
  messages: Array<{
    role: "user" | "assistant" | "system";
    content: string;
    timestamp?: Date;
  }>;
  context?: string;        // Context from Memory System
  tools?: ToolDefinition[]; // Available tools
  metadata?: {
    model: string;
    temperature?: number;
    maxTokens?: number;
    resourceUsage: ResourceMetrics;
  };
}
```

### Protocol Flow

1. The Task System creates a Handler instance with configuration
2. The Handler creates a HandlerSession to manage conversation state
3. The Evaluator ensures all placeholders are substituted
4. The Handler constructs a HandlerPayload via session.constructPayload()
5. Provider-specific adapters transform the payload to appropriate formats
6. LLM response is processed via handler.processLLMResponse()
7. Tool calls (including user input requests) are handled
8. Session state is updated with new messages
9. Resource usage is tracked and limits enforced

This standardized protocol ensures consistent handling of LLM interactions across different providers while maintaining proper conversation tracking and resource management.

### Request Structure

```typescript
interface SubtaskRequest {
  // Required fields
  type: TaskType;                      // Type of subtask to spawn
  description: string;                 // Description of the subtask
  inputs: Record<string, any>;         // Input parameters for the subtask
  
  // Optional fields
  template_hints?: string[];           // Hints for template selection
  context_management?: {               // Override default context settings
    inherit_context?: 'full' | 'none' | 'subset';
    accumulate_data?: boolean;
    accumulation_format?: 'notes_only' | 'full_output';
    fresh_context?: 'enabled' | 'disabled';
  };
  max_depth?: number;                  // Override default max nesting depth
  subtype?: string;                    // Optional subtype for atomic tasks
  
  /**
   * Optional list of specific file paths to include in subtask context.
   * Takes precedence over associative matching when provided.
   * Paths can be absolute or relative to repo root.
   * Invalid paths will generate warnings but execution will continue.
   */
  file_paths?: string[];
}
```

### Execution Flow

1. **Request Generation**: A parent task returns a result with `status: "CONTINUATION"` and includes a `subtask_request` in its notes.

2. **Request Validation**: The system validates the subtask request structure, ensuring all required fields are present and correctly formatted.

3. **Template Selection**: The system selects an appropriate template based on:
   - The `type` and optional `subtype` fields
   - The `description` field for associative matching
   - Any provided `template_hints`

4. **Depth Control**: The system checks:
   - Current nesting depth against maximum allowed depth (default: 5)
   - Cycle detection to prevent recursive spawning of identical tasks
   - Resource usage across the entire subtask chain

5. **Subtask Execution**: The system executes the subtask with:
   - Direct parameter passing from the `inputs` field
   - Context management according to defaults or overrides
   - Resource tracking linked to the parent task

6. **Result Handling**: The subtask result is passed back to the parent task when execution resumes, with the parent receiving the complete TaskResult structure.

### Error Handling

If a subtask fails, a standardized error structure is generated:

```typescript
{
  type: 'TASK_FAILURE',
  reason: 'subtask_failure',
  message: 'Subtask execution failed',
  details: {
    subtaskRequest: SubtaskRequest;    // The original request
    subtaskError: TaskError;           // The error from the subtask
    nestingDepth: number;              // Current nesting depth
    partialOutput?: string;            // Any partial output if available
  }
}
```

This structure preserves the complete error context, allowing for potential recovery strategies.

### Depth Control Mechanisms

To prevent infinite recursion and resource exhaustion:

1. **Maximum Nesting Depth**: Default limit of 5 levels of nested subtasks
2. **Cycle Detection**: Prevention of tasks spawning identical subtasks
3. **Resource Tracking**: Monitoring of total resource usage across the subtask chain
4. **Timeout Enforcement**: Overall time limits for the complete subtask chain

These mechanisms ensure that subtask spawning remains controlled and resource-efficient.

### Function-Based Templates

The XML schema now supports function-based templates with explicit parameter declarations:

```xml
<template name="analyze_data" params="dataset,config">
  <task>
    <description>Analyze {{dataset}} using {{config}}</description>
  </task>
</template>
```

And function calls with positional arguments:

```xml
<call template="analyze_data">
  <arg>weather_data</arg>
  <arg>standard_config</arg>
</call>
```

This enforces strict scope boundaries - templates can only access explicitly passed parameters.

#### Parameter Resolution

- Parameter names are declared in the comma-separated `params` attribute
- Inside templates, `{{...}}` placeholders only reference declared parameters
- Arguments are evaluated in the caller's environment before being passed to the template
- String arguments can be either variable references or literal values

#### Return Types

Templates can optionally specify a return type using the `returns` attribute:

```xml
<template name="get_file_info" params="filepath" returns="object">
  <task>
    <description>Get metadata for {{filepath}}</description>
    <output_format type="json" schema="object" />
  </task>
</template>
```

This aids in type validation and enables better composition between templates.

### Output Format Specification

Tasks can specify structured output format:

```xml
<task>
  <description>List files in directory</description>
  <output_format type="json" schema="string[]" />
</task>
```

The `schema` attribute provides basic type information:
- "object" - JSON object
- "array" or "[]" - JSON array
- "string[]" - Array of strings
- "number" - Numeric value
- "boolean" - Boolean value

Output validation ensures the result matches the specified type.

### Context Management Configuration

The `<context_management>` element controls how context is managed during task execution:

```xml
<context_management>
    <inherit_context>full|none|subset</inherit_context>
    <accumulate_data>true|false</accumulate_data>
    <accumulation_format>notes_only|full_output</accumulation_format>
    <fresh_context>enabled|disabled</fresh_context>
</context_management>
```

### Context Management Constraints

The following constraints apply to context management settings:

1. **Mutual Exclusivity**: `fresh_context="enabled"` cannot be combined with `inherit_context="full"` or `inherit_context="subset"`
   - If `inherit_context` is "full" or "subset", `fresh_context` must be "disabled"
   - If `fresh_context` is "enabled", `inherit_context` must be "none"

2. **Validation Errors**: Templates violating these constraints will fail validation with clear error messages

Each operator type has specific default settings that apply when the `<context_management>` element is omitted:

| Operator Type | inherit_context | accumulate_data | accumulation_format | fresh_context |
|---------------|-----------------|-----------------|---------------------|---------------|
| atomic        | full            | false           | minimal             | enabled       |
| sequential    | full            | true            | minimal             | enabled       |
| reduce        | none            | true            | minimal             | enabled       |
| script        | full            | false           | minimal             | disabled      |
| director_evaluator_loop | none  | true            | minimal             | enabled       |

When the `<context_management>` element is present, its settings override the operator defaults. Settings are merged during template loading, with explicit settings taking precedence over defaults.

### Field Definitions

- The optional `ref` attribute is used to reference a pre-registered task in the TaskLibrary.
- The optional `subtype` attribute refines the task type (for example, indicating "director", "evaluator", etc.)
- The `output_format` element specifies structured output format and validation requirements.
- The `template` element defines a function-like template with explicit parameters.
- The `call` element invokes a function template with positional arguments.

Example:
```xml
<cond>
  <case test="output.valid == true">
    <task type="atomic" subtype="success_handler">
      <description>Handle success</description>
    </task>
  </case>
  <case test="output.errors > 0">
    <task type="atomic" subtype="error_handler">
      <description>Handle errors</description>
    </task>
  </case>
</cond>
```
All required and optional fields (including `instructions`, `system`, `model`, and `inputs`) are defined by this schema. For full details and allowed values, please see Appendix A in [Contract:Resources:1.0].

### Example Template

```xml
<task>
  <instructions>Analyze the given code for readability issues.</instructions>
  <system>You are a code quality expert focused on readability.</system>
  <model>claude-3-sonnet</model>
  <!-- The criteria element provides a free-form description used for dynamic evaluation template selection via associative matching -->
  <criteria>validate, log</criteria>
  <inputs>
    <input name="code">The code to analyze</input>
  </inputs>
  <output_format type="json" schema="object" />
  <manual_xml>false</manual_xml>
  <disable_reparsing>false</disable_reparsing>
</task>
```

### Validation Rules

1. All required fields must be present
2. Input names must be unique
3. Boolean fields must be "true" or "false"
4. Model must be a valid LLM identifier
5. Output schema must match basic type validation rules
6. Template parameters must match call arguments

### Error Response Schema

```xml
<xs:complexType name="TaskError">
  <xs:choice>
    <xs:element name="resource_exhaustion">
      <xs:complexType>
        <xs:sequence>
          <xs:element name="resource" type="xs:string"/>
          <xs:element name="message" type="xs:string"/>
          <xs:element name="metrics" type="MetricsType" minOccurs="0"/>
        </xs:sequence>
      </xs:complexType>
    </xs:element>
    <xs:element name="task_failure">
      <xs:complexType>
        <xs:sequence>
          <xs:element name="reason" type="TaskFailureReason"/>
          <xs:element name="message" type="xs:string"/>
          <xs:element name="details" type="DetailsType" minOccurs="0"/>
        </xs:sequence>
      </xs:complexType>
    </xs:element>
  </xs:choice>
</xs:complexType>

<xs:simpleType name="TaskFailureReason">
  <xs:restriction base="xs:string">
    <xs:enumeration value="context_retrieval_failure"/>
    <xs:enumeration value="context_matching_failure"/>
    <xs:enumeration value="context_parsing_failure"/>
    <xs:enumeration value="xml_validation_failure"/>
    <xs:enumeration value="output_format_failure"/>
    <xs:enumeration value="execution_timeout"/>
    <xs:enumeration value="execution_halted"/>
    <xs:enumeration value="subtask_failure"/>
    <xs:enumeration value="input_validation_failure"/>
    <xs:enumeration value="unexpected_error"/>
  </xs:restriction>
</xs:simpleType>
```

### Interface Mapping

This schema is used by the TaskSystem component. For implementation details and interface definitions, see:
- TaskTemplate interface in spec/types.md [Type:TaskSystem:TaskTemplate:1.0]
- Template validation in TaskSystem.validateTemplate() 
- Template parsing in TaskSystem constructor

Note: The new XML attributes (`ref` and `subtype`) and the `<cond>` element map to the corresponding types (i.e., TaskDefinition and FunctionCall) used in the Task System.

### Map Pattern Implementation
Refer to the XML schema for correct usage. For a complete example, please see the Task System documentation.
</file>
<file path="./system/README.md" project="">
# LLM interpreter system-level description

This system proposes a programming environment for LLMs.

```mermaid
graph TD
    subgraph "User Input & Compilation"
        NL[Natural Language Input] --> Compiler
        Compiler -->|Generate XML or DSL| AST[Task AST]
    end
    
    subgraph "Evaluation"
        AST --> Evaluator[Evaluator]
        Evaluator -->|Execute Atomic| Handler[Handler]
        Evaluator -->|Subtask / DSL| Evaluator
        Evaluator -->|Context Query| Memory[Memory System]
    end
    
    subgraph "Memory System"
        LongTerm[Global Metadata Index]
        Working[Working Memory]
        LongTerm -->|Associative Match| Working
    end
    
    subgraph "Runtime / Tools"
        LLM[LLM Session]
        External[Scripts & External Tools]
        Handler --> LLM
        Handler --> External
    end
```

## Table of Contents

## Documentation Guide

### Documentation Structure
This documentation is organized into several key areas:
- **System Level**: Architecture, patterns, and system-wide contracts in `/system`
- **Component Level**: Detailed documentation for each component in `/components`
- **Implementation Details**: Specific implementation guidance in `/components/*/impl`
- **Interface Definitions**: Public and internal interfaces in `/components/*/api`

### Canonical Sources and Cross-References
To reduce duplication and maintain consistency, this documentation follows a pattern of canonical sources and cross-references:

- **Canonical Sources**: Definitive documentation for each concept lives in a single location
- **Cross-References**: Other documents reference the canonical source rather than duplicating content

### Cross-Reference Syntax
The documentation uses a consistent cross-reference syntax:
- `[Type:Name:Version]` for type references (e.g., `[Type:TaskSystem:TaskResult:1.0]`)
- `[Pattern:Name:Version]` for pattern references (e.g., `[Pattern:DirectorEvaluator:1.1]`)
- `[ADR N: Title]` for architecture decision references (e.g., `[ADR 14: Operator Context Configuration]`)
- `[Component:Name:Version]` for component references (e.g., `[Component:Evaluator:1.0]`)
- `[Contract:Category:Name:Version]` for contract references (e.g., `[Contract:Tasks:TemplateSchema:1.0]`)
- `[Interface:Component:Name:Version]` for interface references (e.g., `[Interface:Memory:3.0]`)

### Finding Authoritative Sources
To find the canonical source for a concept:
1. Look for matching files in the pattern directories (`/system/architecture/patterns/`)
2. Check ADRs for decisions (`/system/architecture/decisions/`)
3. Consult component READMEs for component-specific concepts
4. Review contract documents for system-wide agreements

**Note on ADR References**: Architecture Decision Records (ADRs) document the decision-making process at a specific point in time. While they may be referenced for historical context or rationale, all critical functionality should be fully documented in the main documentation without requiring readers to consult ADRs. Documentation should be self-contained with ADRs serving as supplementary, not required, reading.
- [Key Capabilities](#key-capabilities)
- [Core Concepts & DSL Approach](#core-concepts--dsl-approach)
- [Architecture Overview](#architecture-overview)
- [Patterns & Execution Model](#patterns--execution-model)
- [Resource & Error Handling](#resource--error-handling)
- [Usage Workflow](#usage-workflow)
- [Development & Extension](#development--extension)
- [Examples](#examples)
- [References & Documentation Map](#references--documentation-map)

## Key Capabilities

- **Task Decomposition**
  - Convert a complex prompt into smaller, structured subtasks or DSL "functions."
- **DSL & XML Representation**
  - Represent tasks with a domain-specific language (Scheme-like or XML-based) that is easily parsed, versioned, and extended.
- **Resource-Aware Execution**
  - Limit turns and context windows to avoid runaway costs, with warnings at 80% usage.
- **Context Management**
  - Inherit, accumulate, or fetch fresh context using a three-dimensional model (inherit_context, accumulate_data, fresh_context).
- **Director-Evaluator Pattern**
  - Iterative refinement pattern that uses a "director" for output generation and an "evaluator" for feedback, optionally coupled with script execution steps.
- **Subtask Spawning**
  - Dynamically create subtasks if a parent task decides it needs to break the problem down further.
- **Error Taxonomy & Partial Results**
  - Structured error types (resource exhaustion, invalid output, subtask failure) with optional partial outputs for robust debugging and recovery.
- **Modular Components**
  - Compiler for translation, Evaluator for step execution, Memory System for metadata, Handler for LLM calls and script bridging.

## Core Concepts & DSL Approach

### Tasks as Code
A user's request can be viewed as a "program," with subtasks akin to function calls. The system supports two approaches:

1. **Implicit Variable Access** (Legacy):
   Tasks can access variables from their parent environment via `{{variable_name}}` syntax.

2. **Function-Based Templates** (Recommended):
   Templates explicitly declare their parameters, providing clearer scope boundaries and dependencies:

   ```xml
   <template name="analyze_data" params="dataset,config">
     <task>
       <description>Analyze {{dataset}} using {{config}}</description>
     </task>
   </template>
   ```

   These templates are called with positional arguments:
   ```xml
   <call template="analyze_data">
     <arg>weather_data</arg>
     <arg>standard_config</arg>
   </call>
   ```

   This approach offers cleaner variable scoping, explicit dependencies, and improved maintainability.

The system can also compile these instructions into a Scheme-like DSL:

```scheme
(define (process-data data-source)
  (sequential
    (task "load-data" data-source)
    (reduce 
      (lambda (chunk acc)
        (task "analyze-chunk" chunk acc))
      initial-value
      chunks)))
```

### DSL vs. XML

- **XML** is convenient for interoperability, partial validation, and structured storage.
- **DSL** is more expressive and compositional, letting you nest or map tasks in functional style.
- Both representations ultimately feed into the Evaluator, which orchestrates subtask calls.

### Lexical Environment and context management
Each DSL expression or XML subtask runs in an environment that may inherit context window data from its parent, in addition to lexically bound variables. 

## Architecture Overview

| Component | Responsibility | Docs |
|-----------|----------------|------|
| Compiler | Translate natural language or user input into a DSL/AST or XML. | [Compiler README](../components/compiler/README.md) |
| Evaluator | Walk the AST, manage subtask calls, handle resource usage. | [Evaluator README](../components/evaluator/README.md) |
| Task System | Provide operators, templates, and pattern frameworks. | [TaskSystem README](../components/task-system/README.md) |
| Memory System | Manage file metadata & associative context retrieval. | [Memory README](../components/memory/README.md) |
| Handler | LLM interface that exposes a unified tool system and manages resource tracking. | Handler Interface Docs |

### Flow
1. User Input → [Compiler] → AST (XML or DSL)
2. Evaluator interprets AST, possibly fetching context from [Memory].
3. Handler enforces resource limits (turn count, context window) and calls the LLM or external scripts.
4. Subtasks spawn recursively if the Evaluator returns "CONTINUATION" with a subtask request.

## Patterns & Execution Model

### 1. Director-Evaluator Pattern
A powerful approach for iterative refinement:

1. Director produces an initial output.
2. Evaluator checks correctness, possibly runs a validation script.
3. If improvements are needed, the Director is called again with the evaluator's feedback.

This can be done with either a static (predefined) or dynamic (continuation-based) approach.

### 2. Context Management

The system provides comprehensive context control through a three-dimensional model and explicit file selection:

#### Three-Dimensional Context Model
- **inherit_context**: Controls parent context inheritance with values "full" (complete inheritance), "none" (no inheritance), or "subset" (selective inheritance based on relevance).
- **accumulate_data**: Controls whether outputs from prior steps are accumulated (true/false).
- **accumulation_format**: When accumulating data, specifies whether to include "notes_only" or "full_output".
- **fresh_context**: Controls whether new context is fetched via associative matching ("enabled"/"disabled").

#### Explicit File Selection
In addition to the standard context model, tasks can explicitly specify files to include in their context:
```xml
<file_paths>
  <path>./src/main.py</path>
  <path>/absolute/path/file.txt</path>
</file_paths>
```
This feature operates orthogonally to the three-dimensional model, ensuring specified files are always included regardless of other settings.

Each operator type has sensible defaults while allowing explicit overrides through template XML.

### 3. Subtask Spawning
The system implements a standardized mechanism for dynamic task creation and composition through the subtask spawning protocol:

- A parent task returns with `status: "CONTINUATION"` and a `subtask_request` in its notes
- The subtask_request contains type, description, inputs, and optional template_hints
- The system validates the request, checks depth limits, and performs cycle detection
- The subtask executes with its own resource tracking and context management
- Results flow back to the parent task through direct parameter passing
- Error handling preserves partial results and detailed context for recovery

This mechanism enables dynamic task decomposition based on runtime discoveries while maintaining controlled depth management to prevent infinite recursion.

### 4. DSL Environment Model
- Lexical scoping for variables
- Evaluate or apply steps akin to a "metacircular interpreter"
- Potential for dynamic decomposition if tasks fail or are incomplete

## Resource & Error Handling

### Resource Exhaustion
- Handler raises `RESOURCE_EXHAUSTION` if turn count or context window is exceeded.
- The Evaluator can attempt decomposition or surface an error to the parent.

### Invalid Output & Parsing
- If an atomic task yields invalid XML or fails DSL constraints, we capture partial output in notes and either reparse or fail.

### Partial Results
- Operators like "sequential" can preserve prior step outputs even if a later step fails.
- The final error message includes partial outputs if the partial-results policy is enabled.

### Context-Generation Failures
- Typically signaled as a `TASK_FAILURE` with reason codes like "context_retrieval_failure" or "context_parsing_failure."
- The system can attempt fallback or re-try with simpler context if the policy allows.

## Usage Workflow

Below is a typical usage sequence:

1. User submits a high-level command or request.
2. Compiler produces an AST in XML or DSL form.
3. Evaluator loads the AST, checks resource availability, and sets up a Handler.
4. Evaluator executes tasks step by step. For each atomic step:
   - The Handler calls the LLM or runs a script, tracks tokens/turns, returns output.
   - If any step or subtask triggers re-parse or subtask spawning, the Evaluator orchestrates it.
5. Once done, the final TaskResult includes the content, status, and notes fields (which might hold partial results, warnings, or success scores).

## Development & Extension

### Defining New DSL Operators
- Add a new operator for your custom logic (e.g. "map," "filter," "director_evaluator_loop").
- Specify how it inherits context and whether it accumulates partial outputs.

### Authoring Task Templates in XML
- Use `<task type="atomic" ...>` or `<task type="sequential" ...>` with a `<context_management>` block.
- Provide `<inputs>` for data binding and an `<expected_output>` if relevant.

### Error Handling & Recovery
- Consider how partial results help your tasks recover from local failures.
- Possibly define a re-parse template to handle resource exhaustion.

### Resource Configuration
- Per-task or system-wide, set maxTurns, maxContextWindowFraction, etc. in the Handler config.
- Handler automatically enforces these limits.

## Examples

### 1. DSL Example (Scheme-style)
```scheme
(define (director-eval-pipeline initial-data)
  (sequential
    (task "director" initial-data)
    (task "script-execution" (lambda (dirOutput)
       (bash-run "analysis.sh" dirOutput)))
    (task "evaluator" "Evaluate script output" )))
```

### 2. XML Example: Director-Evaluator Loop
```xml
<task type="director_evaluator_loop">
  <description>Iterative code refinement</description>
  <max_iterations>3</max_iterations>
  <context_management>
    <inherit_context>none</inherit_context>
    <accumulate_data>true</accumulate_data>
    <accumulation_format>notes_only</accumulation_format>
    <fresh_context>enabled</fresh_context>
  </context_management>
  <director>
    <description>Generate solution for {{problem}}</description>
    <inputs>
      <input name="problem" from="user_query"/>
      <input name="feedback" from="evaluation_feedback"/>
      <input name="iteration" from="current_iteration"/>
    </inputs>
  </director>
  <evaluator>
    <description>Evaluate solution against requirements</description>
    <inputs>
      <input name="solution" from="director_result"/>
      <input name="requirements" from="user_query"/>
    </inputs>
  </evaluator>
  <script_execution>
    <command>./test_solution.sh</command>
    <timeout>300</timeout>
    <inputs>
      <input name="script_input" from="director_result"/>
    </inputs>
  </script_execution>
  <termination_condition>
    <condition>evaluation.success === true</condition>
  </termination_condition>
</task>
```

### 3. TypeScript Invocation
```typescript
const taskSystem = new TaskSystem({
  maxTurns: 8,
  maxContextWindowFraction: 0.75,
  systemPrompt: "System-level guidance"
});
const memorySystem = new MemorySystem();

const result = await taskSystem.executeTask(
  "Analyze the new code and see if it violates style guidelines.",
  memorySystem
);

if (result.status === "FAILED") {
  console.error("Task failed:", result.notes);
}
```

### 4. Function Template Example
```xml
<!-- Define a template -->
<template name="process_file" params="filepath,options">
  <task type="sequential">
    <description>Process file {{filepath}} with options {{options}}</description>
    <context_management>
      <inherit_context>none</inherit_context>
      <accumulate_data>true</accumulate_data>
      <accumulation_format>notes_only</accumulation_format>
    </context_management>
    <steps>
      <task>
        <description>Load and validate file {{filepath}}</description>
      </task>
      <task>
        <description>Apply transformations with {{options}}</description>
      </task>
    </steps>
  </task>
</template>

<!-- Call the template -->
<call template="process_file">
  <arg>data/sample.csv</arg>
  <arg>{"normalize": true, "filter_nulls": true}</arg>
</call>
```

```typescript
// Register a template
taskSystem.registerTemplate({
  name: "analyze_data",
  parameters: ["dataset", "config"],
  body: /* Task AST node */
});

// Execute a function call
const result = await taskSystem.executeCall({
  templateName: "analyze_data",
  arguments: [
    "weather_data.csv",
    {"method": "statistical", "outliers": "remove"}
  ]
});
```

## References & Documentation Map

### Architecture
- [Overview](./architecture/overview.md)
- [Patterns](./architecture/patterns/)
- [Decisions (ADRs)](./architecture/decisions/)

### Contracts
- [Interfaces](./contracts/interfaces.md)
- [Resources](./contracts/resources.md)
- [Protocols (XML Schema)](./contracts/protocols.md)

### Components
- [Task System](../components/task-system/README.md)
- [Evaluator](../components/evaluator/README.md)
- [Memory System](../components/memory/README.md)
- [Compiler](../components/compiler/README.md)
- Handler Tools
</file>
<file path="./system/architecture/overview.md" project="">
# Architecture Overview

## Problem Statement and Goals

### Atomic Task Matching

The system uses a uniform, heuristic approach for template matching:

- Only atomic tasks have templates; composite tasks are created by combining atomic tasks
- Template matching is performed heuristically by user-defined associative matching tasks with fixed I/O signatures
- Matching logic is uniform for MVP (no operator-specific differences or versioning)
- System always selects the highest-scoring candidate template
- Optional "disable context" flag allows tasks to run without inherited context
- Task results may include optional success score in "notes" field for future adaptive scoring

```mermaid
flowchart TD
    A[User Input] --> B[Create ContextGenerationInput]
    B --> C{Context Disabled?}
    C -->|No| D[Include inheritedContext]
    C -->|Yes| E[Omit inheritedContext]
    D --> F[Call getRelevantContextFor]
    E --> F
    F --> G[Score Candidates]
    G --> H[Select Highest Score]
```

This document provides a high‑level overview of the system architecture. Detailed technical discussions have been moved into canonical files in the sub‑folders:

- **Patterns:** Core patterns such as Director‑Evaluator, Error Handling, and Resource Management (see files under `system/architecture/patterns/`).
– **Decisions (ADRs):** Architecture Decision Records on topics such as context management and memory system design (see `system/architecture/decisions/`).
– **Q&A and Open Questions:** Clarifications and unresolved issues (see `system/architecture/qa/` and `system/architecture/questions.md`).

## Document Map

**This folder contains:**
 - `overview.md`: This high‑level summary and navigation index.
 - `patterns/`: Detailed technical descriptions of core patterns.
 - `decisions/`: Architecture Decision Records (ADRs) with rationale and scope.
 - `qa/`: Frequently asked questions and clarifications.
 - `questions.md`: A list of open and unresolved architecture questions.

For full technical details on any topic, please refer to the canonical file listed above.

### System Goals
1. Primary Goals
- Provide reliable task automation through structured decomposition and execution
- Ensure consistent task processing despite resource constraints
- Enable robust error recovery without human intervention
- Maintain system coherence across task boundaries

2. Quality Goals
- Predictable resource usage through explicit tracking and limits
- Consistent behavior through standardized protocols and interfaces
- Extensible task handling via template-based architecture
- Maintainable system through clear component boundaries

3. Operational Goals
- Handle varying task complexities through dynamic decomposition
- Support diverse task types through flexible template system
- Preserve critical context across task boundaries
- Manage resources efficiently within defined constraints

### System Constraints

#### Resource Constraints
- Fixed context window size
- Limited turn counts
- Synchronous operation only
- File access via Handler tools only

#### Operational Constraints  
- One Handler per task execution
- Immutable Handler configuration
- No persistent state maintenance
- Template immutability during execution

## Core Patterns

### Director-Evaluator Pattern [Pattern:DirectorEvaluator:1.1]

The system implements a standardized mechanism for iterative refinement that supports both static (predefined) and dynamic (continuation-based) variants. This pattern enables:

- Iterative improvement through feedback loops
- Optional integration with script execution
- Explicit termination conditions based on evaluation results

For the complete specification, including implementation details, integration points, and context management integration, see [Pattern:DirectorEvaluator:1.1] in `system/architecture/patterns/director-evaluator.md`.

The system follows a unified context management model. Task input values are dynamically substituted using the `{{...}}` syntax, allowing direct parameter passing between tasks. This explicit binding mechanism (using the `from` attribute) improves clarity and flexibility in task execution by making data dependencies clear and reducing reliance on environment variables.

Context is managed via a single `<context_management>` block that distinguishes between:
 - **Inheritance:** (using the new `inherit_context` enumeration)
 - **Accumulation:** (using the boolean `accumulate_data` and the `accumulation_format` setting)

### Error Handling [Pattern:Error:1.0]
Defines how errors propagate and recover across component boundaries.

See [Interface:ErrorHandling:1.0] in system/contracts/interfaces.md for complete specification.

### Resource Management [Pattern:ResourceManagement:1.0]
Defines resource usage tracking and lifecycle across components.

#### Core Principles
- Handler-based resource isolation
- Per-task resource tracking
- Context window management
- Memory system integration
- No cross-Handler resource sharing
- Read-only memory access

#### Component Responsibilities

##### Handler
- Owns turn counting per task execution
- Manages context window size
- Tracks token usage
- Enforces resource limits
- Ensures session termination

##### Task System
- Creates Handler instances
- Configures immutable resource limits
- Delegates resource tracking to Handler
- Manages template persistence

##### Memory System
- Maintains task context data
- Provides context management interface
- Maintains global file metadata index
- No file content storage

#### Resource Types and Protocols
- Turn Counter: Per-Handler atomic tracking with strict limits
- Context Window: Token-based size monitoring and enforcement
- Memory Resources: Short-term task context with clear boundaries
- Resource Release: Coordinated cleanup and state invalidation
- Error Handling: Resource exhaustion detection and preservation

See [Contract:Resources:1.0] in system/contracts/resources.md for complete specification.

### Task Execution [Pattern:TaskExecution:2.0]
Defines how tasks are structured, executed, and managed.

Key concepts:
- Template-based definition
- Handler-managed execution
- Resource-aware processing
- XML-based protocols

See [Contract:Tasks:2.0] in system/contracts/protocols.md for complete specification.

## Delegation Mechanisms

The system provides a unified tool interface with distinct implementation mechanisms:

### Template Substitution Delegation
- The Evaluator is solely responsible for all template variable substitution
- Handlers receive fully resolved content with no remaining template variables
- This separation ensures clean component boundaries and single responsibility

### Tool Interface
What the LLM sees and interacts with:
- Consistent tool-based invocation pattern for all operations
- Unified parameter schemas and error handling
- Standardized result format regardless of implementation

### Implementation Mechanisms

1. **Direct Tool Implementation**
   - Used for external interactions (files, APIs, scripts)
   - No continuation mechanism or complex context management
   - Executed directly by the Handler component

2. **Subtask Tool Implementation**
   - Involve Memory System for context management
   - Use SubtaskRequest structure and CONTINUATION status
   - Follow the context management model defined in ADR 14

See [Pattern:ToolInterface:1.0] for a detailed explanation of this unified approach.

## Component Architecture

The system consists of four core components working together to process, execute, and manage tasks:

### Handler [Component:Handler:1.0]
LLM interface and resource tracking component.
- Performs ALL file I/O operations (reading, writing, deletion)
- Supports multiple LLM providers with appropriate tool configurations
- Abstracts provider-specific implementation details while maintaining consistent capabilities
- Manages resource usage tracking (turns, tokens)
- Handles LLM interactions and session management
- Works with fully resolved content (no template variable substitution)
- Works with fully resolved content (no template variable substitution)

### Compiler [Component:Compiler:1.0]
Task parsing and transformation component.
- Translates natural language to XML/AST
- Validates against XML schema
- Handles task transformation
- Manages template validation

See [Contract:Integration:CompilerTask:1.0] for integration specification.

### Evaluator [Component:Evaluator:1.0]
Execution control component.
- Controls AST processing and execution
- Manages all lexical environments and variable scoping
- Performs all template variable substitution before Handler invocation
- Handles different substitution rules for function vs. standard templates
- Manages failure recovery
- Tracks resource usage
- Handles reparse requests

See [Contract:Integration:EvaluatorTask:1.0] for integration specification.

### Task System [Component:TaskSystem:1.0]
Task execution and management component.
- Coordinates task execution via Handlers
- Manages task templates and matching
- Interfaces with Memory System
- Processes XML input/output

See components/task-system/README.md for complete specification.

### Memory System [Component:Memory:3.0]
Metadata management component.
- Maintains global file metadata index (paths and descriptive strings)
- Provides metadata for associative matching without ranking or prioritization
- Supplies metadata for file-based lookup and partial matching
- Does NOT store file content, perform file operations, track resources, or rank matches
- NEVER performs file I/O operations - all file access is handled by Handler
- Follows read-only context model (no updateContext capability)

See [Contract:Integration:TaskMemory:3.0] for integration specification.

## Component Integration

### Core Integration Patterns
Components interact through defined contracts:

#### Compiler ↔ Task System
- Task parsing coordination
- Schema validation
- Template utilization

#### Evaluator ↔ Compiler
- AST execution feedback
- Reparse requests
- Resource usage updates

#### Task System ↔ Evaluator
- Execution coordination
- Resource allocation
- State management

#### Task System ↔ Memory System
- Context management
- Metadata index access
- Associative matching support

See system/contracts/interfaces.md for complete contract specifications.

### Resource Ownership
- Handlers own task resources
- Memory system owns context storage
- Task system coordinates allocation
- Clean resource release required

See system/contracts/resources.md for complete ownership model.

### System-Wide Protocols
- XML-based task definitions and protocols
- Standard error propagation
- Resource usage tracking
- Context management

---

### Function-Based Template Pattern [Pattern:FunctionTemplate:1.0]

The system implements a function-based template pattern that enforces clear boundaries between caller and callee contexts:

1. **Template Definition**
   - Templates explicitly declare their parameters using a `params` attribute
   - Each template has its own lexical scope containing only its parameters
   - Templates are registered in a central TaskLibrary during parsing

2. **Function Calling**
   - Function calls use positional arguments evaluated in the caller's context
   - Arguments can be literals, variable references, or nested expressions
   - A new environment is created for each function call with bindings for parameters
   - No implicit access to the caller's environment is allowed

3. **AST Representation**
   - TemplateNode represents function definitions
   - FunctionCallNode represents function invocations
   - ArgumentNode represents argument values

This pattern enables:
 - Clear data dependencies between components
 - Improved reasoning about variable scope
 - Better encapsulation of implementation details
 - Foundations for more advanced functional patterns

### Sequential Task Management [Pattern:SequentialTask:2.0]

The system maintains **explicit task history** for sequential operations. This design clarifies how multiple steps in a single task can share context in a controlled, trackable way, and how partial or final outputs are captured.

1. **Output Tracking**
   - The Evaluator maintains a list of all previous task outputs, keyed by step index or ID.
   - The lifecycle for this history is well-defined; it is preserved until the sequence finishes.
   - Storage must remain resource-aware to avoid memory limit issues. If output is large, the evaluator can store a summarized version or notes-only.

### Subtask Spawning Mechanism [Pattern:SubtaskSpawning:1.0]

The system implements a standardized subtask spawning mechanism that enables dynamic task creation and composition:

1. **Continuation Protocol**
   - A parent task returns with `status: "CONTINUATION"` and a `subtask_request` in its notes
   - The subtask_request contains type, description, inputs, and optional template_hints
   - The system validates the request structure before processing
   - Depth control prevents infinite recursion and detects cycles

2. **Request Structure**
   ```typescript
   interface SubtaskRequest {
     // Required fields
     type: TaskType;                      // Type of subtask to spawn
     description: string;                 // Description of the subtask
     inputs: Record<string, any>;         // Input parameters for the subtask
     
     // Optional fields
     template_hints?: string[];           // Hints for template selection
     context_management?: {               // Override default context settings
       inherit_context?: 'full' | 'none' | 'subset';
       accumulate_data?: boolean;
       accumulation_format?: 'notes_only' | 'full_output';
       fresh_context?: 'enabled' | 'disabled';
     };
     max_depth?: number;                  // Override default max nesting depth
     subtype?: string;                    // Optional subtype for atomic tasks
   }
   ```

3. **Data Flow**
   ```mermaid
   flowchart LR
       A[Parent LLM] -->|"tools.analyzeData({...})"| B[Handler]
       B -->|CONTINUATION with subtask_request| C[Task System]
       C -->|Execute subtask| D[Subtask LLM]
       D -->|Result| C
       C -->|Add as tool response| B
       B -->|Continue with tool result| A
   ```
   - Parent tasks make tool calls that require complex processing
   - Handler returns CONTINUATION status with subtask_request
   - Task System executes subtask independently
   - Subtask result is added as a tool response to parent's session
   - Parent continues execution with the tool result in conversation history
   - From the LLM's perspective, this appears as a normal tool call and response
   - No special resumption methods or complex continuation mechanisms required

4. **Context Integration**
   - Subtasks have default context management settings:
     | inherit_context | accumulate_data | accumulation_format | fresh_context |
     |-----------------|-----------------|---------------------|---------------|
     | subset          | false           | notes_only          | enabled       |
   - These defaults can be overridden through explicit configuration in the subtask_request
   - Context management follows the hybrid configuration approach with operator-specific defaults and explicit overrides

5. **Resource Protection**
   - Maximum nesting depth prevents infinite recursion (default: 5 levels)
   - Cycle detection prevents tasks from spawning themselves
   - Resource usage is tracked across the entire subtask chain
   - Partial results are preserved when subtasks fail

6. **Error Handling**
   - Subtask failures are wrapped in standardized error structures
   - Parent tasks receive detailed error information including:
     - Original subtask request
     - Specific error details from the subtask
     - Current nesting depth
     - Any partial results that were generated before failure
   - This enables robust recovery strategies at the parent task level

For historical context and decision rationale, see [ADR 11: Subtask Spawning Mechanism].

2. **Context Management**
   - The system implements a hybrid configuration approach with operator-specific defaults and explicit overrides:
     - **inherit_context**: Controls whether a subtask inherits "full" parent context, "none", or a "subset" based on relevance.
     - **accumulate_data**: Controls whether outputs from prior steps are accumulated.
     - **accumulation_format**: Specifies whether to store "notes_only" or "full_output" when accumulating data.
     - **fresh_context**: Controls whether new context is generated via associative matching.
   
   - **Constraint**: Fresh context generation (`fresh_context="enabled"`) is mutually exclusive with context inheritance (`inherit_context="full"` or `inherit_context="subset"`):
     - If a task inherits context (full or subset), it must not generate fresh context
     - If a task generates fresh context, it must not inherit context from its parent
     - This simplifies the system by preventing potential context duplication

   - **Subtype-Based Defaults**: Default context settings are determined by both operator type and subtype:
     - For atomic tasks with "standard" subtype: inherit_context="full", fresh_context="disabled"
     - For atomic tasks with "subtask" subtype: inherit_context="none", fresh_context="enabled"
     - This distinction allows different behavior for regular tasks vs. subtasks
   
   - Default settings for sequential tasks:
     | inherit_context | accumulate_data | accumulation_format | fresh_context |
     |-----------------|-----------------|---------------------|---------------|
     | full            | true            | notes_only          | disabled      |
   
   - These defaults can be overridden through an explicit XML structure:
   ```xml
   <context_management>
       <inherit_context>full|none|subset</inherit_context>
       <accumulate_data>true|false</accumulate_data>
       <accumulation_format>notes_only|full_output</accumulation_format>
       <fresh_context>enabled|disabled</fresh_context>
   </context_management>
   ```
   
   - When the context_management block is omitted, operator-specific defaults apply
   - When present, explicit settings override the defaults
   - This hybrid approach provides both consistency and flexibility

3. **Partial Failures**
   - If a step fails, all previous step outputs remain available in the final (error) output.
   - The final error output includes which step number or ID caused the failure.

4. **Resource Handling**
   - Maximum stored-history size is enforced by the system to prevent out-of-memory or context window exhaustion.
   - The evaluator must handle large output storage carefully, possibly discarding or summarizing to keep track usage in check.
   - Clear cleanup protocols ensure that once the sequence completes (successfully or in error), the stored step outputs are removed.

In summary, **SequentialTask** pattern addresses multi-step tasks with optional or partial data inheritance across steps, ensuring that both resource usage and error behavior remain consistent and predictable.

See system/contracts/protocols.md for protocol specifications.
</file>
<file path="./system/architecture/patterns/context-frames.md" project="">
# Context Frame Pattern [Pattern:ContextFrame:1.0]

**Canonical Reference:** This document is the authoritative description of the Context Frame Pattern. All extended descriptions in other files should refer here.

## Related Documents
- Memory System ADR in [ADR:Memory:1.0]
- Memory component in [Component:Memory:3.0]
- Resource Management Pattern in [Pattern:ResourceManagement:1.0]

## Context Frame Operations

### Operator Default Settings

Each operator type and subtype combination has specific default context management settings:

| Operator Type (Subtype) | inherit_context | accumulate_data | accumulation_format | fresh_context |
|-------------------------|-----------------|-----------------|---------------------|---------------|
| atomic (standard)       | full            | false           | minimal             | disabled      |
| atomic (subtask)        | none            | false           | minimal             | enabled       |
| sequential              | full            | true            | minimal             | disabled      |
| reduce                  | none            | true            | minimal             | enabled       |
| script                  | full            | false           | minimal             | disabled      |
| director_evaluator_loop | none            | true            | minimal             | enabled       |

### Context Management Constraints

The following constraints apply to context management settings:

1. **Mutual Exclusivity**: `fresh_context="enabled"` cannot be combined with `inherit_context="full"` or `inherit_context="subset"`
   - If `inherit_context` is "full" or "subset", `fresh_context` must be "disabled"
   - If `fresh_context` is "enabled", `inherit_context` must be "none"

2. **Subtype-based Defaults**: Default context settings depend on both operator type and subtype
   - Atomic tasks with "standard" subtype default to `inherit_context="full"` and `fresh_context="disabled"`
   - Atomic tasks with "subtask" subtype default to `inherit_context="none"` and `fresh_context="enabled"`
   - For CONTINUATION-based subtasks, the "subtask" subtype is automatically applied

3. **Validation Errors**: Templates violating these constraints will fail validation with clear error messages

### File Paths as Orthogonal Context Control

In addition to the three-dimensional context model, the system supports direct file specification through the `file_paths` feature:

- **Purpose**: Explicitly include specific files in task context
- **Behavior**: Operates outside the standard three-dimensional model
- **Priority**: Files specified via `file_paths` are always included in context, regardless of other settings
- **Integration**: Works alongside other context management settings

```xml
<task type="atomic">
  <description>Task with specific file context</description>
  <context_management>
    <inherit_context>none</inherit_context>
    <fresh_context>disabled</fresh_context>
  </context_management>
  <file_paths>
    <path>./src/main.py</path>
    <path>/absolute/path/file.txt</path>
  </file_paths>
</task>
```

When `file_paths` is combined with other context settings:

- With `inherit_context="subset"`: Specified files become the subset (replacing associative matching-based subset selection)
- With `fresh_context="enabled"`: Specified files are forcibly included while associative matching still runs for additional context

### Common Context Management Patterns

The three-dimensional context management model can be configured to achieve various behaviors:

#### 1. Clear All Context (Fresh Start)
```xml
<context_management>
  <inherit_context>none</inherit_context>
  <accumulate_data>false</accumulate_data>
  <fresh_context>enabled</fresh_context>
</context_management>
```
This configuration:
- Ignores any parent/inherited context
- Doesn't accumulate previous results
- Generates entirely fresh context through associative matching

#### 2. Rebuild Context While Preserving History
```xml
<context_management>
  <inherit_context>none</inherit_context>
  <accumulate_data>true</accumulate_data>
  <accumulation_format>minimal</accumulation_format>
  <fresh_context>enabled</fresh_context>
</context_management>
```
This configuration:
- Ignores parent/inherited context
- Keeps accumulated outputs from previous steps
- Generates fresh context in addition to accumulated data
- Specifically, preserves essential metadata from previous steps

#### 3. Complete Context Preservation
```xml
<context_management>
  <inherit_context>full</inherit_context>
  <accumulate_data>true</accumulate_data>
  <accumulation_format>full</accumulation_format>
  <fresh_context>disabled</fresh_context>
</context_management>
```
This configuration:
- Preserves all parent context
- Keeps complete outputs from previous steps
- Doesn't generate additional context

- **Subtype Awareness**: Context defaults depend on both operator type and subtype
  - Standard atomic tasks inherit context but don't generate fresh context
  - Subtasks don't inherit context but do generate fresh context
  - This distinction avoids duplication while supporting different use cases

These defaults apply when no explicit context_management block is provided. When present, the explicit settings override the defaults:

```xml
<context_management>
    <inherit_context>full|none|subset</inherit_context>
    <accumulate_data>true|false</accumulate_data>
    <accumulation_format>notes_only|full_output</accumulation_format>
    <fresh_context>enabled|disabled</fresh_context>
</context_management>
```

Settings are merged during template loading, with explicit settings taking precedence over defaults.

### Context Inheritance Modes

The system supports three context inheritance modes:
- **full**: Complete inheritance of parent context
- **subset**: Selective inheritance of specific context elements
- **none**: No context inheritance

When the "disable context" flag is active, the inherited context is completely omitted from the ContextGenerationInput.

```mermaid
flowchart TD
    A[Task Start] --> B{Context Mode?}
    B -->|Full| C[Use Parent Context]
    B -->|Subset| D[Use Selected Context]
    B -->|None| E[No Inherited Context]
    C --> F[Execute Task]
    D --> F
    E --> F
```

```mermaid
flowchart TD
    A[Atomic Task Begins] --> B[Check "Disable Context" Flag]
    B -- Yes --> C[Omit inheritedContext in ContextGenerationInput]
    B -- No --> D[Include parent context]
    C --> E[Pass to MemorySystem.getRelevantContextFor]
    D --> E
    E --> F[Heuristic Matching & Candidate Scoring]
    F --> G[Select Highest-scoring Template]
```

## Formal Definitions

Example ContextGenerationInput:
```json
{
  "taskText": "analyze experimental data",
  "inheritedContext": "context string if available",
  "previousOutputs": "summarized outputs if any"
}
```

Example AssociativeMatchResult:
```json
{
  "context": "retrieved relevant context",
  "matches": [
    ["fileA.txt", "metadata info"],
    ["fileB.txt", null]
  ]
}
```

See also:
- [ADR:002-context-management]
- [ADR:005-context-handling]

### Frame Creation and Extension
```typescript
interface ContextFrame {
    // Based on Memory System design (see [ADR:Memory:1.0])
    bindings: Map<string, any>;    // Current variable bindings
    context: Map<string, any>;     // Working memory context
    
    extend(bindings: Map<string, any>): ContextFrame;
    cleanup(): void;
}

---
**Related Decisions:** For higher‑level context management decisions, see [decisions/002-context-management.md](../decisions/002-context-management.md) and [decisions/005-context-handling.md](../decisions/005-context-handling.md).
```

## EnvironmentFrame Interface

To support argument passing during task evaluation, we introduce a new interface:

```typescript
interface EnvironmentFrame {
    bindings: Map<string, any>;    // Arguments bound to this scope
    parent?: EnvironmentFrame;     // Parent scope for lookup chain
    context: Record<string, any>;    // Task execution context (separate from bindings)
}
```

This `EnvironmentFrame` is created at the start of each task execution (using the new `createFrame` method) and is chained to any existing parent frame. Its purpose is to maintain a clear separation between argument bindings and the overall task context.

### Frame Immutability
- No modification of existing frames
- New frames created through extension
- Clear task isolation boundaries
- Minimal required context principle

### Memory System Integration
- Associative memory system mediates between long-term and working memory
- Working memory instantiated from long-term storage using associative retrieval
- Context updates managed through frame extension
- Resource tracking delegated to Handler

## Implementation Examples

### Frame Creation
```typescript
// Example based on [Component:Memory:1.0] implementation
class ContextFrame implements IContextFrame {
    private bindings: Map<string, any>;
    private context: Map<string, any>;
    
    extend(newBindings: Map<string, any>): ContextFrame {
        const frame = new ContextFrame();
        frame.bindings = new Map([...this.bindings, ...newBindings]);
        frame.context = this.context;  // Shared context reference
        return frame;
    }
    
    cleanup(): void {
        // Resource cleanup handled by Handler
        this.bindings.clear();
        this.context = null;
    }
}
</file>
<file path="./system/architecture/patterns/director-evaluator.md" project="">
# Director‑Evaluator Pattern [Pattern:DirectorEvaluator:1.1]

**Canonical Reference:** This document is the authoritative description of the Director‑Evaluator Pattern. All extended descriptions in planning or misc files should refer here.

## Overview

The Director-Evaluator pattern is a specialized variant of the unified task‐subtask execution model, implemented in two ways:

1. **Dynamic Variant**: A parent task (the **Director**) produces an initial output that may require evaluation. When the Director's output returns with a `CONTINUATION` status and an `evaluation_request` object in its `notes`, the evaluation subtask is spawned dynamically.

2. **Static Variant**: A predefined task structure of type `director_evaluator_loop` that explicitly defines the Director, Evaluator, and optional script execution steps with clear iteration control.

**Note:** The `evaluation_request` object must include:
 - `type`: a string indicating the evaluation type,
 - `criteria`: a free-form string providing descriptive criteria used for dynamic evaluation template selection via associative matching,
 - `target`: a string representing the target (for example, the bash script command to run or the evaluation focus).

## Pattern Description

### Dynamic Variant

This pattern follows a three-phase flow:

1. **Parent Task (Director):**
   - Generates an initial output based on the user's instructions.
   - May return a result with `status: 'CONTINUATION'` and a populated `evaluation_request` in its `notes`.
   - Uses a `<context_management>` block with `inherit_context` set to `none` to start a new execution chain.

2. **Evaluation Trigger via Continuation:**
   - The evaluation step is triggered dynamically when the Director task returns a `CONTINUATION` status.
   - The embedded `evaluation_request` specifies both:
       - The **bash script** (or external callback mechanism) to execute, and
       - The **target** string that describes the aspects of the output requiring evaluation.
   - The Evaluator uses this information—with associative matching—to select an appropriate evaluation task template.

3. **Child Task (Evaluator):**
   - Is dynamically spawned by the Evaluator when it detects a `CONTINUATION` status along with an `evaluation_request` in the Director's output.
   - Uses a `<context_management>` block with `inherit_context` set to `subset` and `accumulate_data` enabled to incorporate only the relevant context.
   - Executes the evaluation subtask—which may include invoking the specified bash script via the Handler or Evaluator—and feeds its results back to the parent task.

### Static Variant (Director-Evaluator Loop)

The static variant uses a dedicated task type with a standardized structure:

```xml
<task type="director_evaluator_loop">
  <description>{{task_description}}</description>
  <max_iterations>5</max_iterations>
  <context_management>
    <inherit_context>none</inherit_context>
    <accumulate_data>true</accumulate_data>
    <accumulation_format>notes_only</accumulation_format>
    <fresh_context>enabled</fresh_context>
  </context_management>
  <director>
    <description>Generate solution for {{original_prompt}}</description>
    <inputs>
      <input name="original_prompt" from="user_query"/>
      <input name="feedback" from="evaluation_feedback"/>
      <input name="iteration" from="current_iteration"/>
    </inputs>
  </director>
  <evaluator>
    <description>Evaluate solution against {{original_prompt}}</description>
    <inputs>
      <input name="solution" from="director_result"/>
      <input name="original_prompt" from="user_query"/>
    </inputs>
  </evaluator>
  <script_execution>
    <!-- Optional script execution -->
    <command>{{script_path}}</command>
    <timeout>300</timeout>
    <inputs>
      <input name="script_input" from="director_result"/>
    </inputs>
  </script_execution>
  <termination_condition>
    <!-- Optional early termination -->
    <condition>evaluation.success === true</condition>
  </termination_condition>
</task>
```

### Parameter Passing

The Director-Evaluator loop uses direct parameter passing rather than environment variables:

```mermaid
flowchart LR
    A[Parent Task] -->|"TaskResult{status:CONTINUATION,\n notes:{subtask_request}}"| B[Task System]
    B -->|"Template Selection\n& Direct Input Passing"| C[Subtask]
    C -->|"TaskResult"| D[Task System]
    D -->|"Resume with\n{subtask_result:TaskResult}"| A
```

### Result Structure

All task results follow a consistent base structure with extensions for specific needs:

```typescript
// Base task result structure
interface TaskResult {
    content: string;
    status: "COMPLETE" | "CONTINUATION" | "WAITING" | "FAILED";
    notes: {
        [key: string]: any;
    };
}

// Specialized structure for evaluator feedback
interface EvaluationResult extends TaskResult {
    notes: {
        success: boolean;        // Whether the evaluation passed
        feedback: string;        // Human-readable feedback message
        details?: {              // Optional structured details
            metrics?: Record<string, number>; // Optional evaluation metrics
            violations?: string[];            // Specific validation failures
            suggestions?: string[];           // Suggested improvements
            [key: string]: any;               // Extension point
        };
        scriptOutput?: {         // Present when script execution is involved
            stdout: string;      // Standard output from script
            stderr: string;      // Standard error output from script
            exitCode: number;    // Exit code from script
        };
    };
}
```

### Example Workflow

Below is a conceptual example (in pseudocode) illustrating the updated director-evaluator flow. In this scenario, the Director task returns a result with `status: 'CONTINUATION'` and an embedded `evaluation_request`:

```typescript
// Example TaskResult returned by the Director task
const taskResult: TaskResult = {
    content: "Initial solution output...",
    status: 'CONTINUATION',
    notes: {
        evaluation_request: {
            type: "bash_script",
            criteria: ["validate", "log"],
            target: "run_analysis.sh"
        }
    }
};
```

Upon receiving this result, the Evaluator:
1. Uses the `evaluation_request` details (including the target string) to perform associative matching and select an appropriate evaluation task template.
2. Dynamically spawns an evaluation subtask (the Child Task) with a `<context_management>` block set to inherit a subset of context.
3. If necessary, invokes the specified bash script callback via the Handler or its own mechanism.
4. Feeds the evaluation results back to the Director, allowing the overall task to continue.

## Integration with the Unified Architecture

### Context Management Defaults

The Director-Evaluator pattern has specific default context management settings:

| Task Type | inherit_context | accumulate_data | accumulation_format | fresh_context |
|-----------|-----------------|-----------------|---------------------|---------------|
| director_evaluator_loop | none | true | notes_only | enabled |
| director (component) | full | false | notes_only | disabled |
| evaluator (component) | full | false | notes_only | disabled |

These defaults adhere to the mutual exclusivity constraint: when `inherit_context` is "full", `fresh_context` must be "disabled".

These defaults can be overridden through explicit configuration:

```xml
<task type="director_evaluator_loop">
  <description>Iterative refinement process</description>
  <context_management>
    <inherit_context>none</inherit_context>
    <accumulate_data>true</accumulate_data>
    <accumulation_format>full_output</accumulation_format>
    <fresh_context>enabled</fresh_context>
  </context_management>
  <!-- other elements -->
</task>
```

The Director-Evaluator pattern fully embraces the hybrid configuration approach and integrates with the three-dimensional context management model:

- **Inherited Context**: The parent task's context, controlled by `inherit_context` setting.
- **Accumulated Data**: The step-by-step outputs collected during sequential execution, controlled by `accumulate_data` setting.
- **Fresh Context**: New context generated via associative matching, controlled by `fresh_context` setting.

When the context_management block is omitted, the operator-specific defaults apply. When present, explicit settings override the defaults, providing both consistency and flexibility.

### Script Execution Integration

Script execution is implemented as a tool call handled by the Handler:

```xml
<script_execution>
  <command>{{script_path}}</command>
  <timeout>300</timeout>
  <inputs>
    <input name="script_input" from="director_result"/>
  </inputs>
</script_execution>
```

When a script execution step is included:
- The script receives the Director's output as input
- Script output (stdout, stderr, exit code) is captured
- Results are passed to the Evaluator for processing
- The Evaluator considers both the original output and script results
- As a tool call, script execution does not use the continuation mechanism

## Relationship to Subtask Spawning

The Director-Evaluator Loop and Subtask Spawning mechanism are complementary features:

### Integration Points

1. **Dynamic Director-Evaluator Implementation**
   - The dynamic variant of Director-Evaluator uses the subtask spawning mechanism
   - When a Director task returns with `CONTINUATION` status and an `evaluation_request`, it's using subtask spawning
   - The Evaluator component handles the subtask creation and execution
   - Results flow back to the parent task using the standard subtask result passing

2. **Context Management Alignment**
   - Both patterns follow the hybrid configuration approach from [ADR 14: Operator Context Configuration]
   - Director-Evaluator Loop uses `inherit_context: none` by default
   - Subtasks use `inherit_context: subset` by default
   - Both can be explicitly configured through their respective XML structures

### Pattern Selection Guide

**Director-Evaluator Loop** is optimal for:
- Iterative refinement processes requiring multiple feedback cycles
- Workflows needing external validation via scripts with standardized input/output
- Scenarios with well-defined evaluation criteria and termination conditions
- Cases requiring preservation of iteration history for auditing or debugging

**Subtask Spawning** is better for:
- Ad-hoc dynamic task creation based on runtime discoveries
- Complex task trees with varying subtypes and unpredictable branching
- Workflows requiring specialized template selection per subtask
- Situations where flexibility in execution path is more important than iteration structure

The patterns can be used together, with Director-Evaluator loops spawning subtasks when needed for specialized processing.

## Conclusion

The Director-Evaluator pattern provides a structured approach to iterative refinement, with both dynamic and static variants. It uses direct parameter passing for clean data flow and integrates fully with the unified context management model. This approach ensures flexibility and seamless integration within the overall task execution architecture.
</file>
<file path="./system/architecture/patterns/error-resources.md" project="">
# Error Condition Resource Pattern [Pattern:ErrorResource:1.0]

## Resource Handling During Errors

From task-system/spec/behaviors.md and impl/resource-management.md:

### Required Actions
On error detection:
- Stop resource usage
- Complete resource accounting
- Begin cleanup process
- Surface error with metrics

### Cleanup Requirements
From task-system/impl/resource-management.md:
- Handler cleanup
- Resource release
- Memory cleanup
- Metric collection

## Implementation

```typescript
interface ErrorResourceMetrics {
    // From task-system/spec/types.md
    turns: {
        used: number;
        limit: number;
    };
    context: {
        used: number;
        limit: number;
        peakUsage: number;
    };
}
```</file>
<file path="./system/architecture/patterns/errors.md" project="">
# Error Handling and Recovery Pattern [Pattern:Error:1.0]

**Canonical Reference:** This document is the authoritative description of the Error Handling and Recovery Pattern. All extended descriptions in other files should refer here.

**Intended Focus:** This document covers overall error detection, classification, and recovery strategies. For resource‑related cleanup details, see the "Resource Cleanup" subsection below. (For low‑level resource metrics, refer to [Pattern:ResourceManagement:1.0] and related docs.)

## 1. Pattern Definition

### 1.1 Purpose
Error handling pattern for the task execution system, focusing on three key concerns:
- Resource exhaustion detection and recovery
- Invalid output handling
- Progress failure management

### 1.2 Context
This pattern is used by:
- [Component:TaskSystem:1.0] for error detection
- [Component:Evaluator:1.0] for error recovery
- [Component:Handler:1.0] for resource monitoring

### 1.3 Core Elements
- Error Type System: See [Type:TaskSystem:TaskError:1.0]
- Recovery Protocols: See [Protocol:Tasks:Reparse:1.0]
- Resource Contracts: See [Contract:Resources:1.0]

## 2. Error Categories

### 2.1 Resource Exhaustion
Handled by [Protocol:Tasks:Reparse:1.0]

Resource exhaustion occurs when a task exceeds allocated system resources:
- **Type**: 'RESOURCE_EXHAUSTION'
- **Resources**: 'turns' | 'context' | 'output'
- **Metrics**: Contains usage and limit values
- **Recovery**: Attempt task decomposition

### 2.2 Task Failure
Related to [Contract:Tasks:TemplateSchema:1.0]

Task failures now include a standardized `reason` field for more specific categorization:

```typescript
type TaskFailureReason = 
  | 'context_retrieval_failure'    // Failure to retrieve context data
  | 'context_matching_failure'     // Failure in associative matching algorithm 
  | 'context_parsing_failure'      // Failure to parse or process retrieved context
  | 'xml_validation_failure'       // Output doesn't conform to expected XML schema
  | 'output_format_failure'        // Output doesn't meet format requirements
  | 'execution_timeout'            // Task execution exceeded time limits
  | 'execution_halted'             // Task execution was deliberately terminated
  | 'subtask_failure'              // A subtask failed, causing parent task failure
  | 'input_validation_failure'     // Input data didn't meet requirements
  | 'unexpected_error';            // Catch-all for truly unexpected errors
```

Task failures also include a structured details object that may contain error-specific information such as:
- partial_context: Any partial context that was retrieved before failure
- context_metrics: Metrics related to context retrieval
- violations: Specific validation rule violations
- partialResults: Results from steps that completed before failure
- failedStep: Index of the step that failed in a sequential task

### 2.3 Failure to Make Progress
Progress failures occur when a task cannot advance despite resources being available:
- **Type**: 'TASK_FAILURE'
- **Reason**: 'execution_halted'
- **Indicators**: Multiple rounds with no state change
- **Recovery**: Alternative approach or termination

### 2.4 Partial Results Handling

The system maintains a standardized approach for preserving partial results when multi-step operations fail:

### Error Output Structure

#### Atomic Tasks
```typescript
// Successful atomic task
{
  content: "Complete task output",
  status: "COMPLETE", 
  notes: {
    dataUsage: "Resource usage statistics",
    successScore: 0.95
  }
}

// Failed atomic task
{
  content: "Partial output generated before failure",  // Partial content in content field
  status: "FAILED",
  notes: {
    dataUsage: "Resource usage statistics",
    executionStage: "validation",
    completionPercentage: 60
  }
}
```
Task status (`COMPLETE` vs `FAILED`) indicates whether content is complete or partial.

#### Sequential Tasks
```typescript
{
  type: 'TASK_FAILURE',
  reason: 'subtask_failure',
  message: 'Sequential task "Process Dataset" failed at step 3',
  details: {
    failedStep: 2,
    totalSteps: 5,
    partialResults: [
      { 
        stepIndex: 0, 
        content: "Data loaded successfully: 1000 records",
        notes: { 
          recordCount: 1000, 
          status: "completed"
        }
      },
      { 
        stepIndex: 1, 
        content: "Data transformed to required format",
        notes: { 
          transformType: "normalization", 
          status: "completed"
        }
      }
    ]
  }
}
```

#### Reduce Tasks
```typescript
{
  type: 'TASK_FAILURE',
  reason: 'subtask_failure',
  message: 'Reduce task "Aggregate metrics" failed processing input 2',
  details: {
    failedInputIndex: 2,
    totalInputs: 5,
    processedInputs: [0, 1],
    currentAccumulator: { totalCount: 1500, averageValue: 42.3 },
    partialResults: [
      { 
        inputIndex: 0, 
        content: "Processed metrics for server 1",
        notes: { 
          status: "completed",
          serverName: "server-01",
          metricsCount: 250
        }
      },
      { 
        inputIndex: 1, 
        content: "Processed metrics for server 2",
        notes: { 
          status: "completed",
          serverName: "server-02",
          metricsCount: 180
        }
      }
    ]
  }
}
```

#### Storage Format Control
The format of preserved partial results depends on the task's `accumulation_format` setting:
- `notes_only`: Only the notes field is preserved (default for memory efficiency)
- `full_output`: Both content and notes fields are preserved (with size limits)

```typescript
// With accumulation_format="notes_only"
partialResults: [
  { 
    stepIndex: 0, 
    notes: { 
      recordCount: 1000,
      status: "completed"
      // Other essential metadata
    }
  }
]

// With accumulation_format="full_output"
partialResults: [
  { 
    stepIndex: 0,
    content: "Complete step output text",
    notes: { 
      recordCount: 1000,
      status: "completed"
    }
  }
]
```

Each operator type has specific default settings for context management. For sequential tasks, the default `accumulation_format` is `minimal`. These defaults apply when the `context_management` block is omitted. When present, explicit settings override the defaults, following the hybrid configuration approach.

#### Size Management
To prevent memory issues:
- Individual step outputs are kept reasonably sized
- When accumulated data becomes too large, older results may be summarized or truncated
- The system indicates when truncation has occurred

### 2.5 Output Format Validation

When a task specifies an output format using `<output_format type="json" schema="...">`, validation failures result in:

```typescript
{
  type: 'TASK_FAILURE',
  reason: 'output_format_failure',
  message: 'Expected output of type "array" but got "object"',
  details: {
    expectedType: "array",
    actualType: "object",
    originalOutput: "..." // The original output
  }
}
```

This error occurs when:
1. The task specifies an output format with `type="json"`
2. The output is successfully parsed as JSON
3. The parsed content doesn't match the specified schema type

The original output is preserved in the `partialOutput` field to allow for potential recovery or manual parsing. The error includes both the expected and actual types to aid in debugging and recovery.

## 3. Recovery Process

### 3.1 Detection Phase
See [Interface:Handler:ResourceMonitoring:1.0]

Error detection now includes identifying:
- Context retrieval failures
- Context matching failures
- Context parsing failures
- Output format validation failures

```typescript
function handleTaskError(error: TaskError) {
  if (error.type === 'RESOURCE_EXHAUSTION') {
    // Handle resource exhaustion based on resource type
    handleResourceExhaustion(error);
  } else if (error.type === 'TASK_FAILURE') {
    // Handle task failure based on reason
    if (error.reason.startsWith('context_')) {
      // Handle context-related failures
      handleContextFailure(error);
    } else if (error.reason === 'subtask_failure') {
      // Handle subtask failures, potentially using partial results
      handleSubtaskFailure(error);
    } else if (error.reason === 'output_format_failure') {
      // Handle output validation failures
      handleOutputFormatFailure(error);
    } else {
      // Handle other failures
      handleGeneralFailure(error);
    }
  }
}
```

### 3.2 Planning Phase
See [Component:Evaluator:1.0] for error handling.

Based on error type and reason, the system will surface appropriate error information:
- **Resource Exhaustion**: Complete resource metrics and context
- **Context Failures**: Context-related error details
- **Validation Errors**: Validation failure specifics
- **Subtask Failures**: Error details including partial execution data

### 3.3 Execution Phase
See [Protocol:Tasks:Reparse:1.0] for execution details.

Error handling involves:
- Preparing complete error context
- Including relevant partial execution data
- Surfacing errors through standard error flow
- Providing detailed diagnostics

- **Associative Matching Failures:** If an associative matching task encounters an error—such as insufficient context or partial output—it will automatically trigger a retry. These errors will include any partial output and, if available, an optional success score (recorded in the task's `notes` field) to support future adaptive behavior.

#### Error Recovery Flow for Associative Matching

```mermaid
flowchart TD
    A[Associative Matching Task Initiated] --> B{Error Detected?}
    B -- Yes --> C[Trigger Automatic Retry]
    C --> D[Capture Partial Output & Success Score]
    D --> E[Reattempt Associative Matching]
    B -- No --> F[Proceed with Normal Execution]
```

### Subtask Failure Handling

When a subtask fails, the system provides a standardized error structure that preserves context and enables recovery:

```typescript
{
  type: 'TASK_FAILURE',
  reason: 'subtask_failure',
  message: 'Subtask "Process complex data" failed',
  details: {
    subtaskRequest: {
      type: 'atomic',
      description: 'Process complex data',
      inputs: { /* original inputs */ }
    },
    subtaskError: {
      type: 'TASK_FAILURE',
      reason: 'execution_halted',
      message: 'Failed to process data format'
    },
    nestingDepth: 2,
    partialOutput: "Partial processing results before failure"
  }
}
```

This standardized structure provides several benefits:
1. Complete error context preservation
2. Clear indication of which subtask failed
3. Access to the original subtask request for potential retry
4. Preservation of partial results for recovery

Example of parent task handling subtask failures:
```typescript
try {
  const result = await taskSystem.executeTask(complexTask);
} catch (error) {
  if (error.type === 'TASK_FAILURE' && error.reason === 'subtask_failure') {
    console.log(`Subtask failed: ${error.details.subtaskRequest.description}`);
    
    // Access the original subtask request for potential retry
    const modifiedRequest = {
      ...error.details.subtaskRequest,
      description: `Retry: ${error.details.subtaskRequest.description} with simplified approach`
    };
    
    // Use partial results if available
    if (error.details.partialOutput) {
      console.log(`Using partial output: ${error.details.partialOutput}`);
    }
    
    // Attempt recovery with modified request
    const recoveryResult = await taskSystem.executeTask(modifiedRequest);
  }
}
```

### 3.4 Validation Phase
Recovery validation includes:
- Verifying resource usage of recovery approach
- Ensuring progress is made
- Validating output structure and format
- Limiting recovery depth to prevent infinite loops

## 4. Pattern Examples
See components/task-system/impl/examples.md for concrete examples.

### Context Retrieval Failure Example
```typescript
// Memory System's file index is corrupted or unavailable
const error = {
  type: 'TASK_FAILURE',
  reason: 'context_retrieval_failure',
  message: 'Failed to access memory system index',
  details: { error: 'Index corruption detected' }
};

// Recovery involves alternative context strategy
const recovery = await evaluator.recoverFromContextFailure(error);
```

### Sequential Task Failure Example
```typescript
try {
  const result = await taskSystem.executeTask(
    "process data in multiple steps",
    memorySystem
  );
} catch (error) {
  if (error.type === 'TASK_FAILURE' && error.reason === 'subtask_failure') {
    console.log(`Failed at step ${error.details.failedStep} of ${error.details.totalSteps}`);
    
    // Access partial results from completed steps
    error.details.partialResults.forEach(result => {
      console.log(`Step ${result.stepIndex} output: ${result.output}`);
    });
    
    // Potentially use partial results for recovery
    const recoveryResult = await evaluator.recoverWithPartialResults(error);
  }
}
```

## 5. Known Limitations
- **Recovery Depth**: Limited to prevent infinite loops
- **Partial Result Size**: May be truncated for very large outputs
- **Stateful Recovery**: Not supported across sessions
- **Complex Dependencies**: Recovery may not work for deeply nested failures

## 6. Related Patterns
- [Pattern:ResourceManagement:1.0]
- [Pattern:TaskExecution:2.0]
</file>
<file path="./system/architecture/patterns/resource-warnings.md" project="">
<!-- This file has been consolidated into resource-management.md. Refer to the "Warning Thresholds" subsection there. -->
</file>
<file path="./system/architecture/patterns/tool-interface.md" project="">
# Unified Tool Interface Pattern [Pattern:ToolInterface:1.0]

**Canonical Reference:** This document is the authoritative description of the Unified Tool Interface Pattern. All extended descriptions in other files should refer here.

## Purpose

Define a consistent tool-based delegation model for LLMs that unifies direct operations (file I/O, scripts) and complex operations (subtasks, evaluations) under a common interface while maintaining separate implementation mechanisms.

## Pattern Description

The Unified Tool Interface pattern separates the LLM-facing interface from the underlying implementation mechanisms:

1. **Interface Layer**: What the LLM sees and interacts with
   - Consistent tool-based invocation pattern
   - Standardized parameter schemas
   - Unified error handling

2. **Implementation Layer**: How tools are executed
   - **Direct Tools**: Synchronous execution via Handler
   - **Subtask Tools**: Asynchronous execution via CONTINUATION mechanism

```mermaid
flowchart TD
    A[LLM] -->|Calls| B[Unified Tool Interface]
    B -->|Direct Operations| C[Handler Tools]
    B -->|Complex Operations| D[Subtask Mechanism]
    C -->|Synchronous| E[File Systems/Scripts]
    D -->|Asynchronous| F[Template Selection]
    F -->|CONTINUATION| G[Child LLM Tasks]
```

## Interface vs. Implementation

### What the LLM Sees

From the LLM's perspective, all tools share a common invocation pattern:

```python
# Direct tool example
result = tools.readFile("path/to/file")

# Subtask tool example
result = tools.analyzeData({
  "data": fileContent,
  "method": "statistical" 
})
```

### How It's Implemented

Behind the interface, tools operate differently:

1. **Direct Tools**:
   - Executed synchronously by the Handler
   - No complex context management
   - Predictable resource usage
   - Examples: file operations, script execution, API calls

2. **Subtask Tools**:
   - Implemented via CONTINUATION mechanism
   - Managed by Task System and Evaluator
   - Full context management capabilities
   - Depth tracking and cycle detection
   - Examples: data analysis, creative generation, complex reasoning

## Implementation Details

### Handler Responsibilities

The Handler component exposes both types of tools through a unified registry:

```typescript
interface ToolRegistry {
  registerDirectTool(name: string, handler: Function): void;
  registerSubtaskTool(name: string, templateHints: string[]): void;
  
  // Called by LLM through tool interface
  invokeTool(name: string, params: any): Promise<any>;
}
```

When a tool is invoked:
1. Handler determines tool type (direct or subtask)
2. For direct tools: executes immediately and returns result
3. For subtask tools: creates CONTINUATION with SubtaskRequest and yields

## Subtask Results as Tool Responses

When a subtask tool is called, the system implements a streamlined approach that preserves session continuity:

```mermaid
flowchart TD
    A[Parent LLM] -->|"tools.analyzeData({...})"| B[Handler]
    B -->|CONTINUATION with subtask_request| C[Task System]
    C -->|Execute subtask| D[Subtask LLM]
    D -->|Result| C
    C -->|Add as tool response| B
    B -->|Continue with tool result| A
```

### Key Benefits
1. **Session Preservation**: The parent Handler session is maintained throughout execution
2. **Natural Conversation Flow**: From the LLM's perspective, this is just a tool call and response
3. **No Special Methods**: No need for `resumeTask()` or similar special continuation methods
4. **Simplified Implementation**: Clean component boundaries with clear responsibilities

### Example Flow
1. Parent LLM calls a subtask tool (e.g., `tools.analyzeData({...})`)
2. Handler recognizes this as a subtask tool and returns CONTINUATION with subtask_request
3. Task System executes the subtask as a separate LLM interaction
4. Task System adds the subtask result to the parent's session as a tool response
5. Parent LLM continues execution with the tool result in its conversation history

This approach eliminates the need for the LLM to understand continuation concepts - it simply sees its tool call and the corresponding response.

### Task System Integration

For subtask tools, the Task System:
1. Receives CONTINUATION status with subtask_request
2. Selects appropriate template using associative matching
3. Executes subtask with context management
4. Returns result to parent task

## Usage Examples

### Direct Tool Example

```typescript
// LLM invocation
const fileContent = tools.readFile("data.csv");

// Implementation (synchronous via Handler)
async function readFile(path: string): Promise<string> {
  return await fs.readFile(path, 'utf8');
}
```

### Subtask Tool Example

```typescript
// LLM invocation
const analysis = tools.analyzeData({
  "data": fileContent,
  "method": "statistical"
});

// Implementation (asynchronous via CONTINUATION)
async function analyzeData(params: any): Promise<any> {
  return {
    status: "CONTINUATION",
    notes: {
      subtask_request: {
        type: "atomic",
        description: `Analyze data using ${params.method} method`,
        inputs: { data: params.data, method: params.method },
        template_hints: ["data_analysis", "statistics"]
      }
    }
  };
}
```

## Selection Criteria

Use **Direct Tools** when:
- Operations are deterministic with predictable outputs
- No complex reasoning is required
- Response format is simple and structured
- Resource usage is predictable

Use **Subtask Tools** when:
- Complex reasoning or creativity is required
- Dynamic template selection would be beneficial
- Operation might involve multiple steps or iterations
- Context management needs are sophisticated

## Integration with Other Patterns

This pattern complements:
- [Pattern:DirectorEvaluator:1.1] - Can use both mechanism types
- [Pattern:SubtaskSpawning:1.0] - Provides tool interface for spawning
- [Pattern:ContextFrame:1.0] - Works with context management model
- [Pattern:Error:1.0] - Unified error handling across mechanisms

## Constraints

1. **Naming Consistency**: Tool names must be unique across both direct and subtask tools
2. **Parameter Validation**: All tools must validate parameters regardless of implementation
3. **Error Propagation**: Error handling must be consistent across both mechanisms
4. **Resource Tracking**: Both mechanisms must track resource usage appropriately

### User Input Request Tool

The system provides a standardized tool for requesting user input:

```typescript
// Standard tool for requesting user input
const USER_INPUT_TOOL: ToolDefinition = {
  name: "requestUserInput",
  description: "Request input from the user when additional information is needed",
  parameters: {
    type: "object",
    properties: {
      prompt: {
        type: "string",
        description: "The question or prompt to show to the user"
      }
    },
    required: ["prompt"]
  }
};

// LLM usage
const userAnswer = tools.requestUserInput({
  prompt: "What file would you like to analyze?"
});

// Handler implementation
class Handler implements IHandler {
  constructor(config: HandlerConfig) {
    // Register standard tools
    this.registerDirectTool(USER_INPUT_TOOL.name, this.handleUserInputRequest.bind(this));
  }
  
  private async handleUserInputRequest(params: {prompt: string}): Promise<{userInput: string}> {
    if (!this.onRequestInput) {
      throw new Error("No input request handler registered");
    }
    
    const userInput = await this.onRequestInput(params.prompt);
    this.session.addUserMessage(userInput);
    
    return { userInput };
  }
}
```

This standardized approach allows LLMs to consistently request user input across different providers while maintaining proper conversation tracking and resource management.

## Implementation Guidance

1. Register direct tools during Handler initialization
2. Define subtask tools in the TaskLibrary with template hints
3. Implement invocation routing in the Handler
4. Ensure consistent error handling in both paths
5. Provide clear documentation of available tools to the LLM
</file>
<file path="./system/architecture/patterns/resource-management.md" project="">
# Resource Management Pattern [Pattern:ResourceManagement:1.0]

**Canonical Reference:** This document is the authoritative description of the Resource Management Pattern. All extended descriptions in other files should refer here.

## Purpose & Constraints
This document defines the resource management strategy for the task execution system. It covers:
 - Memory Hierarchy
 - Resource Tracking (turn counts, context window, token usage)
 - Warning Thresholds (e.g. 80% limits)
 - Cleanup Procedures

**Note:** Warning signals are purely informative; hard limits trigger termination.

## Core Components

### Memory Hierarchy
1. **Long‑term Memory:** Stores data and procedures; accessed via the Memory System (read‑only during task execution).
2. **Working Memory:** The active computation space (task‑specific context) managed through Environment objects; cleared after task completion.
3. **Context Frames:** Capture complete execution environments (bindings and working memory) using a minimal‑context extension pattern.

### Resource Tracking

The Handler is responsible for:
 - Tracking turn counts and enforcing limits.
 - Monitoring context window usage (tokens) with warning thresholds (e.g. 80%) and hard limits.
 - Reporting resource metrics for error handling.

### Resource Tracking

#### Handler Responsibility
- One Handler per task execution
- Creates a HandlerSession to manage conversation state
- Tracks resource usage through session:
  * Turn counts (incremented for assistant messages)
  * Message history (all user and assistant messages)
  * Context window size (including all conversation history)
  * Token usage and peak usage statistics
- Enforces resource limits at session level
- Manages clean termination
- Structures complete interaction payload via HandlerPayload

#### Session Management
- Maintains complete conversation history
- Tracks message timestamps and roles
- Manages turn counting (assistant messages only)
- Constructs provider-agnostic payloads
- Provides resource metrics for monitoring
- Enforces limits based on configuration

### Context Management

#### Window Management
- Token-based calculation
- Explicit size monitoring
- Fraction-based limits
- No content optimization
- Warning thresholds at 80%
- Hard limits with error handling

#### Context Preservation

- The nested Environment model ensures that each child task gets its own context.
- Proper chaining of environments prevents context leakage.
- InheritedContext and any built-in objects (like TaskLibrary) persist unchanged in the chain.
- Context frames capture environments
- Minimal required context per task
- Associative memory mediation
- Clean extension mechanism

## Resource Configuration

### Default Configuration
- Base resource limits
- Warning thresholds
- Default turn counts
- Context window limits

### Per-Task Overrides
- Task-specific limits
- Custom thresholds
- Resource constraints
- Performance targets

## Script Execution Resource Management

- Script tasks (type="script") are executed by the Handler.
- The Handler captures stdout, stderr, and exitCode for script tasks.
- A non-zero exitCode is treated as a TASK_FAILURE error.

Example interface:
```typescript
interface ScriptTaskResult {
    stdout: string;
    stderr: string;
    exitCode: number;
}
```

## Interactions

### With Memory System [Component:Memory:3.0]
- Maintains global file metadata index
- Provides associative matching services
- Manages read-only context retrieval
- No file content operations (delegated to Handler)

### With Task System [Component:TaskSystem:1.0]
- Creates/manages Handlers
- Enforces resource limits
- Manages task execution
- Handles resource errors

### With Handler [Component:Handler:1.0]
- Tracks resource usage
- Enforces limits
- Manages session lifecycle
- Reports resource metrics

## Implementation Requirements

### Resource Tracking
```typescript
// Resource metrics definition moved to spec/types.md
// See [Type:ResourceMetrics:1.0] for the complete interface
```

### Handler Configuration
```typescript
interface HandlerConfig {
    maxTurns: number;
    maxContextWindowFraction: number;
    warningThreshold: number;
    defaultModel?: string;
    systemPrompt: string;
}
```

## Error Handling

### Resource Exhaustion
- Immediate task termination
- Clean resource release
- Error surfacing with metrics
- No automatic retry

### Context Overflow
- Token limit enforcement
- Warning at threshold
- Clean termination
- Context metrics reported

### Progress Failure
- Resource accounting completion
- State cleanup
- Error propagation
- Recovery guidance

### Sequential Task Resources

For **sequential** tasks, the Evaluator maintains a step-by-step record of partial results and history, often called `SequentialHistory`. This history is **not** tracked against the Handler's context window limits. Instead:

- **Evaluator** owns the entire sequential history for multi-step tasks.
- **Handler** continues to track standard resource usage (turns, tokens).
- Because the sequential history is purely textual or metadata that the Evaluator stores separately, it does not consume the Handler's context window.
- If the task is configured to accumulate data (`<accumulate_data>true</accumulate_data>`), the Evaluator may pass prior step outputs into `MemorySystem.getRelevantContextFor()`.
- The Evaluator maintains a SequentialHistory for sequential (type="sequential") tasks.
- The SequentialHistory (holding step outputs and metadata) is not counted against the Handler’s context window limits.
- Accumulated outputs are retained for the entire sequential task execution loop and then discarded (or archived) once the sequence completes.

Example interface:
```typescript
interface SequentialHistory {
    outputs: TaskOutput[];
    metadata: { startTime: Date; currentStep: number; resourceUsage: ResourceMetrics; };
}
interface TaskOutput {
    stepId: string;
    output: string;
    notes: string;
    timestamp: Date;
}
```
- This separation ensures Handler resource metrics stay consistent and the Evaluator can keep any relevant partial outputs for as long as needed.
- Once the sequential task completes (success or failure), the Evaluator discards (or archives) the sequential history to free memory.

This design ensures a clean boundary between higher-level multi-step results (owned by the Evaluator) and resource usage constraints (handled by the Handler).

## Related Patterns
- [Pattern:Error:1.0] - Error handling strategy
- [Pattern:TaskExecution:1.0] - Task execution flow
</file>
<file path="./system/architecture/qa/resource-management.md" project="">
# Resource Management Q&A

## Memory Types & Organization

Q: What types of memory need to be managed?
A: The system has a hierarchical memory design:
- Long-term memory for data and procedures
- Working memory for active computations
- Context frames that capture execution environments (including working memory)
- Variable bindings within environments

Q: How is context organized and managed?
A: Through Environment objects that combine:
- Variable bindings (local scope)
- Working memory context
- Context frames are passed and extended through task execution

Q: How is minimal required context determined?
A: Context selection is handled at the LLM + prompt level, not by the system architecture. This allows for more intelligent and flexible context selection based on task requirements.

## Resource Ownership & Isolation

Q: Who owns and tracks resources?
A: Per existing documentation:
- Handler-based resource tracking
- One Handler per task execution
- No cross-Handler resource pooling
- Per-session resource isolation

Q: How are resources allocated and released?
A: As documented:
- Clean resource release on completion
- Turn limit passed during Handler initialization
- Default resource limits from config
- Per-task limit overrides possible

Q: How are memory operation failures handled?
A: Through a simple retry mechanism:
- Failed operations should be retried once
- If retry fails, an informative error is surfaced
- No complex recovery mechanisms in MVP

## Context Window Management

Q: How is the context window managed?
A: Through explicit tracking:
- Token-based calculation
- Window size monitoring
- Fraction-based limits
- No content optimization
- Warning at 80% threshold
- Error at limit reached

Q: How does context preservation work between tasks?
A: Through the Environment system:
- Context frames capture complete execution environments
- Each task receives minimal required context
- Associative memory system mediates between long-term and working memory
- Environment.extend() creates new contexts with additional bindings
- No context merging in MVP - extension patterns only

## Resource Metrics & Limits

Q: What resource metrics are tracked?
A: Currently documented:
- Turn counts
- Context window size
- Token usage
- Peak usage statistics

Q: How are resource limits handled?
A: As specified:
- Default limits from config
- Per-task overrides possible
- Warning thresholds (80%) are purely informative
- Hard limits with error handling
- Clean termination on limit violation
</file>
<file path="./system/architecture/decisions/completed/014-operator-ctx-config.md" project="">
# ADR 14: Operator Context Configuration [Summary]

## Status
Completed - Full version moved to completed_plans directory

## Context
Operators needed a standardized way to configure context management with sensible defaults for different operator types.

## Decision
Implemented a hybrid configuration approach where:
- Each operator type has specific default settings
- Explicit settings in `context_management` blocks override defaults
- Sequential tasks default to `notes_only` accumulation format
- Context inheritance and accumulation behaviors are configurable

## Consequences
- Reduced boilerplate in task definitions
- Improved memory efficiency with appropriate defaults
- Maintained flexibility through explicit overrides
- Standardized context management across operator types
</file>
<file path="./system/architecture/decisions/completed/011-subtask-spawning.md" project="">
# ADR 11: Subtask Spawning Mechanism (Summary)

## Status
Accepted

## Context
The system needed a standardized mechanism for dynamic task creation and composition. Previous implementations of continuation-based task spawning were inconsistent across components, with varying approaches to context management, parameter passing, and error handling.

## Decision
Implement a standardized subtask spawning mechanism with the following key features:

1. **Standardized Request Structure**: A well-defined `SubtaskRequest` interface with required fields (type, description, inputs) and optional fields (template_hints, context_management, max_depth, subtype).

2. **Direct Parameter Passing**: All data flow uses direct parameter passing rather than environment variables, ensuring clear dependencies and improved testability.

3. **Context Management Integration**: Default settings with optional overrides following the hybrid approach from ADR 14:
   - `inherit_context: subset` (default)
   - `accumulate_data: false` (default)
   - `accumulation_format: notes_only` (default)
   - `fresh_context: enabled` (default)

4. **Depth Control**: Maximum nesting depth (default: 5) and cycle detection to prevent infinite recursion.

5. **Error Handling**: Standardized error structure that preserves the complete error context, allowing for potential recovery strategies.

6. **Resource Protection**: Resource tracking across the entire subtask chain to prevent exhaustion.

## Consequences
- Consistent subtask spawning behavior across all components
- Clear data flow with explicit dependencies
- Controlled resource usage with protection against infinite recursion
- Improved error handling with context preservation
- Flexible context management with sensible defaults

The full ADR has been moved to ../../../../completed_plans/ for space efficiency.
</file>
<file path="./system/architecture/decisions/completed/009-partial-results.md" project="">
# ADR 9: Partial Results Policy (Summary)

## Status
Accepted and Implemented

## Context
When multi-step operations fail partway through execution, the system needs a standardized approach for preserving and exposing partial results. This is particularly important for sequential and reduce tasks where significant work may have been completed before failure.

## Decision
We will implement a standardized approach for preserving partial results across different task types:

1. **Atomic Tasks**: Store partial content in `notes.partialOutput`
2. **Sequential Tasks**: Store step-by-step outputs in `details.partialResults` with metadata
3. **Reduce Tasks**: Store processed input results and current accumulator state

The format of preserved partial results will be controlled by the task's `accumulation_format` setting:
- `notes_only`: Only summary information (default for memory efficiency)
- `full_output`: Complete outputs (with size limits)

## Consequences
- Improved debugging and recovery capabilities
- Consistent error structures across task types
- Memory usage management through format control
- Ability to resume or recover from partial execution

## Implementation Notes
- All partial results storage is ephemeral (in-memory only)
- Size limits prevent memory issues with large partial outputs
- Recovery is limited to simple retries in the MVP
- Error structures are consistent with the error taxonomy in ADR 8

## Related Documents
- Full ADR: ../../../../completed_plans/adr9-partial-results.md
- Error Handling: ../patterns/errors.md
</file>
<file path="./system/architecture/decisions/completed/010-evaluator-director.md" project="">
# ADR 10: Evaluator-to-Director Feedback Flow [Summary]

## Status
Accepted

## Context
The system needed a standardized approach for feedback flow between Director and Evaluator components, replacing ad-hoc environment variable usage with direct parameter passing.

## Decision
1. Replace environment variable references (e.g., `last_evaluator_output`) with direct parameter passing
2. Add a dedicated `director_evaluator_loop` task type
3. Update XML schema and type definitions
4. Standardize the Director-Evaluator pattern with both dynamic and static variants

## Consequences
### Positive
- Clearer data flow between components
- Improved testability with explicit dependencies
- Better support for iterative refinement processes
- Consistent XML schema for both variants

### Negative
- Required updates to existing documentation and code
- Slightly more verbose XML structure

## Implementation
The implementation included:
- Adding `director_evaluator_loop` task type to TaskType enum
- Creating DirectorEvaluatorLoopTask interface
- Standardizing EvaluationResult structure
- Updating XML schema with new elements
- Removing environment variable references

See the full ADR for complete implementation details.
</file>
<file path="./system/architecture/decisions/completed/013-json-output.md" project="">
# ADR 13: JSON-based Output Standardization (Summary)

## Status
Accepted and Implemented

## Context
The system needed a standardized approach for handling structured outputs from tasks. Previously, tasks returned outputs in arbitrary text formats, leading to type ambiguity, structure inconsistency, parsing burden, error-prone data exchange, and poor composability.

## Decision
We implemented a JSON-based output standardization mechanism with the following key features:

1. Added an `<output_format>` XML element to specify structured output requirements:
   ```xml
   <output_format type="json" schema="string[]" />
   ```

2. Added a `returns` attribute to function templates for type validation:
   ```xml
   <template name="get_file_info" params="filepath" returns="object">
   ```

3. Enhanced TaskResult interface with a `parsedContent` property to store parsed JSON data.

4. Added output format validation with specific error handling for format violations.

5. Implemented automatic JSON detection and parsing for tasks with `type="json"`.

## Consequences
- Tasks can now specify structured output formats with validation
- Function templates can declare return types
- Consumers can access parsed JSON data directly via the TaskResult interface
- Type validation ensures outputs match specified schemas
- Error handling provides clear feedback for format violations

## Implementation
The implementation includes:
- XML schema updates for `<output_format>` and `returns` attributes
- TaskResult interface enhancements
- parseTaskOutput method in TaskSystem interface
- Output validation logic
- Error handling for format violations

## References
Full ADR: ../../../../completed_plans/013-json-output.md
</file>
<file path="./system/architecture/decisions/completed/012-function-based-templates.md" project="">
# ADR 12: Function-Based Template Model

## Status
Accepted

## Context
The system needed a cleaner approach to variable scoping and parameter passing between tasks. The previous model allowed implicit access to parent environment variables through `{{variable_name}}` syntax, which created unclear dependencies and made reasoning about scope difficult.

## Decision
Implement a function-based template model with explicit parameter declarations:

1. **Template Definition**
   - Templates explicitly declare parameters using a `params` attribute
   - Each template has its own lexical scope containing only its parameters
   - Templates are registered in a central TaskLibrary

2. **Function Calling**
   - Function calls use positional arguments evaluated in the caller's context
   - Arguments can be literals, variable references, or nested expressions
   - A new environment is created for each function call with bindings for parameters
   - No implicit access to the caller's environment is allowed

3. **XML Representation**
   ```xml
   <!-- Template definition -->
   <template name="analyze_data" params="dataset,config">
     <task>
       <description>Analyze {{dataset}} using {{config}}</description>
     </task>
   </template>

   <!-- Function call -->
   <call template="analyze_data">
     <arg>weather_data</arg>
     <arg>standard_config</arg>
   </call>
   ```

## Consequences
- **Positive**
  - Clear data dependencies between components
  - Improved reasoning about variable scope
  - Better encapsulation of implementation details
  - Foundation for more advanced functional patterns
  - Explicit parameter documentation

- **Negative**
  - Slightly more verbose syntax
  - Migration effort for existing templates
  - Additional parsing complexity

## Implementation
The implementation includes:
- New AST node types: TemplateNode, FunctionCallNode, ArgumentNode
- Environment extension mechanism for creating child scopes
- Parameter binding during function calls
- Argument resolution strategy for variable references vs. literals

The full ADR with implementation details has been moved outside the docs/ tree to save space.
</file>
<file path="./system/architecture/decisions/10-evaluator-director.md" project="">
# Architecture Decision Record: Evaluator-to-Director Feedback Flow

## Status
Accepted

## Context
The Director-Evaluator pattern is a foundational component of our architecture that allows tasks to generate output, have it evaluated, and receive feedback for further refinement. Current implementation approaches lack standardization regarding context management, data passing between components, and iteration control.

## Decision
We will implement a unified Director-Evaluator pattern with direct result passing between components, explicit context management, standardized result structures, and configurable iteration control through a dedicated task type.

## Specification

### 1. Director-Evaluator Loop Task Structure

```xml
<task type="director_evaluator_loop">
  <description>{{task_description}}</description>
  <max_iterations>5</max_iterations>
  <context_management>
    <inherit_context>none</inherit_context>
    <accumulate_data>true</accumulate_data>
    <accumulation_format>notes_only</accumulation_format>
    <fresh_context>enabled</fresh_context>
  </context_management>
  <director>
    <description>Generate solution for {{original_prompt}}</description>
    <inputs>
      <input name="original_prompt" from="user_query"/>
      <input name="feedback" from="evaluation_feedback"/>
      <input name="iteration" from="current_iteration"/>
    </inputs>
  </director>
  <evaluator>
    <description>Evaluate solution against {{original_prompt}}</description>
    <inputs>
      <input name="solution" from="director_result"/>
      <input name="original_prompt" from="user_query"/>
    </inputs>
  </evaluator>
  <script_execution>
    <!-- Optional script execution -->
    <command>{{script_path}}</command>
    <timeout>300</timeout>
    <inputs>
      <input name="script_input" from="director_result"/>
    </inputs>
  </script_execution>
  <termination_condition>
    <!-- Optional early termination -->
    <condition>evaluation.success === true</condition>
  </termination_condition>
</task>
```

### 2. Standardized Result Structure

```typescript
// Base task result structure - consistent with other task types
interface TaskResult {
    content: string;
    status: "COMPLETE" | "CONTINUATION" | "WAITING" | "FAILED";
    notes: {
        [key: string]: any;
    };
}

// Specialized structure for evaluator feedback
interface EvaluationResult extends TaskResult {
    notes: {
        success: boolean;        // Whether the evaluation passed
        feedback: string;        // Human-readable feedback message
        details?: {              // Optional structured details
            metrics?: Record<string, number>; // Optional evaluation metrics
            violations?: string[];            // Specific validation failures
            suggestions?: string[];           // Suggested improvements
            [key: string]: any;               // Extension point
        };
        scriptOutput?: {         // Present when script execution is involved
            stdout: string;      // Standard output from script
            stderr: string;      // Standard error output from script
            exitCode: number;    // Exit code from script
        };
    };
}
```

### 3. Direct Parameter Passing

The Director-Evaluator loop uses direct parameter passing rather than environment variables:

```typescript
async function executeDirectorEvaluatorLoop(task, inputs) {
  const maxIterations = task.maxIterations || 5;
  const results = [];
  
  let directorOutput = null;
  let evaluationResult = { success: false, feedback: "" };
  
  for (let iteration = 0; iteration < maxIterations; iteration++) {
    // Execute director with current state via direct parameter passing
    directorOutput = await executeTask(
      task.director,
      {
        ...inputs,
        current_iteration: iteration,
        evaluation_feedback: evaluationResult.feedback,
        previous_results: results.length > 0 ? results : undefined
      }
    );
    
    // Store the director's result
    results.push({ iteration, output: directorOutput, evaluation: null });
    
    // Execute script if present - direct parameter passing
    let scriptOutput = null;
    if (task.script_execution) {
      scriptOutput = await executeScript(task.script_execution, {
        script_input: directorOutput.content
      });
    }
    
    // Execute evaluator - direct parameter passing
    evaluationResult = await executeTask(
      task.evaluator,
      {
        solution: directorOutput.content,
        original_prompt: inputs.user_query,
        script_output: scriptOutput
      }
    );
    
    // Update stored evaluation
    results[results.length - 1].evaluation = evaluationResult;
    
    // Check termination condition
    if (evaluationResult.notes.success || 
        (task.termination_condition && evaluateCondition(task.termination_condition, evaluationResult))) {
      break;
    }
  }
  
  // Return final result and history via direct parameter passing
  return {
    final_output: directorOutput,
    final_evaluation: evaluationResult,
    success: evaluationResult.notes.success,
    iterations_completed: results.length,
    iteration_history: results
  };
}
```

### 4. Iteration Control

Director-Evaluator loops include explicit iteration control:

1. **Maximum Iterations**: Default value of 5, configurable via `<max_iterations>` element
2. **Early Termination**: Configurable via `<termination_condition>` element
3. **Automatic Success Termination**: Stops when evaluator returns `success: true`
4. **Complete History**: All iterations and evaluations preserved in the result

### 5. Context Management Integration

Director-Evaluator fully integrates with the three-dimensional context management model:

- **inherit_context**: Controls whether parent context flows into the loop
  - Default: "none" (fresh start for each loop)
- **accumulate_data**: Controls whether results accumulate between iterations
  - Default: "true" (preserving iteration history)
- **accumulation_format**: Controls detail level of accumulated data
  - Default: "notes_only" (summary information)
- **fresh_context**: Controls whether additional context is retrieved
  - Default: "enabled" (allowing retrieval of relevant information)

### 6. Error Handling

Two types of errors are handled:

1. **Negative Evaluations**: Normal flow where evaluator indicates improvement needed (`success: false`)
   - Loop continues until max iterations or termination condition
   - Full details preserved in iteration_history

2. **Execution Errors**: When director, evaluator, or script execution fails:
   - Error fully captured with details, including partial results
   - Loop terminates, returning both results up to failure point and error information

## Relationship to Subtask Spawning

The Director-Evaluator Loop and Subtask Spawning mechanism are complementary features:

| Director-Evaluator Loop | Subtask Spawning Mechanism |
|-------------------------|----------------------------|
| Specialized higher-level pattern | General-purpose primitive |
| Built for iterative refinement | Ad-hoc dynamic task creation |
| Predefined iteration structure | Flexible composition pattern |
| Built-in termination conditions | Manual continuation control |

**When to use Director-Evaluator Loop:**
- Iterative refinement processes
- Create-evaluate feedback cycles
- Multiple potential iterations
- External validation via scripts

**When to use Subtask Spawning:**
- One-off subtask creation
- Dynamic task composition
- Task flows that aren't primarily iterative
- Complex task trees with varying subtypes

## Implementation Requirements

1. **Task System Enhancements**
   - Add `director_evaluator_loop` task type support
   - Implement iteration management
   - Add termination condition evaluation
   - Provide iteration history tracking

2. **Testing Requirements**
   - Unit tests for iteration control
   - Integration tests for data passing between components
   - Script execution validation tests
   - Performance tests for varying iteration counts

## Migration Guidance

### From Previous Version
1. Replace environment variable usage with direct parameter passing:
   ```typescript
   // Old approach - using environment variables
   env.set('last_evaluator_output', evaluationResult);
   
   // New approach - direct parameter passing
   return executeTask(task.director, { 
     evaluation_feedback: evaluationResult.feedback 
   });
   ```

2. Add explicit context_management blocks:
   ```xml
   <!-- Old version - implicit context -->
   <task type="director">...</task>
   
   <!-- New version - explicit context management -->
   <task type="director_evaluator_loop">
     <context_management>
       <inherit_context>none</inherit_context>
       <accumulate_data>true</accumulate_data>
       <accumulation_format>notes_only</accumulation_format>
       <fresh_context>enabled</fresh_context>
     </context_management>
     ...
   </task>
   ```

3. Standardize result structures:
   ```typescript
   // Old approach - varied formats
   return { success: true, message: "Looks good" };
   
   // New approach - standardized format
   return {
     content: "Evaluation complete",
     status: "COMPLETE",
     notes: {
       success: true,
       feedback: "Looks good",
       details: { metrics: { accuracy: 0.95 } }
     }
   };
   ```

## Performance Considerations

- **Iteration Limits**: Default max_iterations (5) suitable for most use cases; increase with caution
- **Context Growth**: Monitor accumulated context size, especially with accumulation_format="full_output"
- **Script Execution**: Apply reasonable timeouts (default: 300s) to prevent hanging loops
- **Memory Management**: Complete iteration history preserved; consider truncation for very large outputs

## Related ADRs
- **Depends on**: [ADR 7: Context Management Standardization], [ADR 8: Error Taxonomy]
- **Extends**: [ADR 9: Partial Results Policy]
- **Related to**: [ADR 11: Subtask Spawning Mechanism]

## Consequences

### Positive
- Unified representation for feedback loops
- Clear, direct data flow without environment variables
- Explicit iteration control
- Flexible termination conditions
- Consistent context management integration
- Compatibility with script execution
- Complete iteration history preserved

### Negative
- Migration effort for existing implementations
- More complex TaskSystem implementation
- Additional memory usage for iteration history
- Learning curve for configuration options
</file>
<file path="./system/architecture/decisions/15-notes.md" project="">
# ADR 15: Notes Field Standardization

## Status
Proposed

## Context
The documentation inconsistently uses "notes" and "summary information" when referring to task outputs, creating ambiguity about:
- What's generated by tasks
- What's preserved during accumulation
- Component responsibilities

Example inconsistencies:
```typescript
// In TaskResult
notes: {
    dataUsage: string;
    successScore?: number;
    partialOutput?: string;
    [key: string]: any;
};

// In SequentialHistory.TaskOutput
interface TaskOutput {
    stepId: string;
    output: string;  // The main content
    notes: any;      // Described as "Additional metadata or partial information"
    timestamp: Date;
}
```

Documentation describing `accumulation_format` options inconsistently refers to "summary information" vs "notes":
```
- `notes_only`: Only summary information is preserved (default)
- `full_output`: Complete output (with reasonable size limits)
```

## Decision

1. **Terminology Standardization**
   - Use "notes" consistently throughout documentation
   - Eliminate ambiguous term "summary information" entirely

2. **Component Responsibilities**
   - LLM/Task: Generates notes content during task execution
   - Evaluator: Stores and manages notes without modification
   - No separate summary generation mechanism needed

3. **Accumulation Format Behavior**
   ```typescript
   // Clarified behavior:
   enum AccumulationFormat {
     NOTES_ONLY,  // Preserves only the complete notes field
     FULL_OUTPUT  // Preserves both content and notes fields
   }
   ```

4. **Sequential History Management**
   ```typescript
   // When accumulation_format is "notes_only":
   sequentialHistory.push({
     stepId: generateStepId(),
     output: null,  // Not preserved
     notes: taskResult.notes,  // Preserved intact
     timestamp: new Date()
   });
   
   // When accumulation_format is "full_output":
   sequentialHistory.push({
     stepId: generateStepId(),
     output: taskResult.content,  // Preserved
     notes: taskResult.notes,     // Preserved
     timestamp: new Date()
   });
   ```

## Consequences

### Positive
- Simplified Evaluator implementation (no summary generation needed)
- Clearer component responsibilities
- Consistent terminology across documentation
- Reduced ambiguity in implementation

### Negative
- Requires documentation updates across multiple components
- Potential confusion for developers familiar with previous terminology

## Implementation

```mermaid
flowchart TD
    A[Task Execution] --> B[TaskResult]
    B --> C{accumulation_format?}
    C -->|notes_only| D[Store notes field only]
    C -->|full_output| E[Store both content and notes]
    D --> F[Sequential History]
    E --> F
    F --> G[Next Task Input]
```

Required documentation updates:
1. `components/task-system/spec/types.md`: Update TaskResult and TaskOutput interfaces
2. `components/task-system/spec/behaviors.md`: Clarify accumulation_format behavior
3. `components/task-system/impl/*.md`: Update implementation examples
4. `system/architecture/patterns/*.md`: Update pattern descriptions 
5. `system/contracts/*.md`: Update contract specifications

Type definition updates:
```typescript
// Updated accumulation_format documentation
/**
 * Controls what information is preserved during sequential task execution
 * - notes_only: Only the notes field is preserved (default)
 * - full_output: Both content and notes fields are preserved
 */
type AccumulationFormat = 'notes_only' | 'full_output';
```
</file>
<file path="./system/architecture/decisions/removed.md" project="">
ADRs 002 and 005 were removed to save space, can be provided if needed
</file>
<file path="./system/architecture/decisions/9-partial-results.md" project="">
# Architecture Decision Record: Partial Results Policy

## Status
Proposed

## Context
When multi-step operations fail partway through execution, the system needs a consistent approach to handling partial results. Currently, there is no standardized way to preserve and represent partial work, which limits error reporting and debugging capabilities.

## Decision
We will implement a standardized partial results policy with these principles:

1. **Uniform structure** across task types
2. **Task-level granularity** for partial results tracking
3. **Ephemeral storage** (in-memory only)
4. **Minimal complexity** for the MVP implementation
5. **Limited recovery** through simple retries only (no reparsing for MVP)

## Specification

### Representation by Task Type

#### Atomic Tasks
```typescript
interface TaskResult {
    content: string;
    status: ReturnStatus;
    notes: {
        partialOutput?: string;  // Partial results stored here
        [key: string]: any;
    };
}
```

#### Sequential Tasks
```typescript
{
    type: 'TASK_FAILURE',
    reason: 'subtask_failure',
    message: 'Sequential task failed at step X',
    details: {
        failedStep: number;
        totalSteps: number;
        partialResults: {
            stepIndex: number;
            output: string;
            notes?: any;
        }[];
    }
}
```

#### Reduce Tasks
```typescript
{
    type: 'TASK_FAILURE',
    reason: 'subtask_failure',
    message: 'Reduce task failed processing input X',
    details: {
        failedInputIndex: number;
        totalInputs: number;
        processedInputs: number[];
        currentAccumulator: any;
        partialResults: {
            inputIndex: number;
            result: any;
        }[];
    }
}
```

### Result Format

The format depends on the task's `accumulation_format` setting:

- **notes_only**: Only summary information from each step
- **full_output**: Complete output (with reasonable size limits)

### Recovery for MVP

For the MVP, recovery is limited to simple retries for transient failures. More complex recovery mechanisms are not included in the MVP.

## Size Management

To prevent memory issues:
- Individual step outputs should be kept reasonably sized
- When accumulated data becomes too large, older results may be summarized or truncated
- The system should indicate when truncation has occurred

## Consequences

### Positive
- Consistent approach across task types
- Improved debugging and error analysis
- Minimal memory overhead
- Simpler error handling in MVP

### Negative
- Limited recovery options in MVP
- Potential information loss due to size management
- No persistence for long-running operations

## Examples

### Sequential Task Failure
```typescript
{
  type: 'TASK_FAILURE',
  reason: 'subtask_failure',
  message: 'Sequential task "Process Dataset" failed at step 3',
  details: {
    failedStep: 2,
    totalSteps: 5,
    partialResults: [
      { 
        stepIndex: 0, 
        output: "Data loaded successfully", 
        notes: { recordCount: 1000 }
      },
      { 
        stepIndex: 1, 
        output: "Data transformed to required format"
      }
    ]
  }
}
```

### Reduce Task Failure
```typescript
{
  type: 'TASK_FAILURE',
  reason: 'subtask_failure',
  message: 'Reduce task "Aggregate metrics" failed processing input 2',
  details: {
    failedInputIndex: 2,
    totalInputs: 5,
    processedInputs: [0, 1],
    currentAccumulator: { totalCount: 1500, averageValue: 42.3 },
    partialResults: [
      { inputIndex: 0, result: "Processed metrics for server 1" },
      { inputIndex: 1, result: "Processed metrics for server 2" }
    ]
  }
}
```

## Related Documents
- Error Taxonomy for Context Issues ADR
- Context Management Standardization ADR
- [Pattern:Error:1.0]
</file>
<file path="./system/architecture/decisions/specs/memory.md" project="">
# Memory System 3.0 Consolidation Specification
> Ingest the information from this file, implement the Low-Level Tasks, and generate the code that will satisfy the High and Mid-Level Objectives.

## High-Level Objective

- Standardize Memory System references to version 3.0 throughout the codebase and documentation

## Mid-Level Objectives

- Update all documentation to consistently reference Memory System 3.0
- Ensure clear definition of interface boundaries between Memory System and Handler tools
- Standardize type definitions across all components
- Remove or update references to deprecated methods (particularly `updateContext`)

## Implementation Notes
- Focus on documentation updates rather than code changes
- Metadata format should remain unspecified to provide implementation flexibility
- Take a light approach to validation, focusing on basic type checking
- Maintain the read-only context model established in ADR-003
- Ensure clear separation between metadata management (Memory System) and file operations (Handler tools)

## Context

### Beginning context
- Documentation contains inconsistent references to Memory System 2.0 and 3.0
- Some documents reference deprecated methods like `updateContext` and `getContext`
- Unclear boundaries between Memory System and Handler responsibilities in some documents
- Inconsistent definitions of core types across components

### Ending context  
- All documentation consistently references Memory System 3.0
- Clear boundary definition between Memory System (metadata) and Handler (file operations)
- Standardized type definitions across all components
- No references to deprecated methods
- Updated architecture documentation reflecting the read-only context model

## Low-Level Tasks
> Ordered from start to finish

1. Create consolidated Memory System 3.0 reference document
```aider
Create a new reference document components/memory/REFERENCE.md that:
- Defines the canonical Memory System 3.0 interface
- Lists all standard type definitions (GlobalIndex, FileMetadata, FileMatch, etc.)
- Clearly states the read-only context model
- Defines the responsibility boundary between Memory System and Handler tools
- Documents the removal of updateContext capability
- Serves as the single source of truth for interface definitions
```

2. Update Task System documentation with consistent references
```aider
Update the following Task System documents to reference Memory System 3.0 consistently:
- components/task-system/spec/interfaces.md
- components/task-system/spec/behaviors.md
- components/task-system/impl/design.md

Remove any references to:
- updateContext method
- getContext method
- Memory System 2.0
- Any context persistence capabilities

Update type imports and interface references to match the canonical definitions.
```

3. Update architecture documentation with consistent references
```aider
Update the following architecture documents:
- system/architecture/overview.md
- system/architecture/decisions/001-memory-system-qa.md
- system/architecture/decisions/needs_update/001-memory-system.md

Ensure all references use Memory System 3.0 terminology.
Clarify that Memory System maintains a read-only approach to context.
Update any diagrams or flowcharts to reflect current architecture.
```

4. Update system contracts with consistent references
```aider
Update the following system contract documents:
- system/contracts/interfaces.md
- system/contracts/resources.md

Ensure [Contract:Integration:TaskMemory:3.0] is consistently referenced.
Verify interface definitions match the canonical reference.
Update any examples to reflect current architecture.
```

5. Create migration guidance document
```aider
Create a new document system/architecture/migrations/memory-system-2-to-3.md that:
- Summarizes changes from Memory System 2.0 to 3.0
- Provides guidance for updating code that used updateContext
- Explains the new read-only context model
- References ADR-003 for the architectural decision rationale
- Includes examples of proper Memory System 3.0 usage
```
</file>
<file path="./system/architecture/decisions/needs_update/001-memory-system.md" project="">
# Architecture Decision Record: Memory System Design

## Context

The memory system needs to manage short-term task context and maintain a global index of file metadata, while delegating file access to direct tool usage. This ADR captures key decisions about responsibilities and interfaces.

## Related Documents
- See [Component:Memory:3.0] in components/memory/api/interfaces.md for interface specification
- See [Component:TaskSystem:1.0] in components/task-system/README.md for task execution details
- See [Interface:Handler:Tools:1.0] for file access tools

## Decisions

### Memory System Scope

1. **Component Responsibilities**
   - Maintains short-term task data context
   - Manages global file metadata index
   - Delegates file access to Handler tools
   - Does not handle file content storage or retrieval
   - Follows read-only context model (no updateContext capability)

2. **Memory Organization**
   - Working Memory includes only data context from associative matching
   - Global Index includes file paths and associated metadata strings
   - Clear separation between metadata indexing and file access

### Context Management

1. **Context Sources**
   - Short-term context comes from:
     * Associative matching results
     * Direct updates during task execution
   - Global index serves as bootstrap for associative matching
   - File content accessed via Handler tools when needed

2. **Context Control**
   - Associative matching uses global index for initial filtering
   - Actual file content accessed through Handler tools
   - Metadata strings provide search context without content storage

### Interface Design

1. **Global Index**
   - Simple map of file paths to metadata strings
   - Bulk updates only
   - No hierarchical organization
   - Serves as search bootstrap

2. **Associative Match Results**
   - Combines unstructured context with file matches
   - File matches include optional per-file metadata 
   - File paths used with Handler tools for content access

## Consequences

1. **Positive**
   - Simplified memory system responsibilities
   - Clear separation between metadata and content
   - More flexible file access via tools
   - Reduced system complexity

2. **Negative**
   - Requires tool support in Handler
   - May need optimization for large file sets
   - Potential context window pressure from file content

## Implementation Notes

1. Update Memory System interface to reflect simplified scope
2. Implement Handler file access tools
3. Ensure efficient global index updates
4. Review associative matching for new approach
</file>
<file path="./system/architecture/decisions/needs_update/004-sequential-context-management.md" project="">
# Architecture Decision Record: Sequential Context Management

## Status
Implemented (see ADR 14)

## Context
We need to separate context inheritance from data accumulation and provide a robust mechanism for step-by-step history tracking.

Currently, tasks either share context automatically or produce data that might or might not propagate. This leads to confusion when partial outputs are produced but we do not have a consistent mechanism to feed them into subsequent steps. Also, memory consumption can balloon if every subtask blindly appends data. 

## Decision
1. **Introduce `<context_management>`** for controlling context inheritance and accumulation in sequential tasks.  
2. **Add explicit outputtracking** in the Evaluator for each step of a sequential task.  
3. **Provide structured ways** to use task history in associative matching or subsequent steps.  
4. **Include step outputs** in error results for failed sequences.  
5. **Enforce resource limits** on total stored step data.

## Implementation
This ADR has been fully implemented as part of ADR 14 (Operator Context Configuration), which introduced a hybrid configuration approach with operator-specific defaults and explicit overrides:

| Operator Type | inherit_context | accumulate_data | accumulation_format | fresh_context |
|---------------|-----------------|-----------------|---------------------|---------------|
| sequential    | full            | true            | notes_only          | enabled       |

These defaults apply when no explicit context_management block is provided. When present, explicit settings override the defaults, providing both consistency and flexibility.

The implementation includes:
1. Default context settings for all operator types
2. XML schema support for the context_management block
3. Template processing with merged settings (defaults + overrides)
4. Evaluator integration for applying the final configuration
5. Partial results preservation in error handling

## Consequences
- **Cleaner separation of concerns** between inheritance and accumulation  
- **More flexible context management** through distinct modes (`inherit_context`, `accumulate_data`, etc.)  
- **Better step-by-step tracking** for partial outputs  
- **Predictable error output**: if step N fails, steps 1..N-1 remain visible  
- **Clear resource usage capping** for historical data

## Related
- [ADR 14 - Operator Context Configuration] for the complete implementation
- [Pattern:SequentialTask:2.0] in system/architecture/overview.md  
- [misc/operators.md] for structural usage  
- [system/contracts/protocols.md] for updated XSD schema  
- [components/task-system/impl/examples.md] for usage examples
</file>
<file path="./system/architecture/decisions/needs_update/003-memory-context-update.md" project="">
# Architecture Decision Record: Remove Context Update Capability

## Status
Completed

## Context
The Memory System's updateContext capability was found to be inconsistent with its architectural goals:
- System primarily provides context FOR tasks
- No persistence guarantees documented
- Context scope limited to current task
- No clear use cases for bi-directional context flow

## Decision
Remove updateContext method and enforce read-only context model.

### Changes
1. Remove updateContext from [Interface:Memory:3.0]
2. Enforce read-only context access
3. Maintain clear task isolation
4. Simplify state management

### Version Impact
- Major version increment to 3.0
- Breaking change for interface consumers
- Required updates to dependent systems

## Consequences

### Positive
- Clearer architectural boundaries
- Simpler interface
- Better task isolation
- Reduced state complexity
- More predictable behavior

### Negative
- Breaking change for existing code
- Migration effort required
- Potential feature impact

## Implementation
1. Update interface definitions
2. Remove update capabilities
3. Update documentation
4. Add migration guide
5. Update cross-references
6. Release version 3.0
</file>
<file path="./system/architecture/decisions/16-output-structure-simplification.md" project="">
# ADR 16: Output Structure Simplification

## Status
Accepted

## Context
The current system has multiple fields for task output:
- `content`: Primary output when task completes
- `partialOutput`/`incompleteContent`: Output from incomplete atomic tasks
- `notes`: Task metadata
- `partialResults`: Results from sequential task steps

This creates unnecessary complexity and ambiguity.

## Decision
We will simplify the output structure:

1. **Atomic Tasks**: Use only `content` and `notes` fields
   - `content`: Contains all output (complete or partial)
   - `notes`: Contains only metadata, never content
   - Task status indicates whether content is complete
   
2. **Sequential/Reduce Tasks**: Maintain existing structure with proper separation
   - Each step/input result contains content and metadata
   - Clear separation between task-level and step-level concerns

## Consequences

### Positive
- Simpler interface with fewer fields
- Clear separation between content and metadata
- Status code provides necessary completion information
- Consistent approach across task types
- Easier error handling and recovery

### Negative
- Requires updates to existing code checking for partialOutput
- Slightly less explicit about content completeness

## Implementation
For atomic tasks, the completion status is determined by the `status` field:
- `COMPLETE`: Content is final and complete
- `FAILED`: Content may be partial or incomplete
- `CONTINUATION`: Content is intermediate

This approach maintains the essential information while simplifying the structure.
</file>
<file path="./system/architecture/decisions/001-memory-system-qa.md" project="">
# Memory System Architecture Q&A

## Related Documents
- Memory component specification in [Component:Memory:3.0]
- Handler interface in [Interface:Handler:ResourceMonitoring:1.0]
- Memory Task Example in components/task-system/impl/examples.md
- Context Frame Pattern in [Pattern:ContextFrame:1.0]
- Resource Management Pattern in [Pattern:ResourceManagement:1.0]

## Memory and Context Concepts

Q: What exactly constitutes working memory?
A: The Memory System only manages two types of data:
1. Short-term task data context generated by associative matching
2. Global file metadata index for bootstrapping associative matching

The Handler manages all other task-related data including chat history and prompt handling.

Q: How is "context" different from "working memory"?
A: "Context" specifically refers to data context, while working memory is broader and includes chat history, system prompts, and templates. See [Pattern:ContextFrame:1.0] for details.

Q: What exactly does associative matching return?
A: Associative matching has two specific return values:
1. An unstructured data context section
2. A list of tuples containing absolute file paths and optional index strings for those files

## Component Responsibilities 

Q: Should the memory system track context window size?
A: No, this is delegated to the Handler (see [Interface:Handler:ResourceMonitoring:1.0]). The memory system stores content but doesn't track usage limits.

Q: Is the memory system responsible for managing system prompts and templates?
A: No, the Memory System does not handle prompts or templates at all. These are managed by the Task System and Handler.

Q: Who handles chat history?
A: The Handler manages all chat history and interaction state. The Memory System does not store any chat history.

Q: Who handles file content access?
A: File content access is handled by the LLM using Handler tools, not by the Memory System. The Memory System only provides the file paths and metadata needed to identify relevant files.

## Global Index

Q: What is the format of file metadata in the global index?
A: The metadata is simply stored as strings, with file paths as keys in a map structure. There is no predefined structure for the metadata content.

Q: What operations are supported on the global index?
A: The global index only supports two operations:
1. Getting the complete index
2. Bulk updates to the index
Individual updates, filtering, and querying are intentionally not supported to maintain simplicity.

## Context Management

Q: How can tasks get their data context?
A: Two ways (see operator specifications in components/task-system/spec/operators.md):
1. Inherit from parent/predecessor task
2. Generate fresh via associative matching using the global file metadata index as bootstrap. The associative matching task returns both context and a list of potentially relevant files, which can then be accessed using Handler tools.

Q: How does associative matching work when there's too much content?
A: When the full contents of all long-term storage files don't fit in the context window (times some factor), the associative matching task uses the existing global index to bootstrap the matching process. The input to the associative matching task includes all file paths in long-term storage.

Q: Should context inheritance vs. generation be a template parameter?
A: No, it should be part of the operator XML syntax in task library entries (see [Contract:Tasks:TemplateSchema:1.0]). It's not a substitutable parameter like task inputs.

Q: Can prior task output influence context generation?
A: Yes, the notes section of prior task output can be passed to associative matching for child/successor tasks. See Memory Task Example in components/task-system/impl/examples.md.

## Interface Design

Q: How should context source be controlled in the XML?
A: Through an optional inherit_context attribute on task elements, with defaults that can vary by operator type. See operator specifications in components/task-system/spec/operators.md.

## Implementation Questions

Q: Do operator defaults need to be standardized?
A: Not immediately. Different operators can have different context inheritance defaults based on their use cases. See operator specifications.

Q: How should context be handled in map/reduce operations?
A: The inherit_context attribute can be part of inner_task and reduction_task elements, following the same pattern as other operators. See operator specifications.

## Future Considerations

Q: When would file pre-extraction be considered?
A: File pre-extraction and concatenation is explicitly deferred for later consideration. The current design relies solely on tool-based file access via the Handler. Any performance optimizations through pre-extraction would only be considered after evaluating the effectiveness of the tool-based approach.
</file>
<file path="./system/architecture/decisions/12-xml-ast.md" project="">
# Architecture Decision Record: Function-Based Template Model

## Status
Proposed

## Context
The current system architecture uses two different variable resolution mechanisms for tasks:

1. **Template Substitution** (`{{variable_name}}`) - Placeholders within text content that are replaced with values from the current lexical environment.

2. **Input Binding** (`<input name="data" from="previous_result"/>`) - Explicit binding of inputs to named environment variables.

This dual approach has created several issues:

- **Unclear Variable Scope**: Templates have implicit access to the caller's entire environment.
- **Inconsistent Binding Patterns**: Multiple ways to access variables leads to confusion.
- **Environment Leakage**: No clear boundaries between caller and callee contexts.
- **Implicit Dependencies**: Templates can access variables that aren't explicitly passed.
- **Complex Execution Model**: The Evaluator must handle different resolution mechanisms.
- **Difficult Reasoning**: It's hard to predict which variables a template might access.

This does not align with established programming language patterns where functions have explicit parameters and clear scope boundaries.

## Decision
We will adopt a clean function-based model for templates with the following simplifications:

1. **Templates as Function Definitions**: Each template will explicitly declare its parameters using a simple attribute.
   ```xml
   <template name="analyze_data" params="dataset,config">
     <task>
       <description>Analyze {{dataset}} using {{config}}</description>
     </task>
   </template>
   ```

2. **Function Calls with Positional Arguments**: Template invocation will use positional arguments.
   ```xml
   <call template="analyze_data">
     <arg>weather_data</arg>
     <arg>standard_config</arg>
   </call>
   ```

3. **Strict Scope Boundaries**: Templates can only access their parameters, not the caller's environment.

4. **Single Variable Resolution Mechanism**: Within templates, `{{...}}` only refers to declared parameters.

5. **Simplified Argument Handling**: Arguments are represented with a simplified structure.
   ```typescript
   interface ArgumentNode {
     type: "argument";
     value: string | ASTNode;  // String for variables/literals, ASTNode for nested
   }
   ```

6. **Implicit Template Registration**: Templates are automatically registered in the TaskLibrary during parsing.

## Consequences

### Positive
- **Clearer Mental Model**: Aligns with established function call semantics.
- **Predictable Behavior**: Templates can only access explicitly passed arguments.
- **Simplified Reasoning**: Easy to understand what variables a template needs.
- **Better Encapsulation**: Template internals are hidden from callers.
- **Reduced Verbosity**: Simplified parameter declaration and argument handling.
- **Streamlined Implementation**: Implicit template registration simplifies management.
- **Foundation for Extensions**: Creates a path to named arguments, default values, etc.

### Negative
- **Breaking Change**: Existing templates will need updates.
- **Migration Effort**: Need to identify parameters and update call sites.
- **Implementation Complexity**: Requires updates to parsing and execution.
- **Argument Ambiguity**: String arguments could be variable references or literals, requiring disambiguation.

### Migration Path
1. **Update XML Schema**: Add new elements for template definitions and calls.
2. **Identify Template Parameters**: For each template, identify required parameters.
3. **Add Parameter Declarations**: Update templates with explicit parameter lists.
4. **Convert Call Sites**: Replace current invocation patterns with `<call>` elements.
5. **Update Variable References**: Ensure templates only reference declared parameters.
6. **Deprecate Old Patterns**: Mark `from` attribute as deprecated during transition.

## Implementation Recommendations

### XML Schema Updates
- Add `<template>` with `name` and `params` attributes
- Add `<call>` and `<arg>` elements
- Deprecate the `from` attribute on `<input>` elements

### AST Structure Updates
```typescript
interface TemplateNode {
  type: "template";
  name: string;
  parameters: string[];  // Parameter names in order
  body: TaskNode;        // The actual task implementation
}

interface FunctionCallNode {
  type: "call";
  templateName: string;
  arguments: ArgumentNode[];  // Evaluated in caller's environment
}

interface ArgumentNode {
  type: "argument";
  value: string | ASTNode;  // String for variables/literals, ASTNode for nested
}
```

### TaskLibrary Enhancements
- Automatic registration of templates during parsing
- Lookup functionality for template resolution during function calls
- Validation for parameter count matching argument count
- Proper error handling for missing templates or parameters

/**
 * Argument Resolution Algorithm
 * 
 * 1. For each argument in function call:
 *    a. If argument is string:
 *       i. Try to resolve as variable in caller's environment
 *       ii. If variable exists, use its value
 *       iii. If not, use string as literal value
 *    b. If argument is ASTNode:
 *       i. Recursively evaluate node in caller's environment
 * 
 * 2. Create new environment with parameter-to-argument bindings:
 *    a. Bind each parameter name to its corresponding evaluated argument
 * 
 * 3. Execute template body in new environment:
 *    a. Template can only access explicitly passed parameters
 *    b. No implicit access to caller's environment
 */

### Evaluator Updates
- Implement clean environment creation for each call
- Update variable resolution to only check parameters
- Add proper argument evaluation in caller's context

### Component Interactions
- **Compiler**: Parse templates and function calls into AST nodes, automatically registering templates
- **TaskLibrary**: Store templates and provide lookup functionality
- **Evaluator**: Handle function invocation, argument evaluation, and scope management

## AST Containment Relationships

The new node types integrate with the existing AST structure as follows:

### Hierarchy Integration
All new node types (`TemplateNode`, `FunctionCallNode`, `ArgumentNode`) implement the base `ASTNode` interface, ensuring consistent traversal and evaluation.

### Containment Rules
1. **TemplateNode**
   - Contains a `TaskNode` as its body (any task type)
   - Is stored in the TaskLibrary, not directly in the AST
   - Cannot be contained within other nodes (only top-level)

2. **FunctionCallNode**
   - Can appear anywhere a `TaskNode` might appear
   - Can be contained within:
     - Sequential task steps
     - Reduce operations
     - Nested inputs
     - Other composite structures
   - Contains an array of `ArgumentNode` elements

3. **ArgumentNode**
   - Can only appear as children of `FunctionCallNode`
   - May contain a string (variable reference or literal) or a nested `ASTNode`
   - When containing a nested node, that node is evaluated before being passed as an argument

### Legal Placement Rules
- `<template>` definitions must be at the top level of a template file
- `<call>` elements can appear anywhere a `<task>` element is valid
- `<arg>` elements can only appear as direct children of `<call>` elements

### Traversal Implications
When traversing the AST:
- `TemplateNode` instances are not traversed directly (they exist in the TaskLibrary)
- When a `FunctionCallNode` is encountered, the evaluator:
  1. Looks up the template by name in the TaskLibrary
  2. Evaluates all argument nodes in the current environment
  3. Creates a new environment with parameters bound to argument values
  4. Traverses and evaluates the template's body in this new environment

A template's body can be any task type (atomic, sequential, reduce, etc.), allowing function-based templates to define both simple and complex operations. This enables function composition across all task types in the system, not just atomic tasks.

## Affected Documentation

The following documents will need updates after this ADR is accepted:

1. **Core Contract Documents**
   - `system/contracts/protocols.md` (XML schema)

2. **Type Definition Documents**
   - `components/task-system/spec/types.md` (AST node types)

3. **Implementation Documents**
   - `components/compiler/README.md` (parsing approach)
   - `components/evaluator/README.md` (execution model)
   - `components/task-system/impl/xml-processing.md` (validation rules)

4. **Pattern Documents**
   - `system/architecture/patterns/context-frames.md` (scope rules)

5. **Overview Documents**
   - `system/README.md` and `system/architecture/overview.md` (high-level descriptions)

## Related Decisions
- This builds on [ADR 7: Context Management Standardization]
- This complements [ADR 11: Subtask Spawning Mechanism]
</file>
<file path="./system/architecture/decisions/8-errors.md" project="">
# Architecture Decision Record: Error Taxonomy for Context Issues

## Status
Proposed

## Context
The current system error taxonomy consists of two primary error types: `RESOURCE_EXHAUSTION` and `TASK_FAILURE`. As we implement more sophisticated context management, we need clear error categorization for context-related failures to enable appropriate error handling and recovery strategies.

The current error system does not provide enough specificity for context-related failures, making it difficult to implement targeted recovery strategies or provide meaningful feedback to users and developers.

## Decision
We will enhance the error taxonomy by refining the existing `TASK_FAILURE` type with a standardized `reason` field and structured `details` object rather than introducing entirely new error types. This approach provides the necessary specificity while maintaining backward compatibility and simplicity.

## Specification

### Error Type Structure

```typescript
type TaskFailureReason = 
  | 'context_retrieval_failure'    // Failure to retrieve context data
  | 'context_matching_failure'     // Failure in associative matching algorithm 
  | 'context_parsing_failure'      // Failure to parse or process retrieved context
  | 'xml_validation_failure'       // Output doesn't conform to expected XML schema
  | 'output_format_failure'        // Output doesn't meet format requirements (non-XML)
  | 'execution_timeout'            // Task execution exceeded time limits
  | 'execution_halted'             // Task execution was deliberately terminated
  | 'subtask_failure'              // A subtask failed, causing parent task failure
  | 'input_validation_failure'     // Input data didn't meet requirements
  | 'unexpected_error';            // Catch-all for truly unexpected errors

type TaskError = 
    | { 
        type: 'RESOURCE_EXHAUSTION';
        resource: 'turns' | 'context' | 'output';
        message: string;
        metrics?: { used: number; limit: number; };
    }
    | { 
        type: 'TASK_FAILURE';
        reason: TaskFailureReason;
        message: string;
        details?: {
            partial_context?: any;
            context_metrics?: any;
            violations?: string[];
            partialResults?: any[];
            failedStep?: number;
            // Other reason-specific details as needed
        };
    };
```

### Error Handling Flow

Error handlers should first check the error type, then examine the reason field for TASK_FAILURE errors:

```typescript
function handleTaskError(error: TaskError) {
  if (error.type === 'RESOURCE_EXHAUSTION') {
    // Handle resource exhaustion based on resource type
    handleResourceExhaustion(error);
  } else if (error.type === 'TASK_FAILURE') {
    // Handle task failure based on reason
    if (error.reason.startsWith('context_')) {
      // Handle context-related failures
      handleContextFailure(error);
    } else if (error.reason === 'subtask_failure') {
      // Handle subtask failures
      handleSubtaskFailure(error);
    } else {
      // Handle other failures
      handleGeneralFailure(error);
    }
  }
}
```

### Partial Results Handling

For failures that occur during multi-step operations, the system should preserve partial results in the `details.partialResults` field. This applies particularly to `subtask_failure` errors in sequential or reduce operations.

## Context Failure Definitions and Examples

### Context Retrieval Failure
Occurs when the Memory System cannot provide required context data due to storage or access issues.

**Example**:
```typescript
// Memory System's file index is corrupted or unavailable
return {
  type: 'TASK_FAILURE',
  reason: 'context_retrieval_failure',
  message: 'Failed to access memory system index',
  details: { error }
};
```

### Context Matching Failure
Occurs when associative matching runs but finds no relevant matches for a query that requires context.

**Example**:
```typescript
// No matches found for a task that needs specific context
return {
  type: 'TASK_FAILURE',
  reason: 'context_matching_failure',
  message: 'No relevant context found for Project X',
  details: { query: "Project X", availableProjects: ["Project Y", "Project Z"] }
};
```

### Context Parsing Failure
Occurs when context is retrieved but cannot be properly processed due to format issues.

**Example**:
```typescript
// Context was retrieved but not in expected format
return {
  type: 'TASK_FAILURE',
  reason: 'context_parsing_failure',
  message: 'Retrieved context is not valid JSON',
  details: { 
    context: contextResult.context.substring(0, 100) + '...',
    error: error.message
  }
};
```

## Consequences

### Positive
- Provides specific error categorization without introducing new error types
- Enables targeted recovery strategies for different failure modes
- Maintains backward compatibility with existing error handling
- Standardizes details objects for consistent error information
- Supports partial results preservation for failed multi-step operations

### Negative
- Requires updates to error handling code to check reason field
- May require additional documentation for error reasons
- Could lead to inconsistent usage if not properly enforced

## Implementation Guidelines

1. **Error Construction**:
   ```typescript
   function createTaskFailure(
     reason: TaskFailureReason, 
     message: string, 
     details?: any
   ): TaskError {
     return {
       type: 'TASK_FAILURE',
       reason,
       message,
       details
     };
   }
   ```

2. **Documentation Requirements**:
   - Each reason should be documented with examples
   - Recovery strategies should be suggested for each reason
   - Error handling hierarchy should be clearly documented

3. **Testing Requirements**:
   - Test each error reason with concrete examples
   - Verify error handling behavior for each reason
   - Ensure partial results are properly preserved

4. **Recovery Strategies**:
   - For context errors, consider retrying with different context parameters
   - For subtask failures, consider alternative task decomposition
   - For validation failures, attempt with simpler formats or requirements

## Relationships

This ADR builds upon:
- Context Management Standardization ADR
- Error Handling Pattern [Pattern:Error:1.0]

It will inform:
- Partial Results Policy (upcoming ADR)
- Evaluator-to-Director Feedback Flow (upcoming ADR)

## Examples

### Sequential Task Failure with Partial Results

```typescript
// A sequential task where the third step fails
const error = {
  type: 'TASK_FAILURE',
  reason: 'subtask_failure',
  message: 'Sequential task "Process Dataset" failed at step 3: Data validation',
  details: {
    failedStep: 2,
    stepCount: 5,
    partialResults: [
      { step: 0, output: "Data loaded successfully" },
      { step: 1, output: "Data transformed to required format" }
    ],
    stepError: {
      type: 'TASK_FAILURE',
      reason: 'input_validation_failure',
      message: 'Invalid data format in dataset'
    }
  }
};
```

### Context Generation Failure

```typescript
// Failure during associative matching
const error = {
  type: 'TASK_FAILURE',
  reason: 'context_matching_failure',
  message: 'Unable to find relevant context for code review task',
  details: {
    query: "Review recent changes to payment processing module",
    attempted_sources: ["codebase", "documentation"],
    partial_context: "Found general documentation about payment processing, but no specifics about recent changes."
  }
};
```

## Notes

The decision to enhance the existing error system rather than create new error types was made to:
1. Minimize breaking changes
2. Avoid unnecessary complexity
3. Ensure backward compatibility
4. Provide necessary specificity without overengineering

This approach follows the YAGNI principle while providing the flexibility needed for effective error handling in the evolving system architecture.
</file>
<file path="./system/architecture/decisions/14-operator-ctx-config.md" project="">
# Architecture Decision Record: Context Management Settings Configuration

## Status
Proposed

## Context
The system's architecture defines a three-dimensional context management model that controls:
1. Context inheritance from parent tasks
2. Accumulation of outputs from previous steps
3. Generation of fresh context through associative matching

We need to determine how these settings should be configured in the system - whether through operator attributes at the code/AST level, explicitly in template XML, or through a hybrid approach.

This decision impacts:
- Template authoring experience
- System flexibility and extensibility
- Runtime behavior predictability
- Backward compatibility
- Implementation complexity

## Decision
We will implement a hybrid approach where:

1. Each operator type (atomic, sequential, reduce, etc.) defines sensible defaults for context management settings
2. Template XML can explicitly override these defaults using a `<context_management>` block
3. The system will merge operator defaults with explicit XML settings at template loading time
4. Validation of combined settings will occur during template registration rather than execution

### Configuration Structure

Operator defaults will be defined in code:

```typescript
// Example for Sequential operator
const SEQUENTIAL_DEFAULTS = {
  inheritContext: "full",
  accumulateData: true,
  accumulationFormat: "notes_only",
  freshContext: "enabled"
};

// Example for Atomic operator
const ATOMIC_DEFAULTS = {
  inheritContext: "full",
  accumulateData: false,  // N/A but defined for consistency
  accumulationFormat: null, // N/A
  freshContext: "enabled"
};
```

XML templates can override these defaults:

```xml
<task type="sequential">
  <description>Process data in steps</description>
  <context_management>
    <inherit_context>none</inherit_context>
    <accumulate_data>true</accumulate_data>
    <accumulation_format>full_output</accumulation_format>
    <fresh_context>disabled</fresh_context>
  </context_management>
  <steps>
    <!-- Tasks defined here -->
  </steps>
</task>
```

### Standard Operator Defaults

| Operator Type (Subtype) | inherit_context | accumulate_data | accumulation_format | fresh_context |
|-------------------------|-----------------|-----------------|---------------------|---------------|
| atomic (standard)       | full            | N/A             | N/A                 | disabled      |
| atomic (subtask)        | none            | N/A             | N/A                 | enabled       |
| sequential              | full            | true            | notes_only          | disabled      |
| reduce                  | none            | N/A             | N/A                 | enabled       |
| reduce.inner_task       | full            | N/A             | N/A                 | disabled      |
| reduce.reduction_task   | full            | N/A             | N/A                 | disabled      |
| script                  | full            | N/A             | N/A                 | disabled      |

### Validation Rules

1. **Mutual Exclusivity**:
   - `fresh_context="enabled"` cannot be combined with `inherit_context="full"` or `inherit_context="subset"`
   - If `inherit_context` is set to "full" or "subset", then `fresh_context` must be "disabled"
   - If `fresh_context` is "enabled", then `inherit_context` must be "none"

2. **Subtype Context Rules**:
   - The `subtype` attribute affects default context settings but not validation rules
   - Explicit context settings in XML always override subtype-based defaults
   - Mutual exclusivity constraint is enforced regardless of subtype

3. **Default Selection Logic**:
   - Context management defaults are determined by operator type + subtype
   - For atomic tasks, defaults differ between "standard" and "subtask" subtypes
   - For CONTINUATION-based subtasks, the "subtask" subtype is automatically applied

4. **Invalid Combinations**:
   - `inherit_context="none"` + `accumulate_data="false"` + `fresh_context="disabled"` results in no context at all, which should trigger a warning

### Implementation Process

1. During template loading:
   - Parse XML template
   - Determine operator type
   - Apply operator defaults
   - Override with any explicit settings from XML
   - Validate the combined settings
   - Store the final settings with the task template

2. During execution:
   - Use the stored settings to control context flow
   - No additional runtime resolution required
   - Consistent behavior for each task type

## Consequences

### Positive
1. **Flexible Configuration:** Tasks can customize their context behavior without changing code
2. **Sensible Defaults:** Common patterns work without verbose configuration
3. **Explicit when Needed:** Context behavior can be made explicit for complex tasks
4. **Template Evolution:** Context management can evolve without code changes
5. **Validation at Source:** Configuration errors are caught during template loading
6. **Documentation Value:** XML configuration serves as self-documentation
7. **LLM Compatibility:** LLM-generated templates can include explicit context settings

### Negative
1. **Implementation Complexity:** More complex merging and validation logic required
2. **Documentation Burden:** Need to clearly document operator defaults and override rules
3. **Validation Complexity:** Must validate combined settings rather than just explicit ones
4. **Learning Curve:** Users need to understand both defaults and override mechanisms
5. **Potential Inconsistency:** Different tasks of the same type could behave differently

## Implementation Requirements

1. **Operator Registry:** Define default settings for each operator type
2. **XML Schema Updates:** Ensure schema validates all context management combinations
3. **Merge Logic:** Implement logic to combine defaults with explicit settings
4. **Validation Rules:** Define validation rules for combined settings
5. **Error Messages:** Create clear error messages for invalid combinations
6. **Documentation:** Document operator defaults and override rules

## Migration Strategy

1. **Backward Compatibility:** Existing templates without explicit context management blocks will continue to work with operator defaults
2. **Graceful Degradation:** Invalid combinations will be detected at load time with clear error messages
3. **Documentation Updates:** Update documentation to explain the hybrid approach
4. **Example Templates:** Provide examples of common override patterns

## Related Decisions

- [ADR 7: Context Management Standardization]
- [ADR 11: Subtask Spawning Mechanism]

## Open Questions

1. Should we allow partial overrides, or require all settings to be specified when overriding?
2. Should we introduce additional validation for operator-specific constraints (e.g., certain operators might have restricted combinations)?
3. How should we handle future additions to the context management model?

## Conclusion

The hybrid approach provides a balance between consistency and flexibility. It offers sensible defaults while enabling explicit customization when needed. This approach aligns with the system's current architecture while providing a clear path for evolution.
</file>
<file path="./system/architecture/decisions/15-notes-field-standardization.md" project="">
# ADR 15: Notes Field Standardization

## Status
Accepted

## Context
The task system currently has an artificial distinction between `notes` and `partialOutput` fields in the TaskResult interface. This creates unnecessary complexity and inconsistency across the codebase:

1. Some documents state partial results are not preserved, while others describe preservation mechanisms
2. Sequential task results have separate `output` and `notes` fields for each step
3. The `accumulation_format` currently has values `notes_only` and `full_output` which are not intuitive
4. Error handling structures have inconsistent approaches to storing partial results

## Decision
We will simplify the task output structure by eliminating the artificial distinction between `notes` and `partialOutput` fields:

1. Use `notes` as the universal container for all task metadata, including partial results
2. Modify accumulation format values from `notes_only`/`full_output` to `minimal`/`full`
3. Update all error handling structures to store partial results directly in the notes field
4. Standardize terminology around `notes` across all documentation

### Simplified TaskResult Interface
```typescript
export interface TaskResult {
    content: string;
    status: ReturnStatus;
    criteria?: string;
    parsedContent?: any;
    notes: {
        dataUsage?: string;
        successScore?: number;
        // Partial results are directly included in notes
        [key: string]: any;
    };
}
```

### Simplified TaskOutput Interface
```typescript
export interface TaskOutput {
    stepId: string;  // or step index
    notes: any;      // Task notes field (always preserved, may include partial results)
    timestamp: Date;
}
```

### Simplified Accumulation Format
```typescript
// Simplified behavior:
enum AccumulationFormat {
    MINIMAL,  // Preserves only success/failure status and critical metadata
    FULL      // Preserves all notes content including partial results
}
```

## Consequences
### Positive
- Reduced complexity in the task output structure
- More intuitive naming for accumulation formats
- Consistent approach to partial results across the system
- Simplified error handling structures
- Better alignment with how LLMs naturally structure output

### Negative
- Requires updates to existing documentation and code
- May require migration for existing implementations

## Implementation
1. Update TaskResult interface to remove partialOutput field
2. Update TaskOutput interface to remove output field
3. Update accumulation format values in XML schema
4. Update operator default settings to use minimal/full values
5. Update error handling documentation to reflect the simplified approach
6. Ensure backward compatibility where possible

## Related Documents
- [Pattern:Error:1.0]
- [Contract:Tasks:TemplateSchema:1.0]
- [Component:TaskSystem:1.0]
</file>
<file path="./system/architecture/decisions/7-context-standardization.md" project="">
# Architecture Decision Record: Context Management Standardization

## Status
Proposed

## Context
The current system architecture has ambiguity regarding context management across different operator types. While sequential tasks have a well-defined context management structure, other operators like Reduce lack standardized context handling. This inconsistency creates several issues:

1. Unclear inheritance behavior for non-sequential operators
2. Ambiguity about whether to preserve partial results on failure
3. Undefined combinations of context inheritance, accumulation, and fresh context generation
4. Lack of clarity about how the "subset" inheritance mode works
5. Inconsistent behavior between operator types

The system needs standardized context management that works consistently across all operators while respecting their unique execution patterns.

## Decision
We will standardize context management across all operators using a three-dimensional model that explicitly controls:
1. Context inheritance from parent
2. Accumulation of outputs from previous steps
3. Generation of fresh context through associative matching

Additionally, we will support an orthogonal file-specific context mechanism:
4. Explicit file inclusion via `file_paths` that overrides standard context retrieval mechanisms

With one key constraint:
- Fresh context generation (`fresh_context="enabled"`) is mutually exclusive with context inheritance (`inherit_context="full"` or `inherit_context="subset"`)

This will be implemented through a unified XML schema structure that can be applied hierarchically within nested operators.

## Specification

### XML Schema for Context Management

```xml
<context_management>
    <!-- Controls parent context inheritance -->
    <inherit_context>full|none|subset</inherit_context>
    
    <!-- Controls accumulation of previous step outputs -->
    <accumulate_data>true|false</accumulate_data>
    <accumulation_format>notes_only|full_output</accumulation_format>
    
    <!-- Controls whether fresh context is generated via associative matching -->
    <fresh_context>enabled|disabled</fresh_context>
</context_management>
```

### Explicit File Path Specification

In addition to the standard context management model, tasks can explicitly specify files to include in their context:

```xml
<file_paths>
    <path>./src/main.py</path>
    <path>/absolute/path/file.txt</path>
</file_paths>
```

This feature operates orthogonally to the three-dimensional context model:
- Specified files are always included in the context, regardless of other settings
- When combined with `inherit_context="subset"`, the specified files become the subset
- When combined with `fresh_context="enabled"`, associative matching still runs but specified files are forcibly included

### Semantics

1. **inherit_context**:
   - `full`: Uses the entire parent context unchanged
   - `none`: Uses no parent context
   - `subset`: Filters parent context through associative matching based on task relevance

2. **accumulate_data**:
   - `true`: Preserves and accumulates outputs from previous steps
   - `false`: No accumulation between steps

3. **accumulation_format** (when accumulate_data is true):
   - `notes_only`: Only preserves summary information from previous steps
   - `full_output`: Preserves complete outputs from previous steps

4. **fresh_context**:
   - `enabled`: Performs associative matching to generate new context from global data
   - `disabled`: No new context generation, uses only inherited/accumulated context

### Context Management Constraints

To simplify the system and prevent context duplication, these settings must follow one key constraint:

- If `inherit_context` is set to "full" or "subset", then `fresh_context` must be "disabled"
- If `fresh_context` is set to "enabled", then `inherit_context` must be "none"

This constraint ensures that either a task inherits context from its parent (fully or partially) OR it generates fresh context via associative matching, but never both. This simplifies implementation and prevents potential duplication of context data.

### Subtype-Based Defaults

For atomic tasks, defaults differ based on the subtype:

- **standard**: Regular atomic tasks default to `inherit_context="full"` and `fresh_context="disabled"`
- **subtask**: Tasks created via CONTINUATION default to `inherit_context="none"` and `fresh_context="enabled"`

This distinction allows regular tasks to smoothly inherit context from their parent, while enabling subtasks to start with fresh, task-specific context.

### Application in Different Operators

#### Atomic Tasks
```xml
<task type="atomic">
    <context_management>
        <inherit_context>full|none|subset</inherit_context>
        <fresh_context>enabled|disabled</fresh_context>
    </context_management>
</task>
```

#### Sequential Tasks
```xml
<task type="sequential">
    <context_management>
        <inherit_context>full|none|subset</inherit_context>
        <accumulate_data>true|false</accumulate_data>
        <accumulation_format>notes_only|full_output</accumulation_format>
        <fresh_context>enabled|disabled</fresh_context>
    </context_management>
</task>
```

#### Reduce Tasks
```xml
<task type="reduce">
    <!-- Parent context management -->
    <context_management>
        <inherit_context>full|none|subset</inherit_context>
        <fresh_context>enabled|disabled</fresh_context>
    </context_management>
    
    <inner_task>
        <!-- Context for processing each input -->
        <context_management>
            <inherit_context>full|none|subset</inherit_context>
            <fresh_context>enabled|disabled</fresh_context>
        </context_management>
    </inner_task>
    
    <reduction_task>
        <!-- Context for combining results -->
        <context_management>
            <inherit_context>full|none|subset</inherit_context>
            <fresh_context>enabled|disabled</fresh_context>
        </context_management>
    </reduction_task>
</task>
```

### Standard Defaults by Operator Type

| Operator Type (Subtype) | inherit_context | accumulate_data | accumulation_format | fresh_context |
|-------------------------|-----------------|-----------------|---------------------|---------------|
| atomic (standard)       | full            | N/A             | N/A                 | disabled      |
| atomic (subtask)        | none            | N/A             | N/A                 | enabled       |
| sequential              | full            | true            | notes_only          | disabled      |
| reduce                  | none            | N/A             | N/A                 | enabled       |
| reduce.inner_task       | full            | N/A             | N/A                 | disabled      |
| reduce.reduction_task   | full            | N/A             | N/A                 | disabled      |

### Validation Rules

1. **Invalid Combinations**:
   - `inherit_context="none"` + `accumulate_data="false"` + `fresh_context="disabled"` results in no context at all, which should trigger a warning
   - When `accumulate_data="true"`, `accumulation_format` must be specified

2. **Required Fields**:
   - `inherit_context` is required in all context_management blocks
   - `fresh_context` is required in all context_management blocks
   - `accumulate_data` is required for sequential tasks
   - If `accumulate_data="true"`, then `accumulation_format` is required

### Context Extension vs. Merging

From ADR:002, we maintain the principle that the system uses **context extension** rather than context merging:

1. Tasks extend from parent context rather than merging multiple contexts
2. No complex merging algorithms are used
3. Context changes are additive, not transformative
4. This simpler model satisfies composition requirements while reducing complexity

### Fixed Configuration Timing

From ADR:005, we maintain that context management settings are fixed at template definition time:

1. Context settings cannot be dynamically modified during task execution
2. Settings remain consistent within a task/sequence
3. This provides predictable behavior and simplifies implementation

### Memory System Scope and Responsibilities

From ADR:005, we clarify that the Memory System:

1. Does not perform ranking or prioritization of matched files
2. Only provides associative matching based on the input structure
3. Returns matches without applying any sorting or relevance scoring
4. Delegates all ranking and selection decisions to the task implementers

### Backward Compatibility

To maintain backward compatibility with existing task definitions:

1. For existing sequential tasks that don't specify `fresh_context`:
   - Default to `fresh_context="enabled"`

2. For existing reduce tasks without context_management:
   - Apply the standard defaults shown in the table above
   - Do not require context_management blocks in existing templates

3. For existing atomic tasks:
   - Default to `inherit_context="full"` and `fresh_context="enabled"`

4. Migration path:
   - Update task templates gradually to include explicit context_management
   - Validate that applied defaults match expected behavior
   - Document any behavior changes in task execution

## Consequences

### Positive

1. **Consistency**: All operators use the same context management model
2. **Clarity**: Explicit control over all three dimensions of context
3. **Flexibility**: Different combinations support diverse workflow needs
4. **Extensibility**: Easy to extend to new operator types
5. **Simplicity**: Maintains extension over merging, reducing complexity

### Negative

1. **Verbosity**: More XML elements required for full specification
2. **Complexity**: More combinations to understand and document
3. **Implementation effort**: Need to update multiple components to support the model

## Implementation Guidelines

### Context Assembly Process

The Evaluator should implement context assembly as follows:

```typescript
function assembleContextForTask(task, parentContext, previousOutputs) {
  // 1. Construct base context input
  const contextInput: ContextGenerationInput = {
    taskText: task.description
  };
  
  // 2. Add parent context based on inheritance setting
  if (task.contextManagement.inheritContext !== 'none') {
    contextInput.inheritedContext = parentContext;
  }
  
  // 3. Add accumulated outputs if applicable
  if (task.contextManagement.accumulateData === true) {
    contextInput.previousOutputs = formatAccumulatedOutputs(
      previousOutputs,
      task.contextManagement.accumulationFormat
    );
  }
  
  // 4. Perform associative matching if fresh context is enabled
  if (task.contextManagement.freshContext === 'enabled') {
    return memorySystem.getRelevantContextFor(contextInput);
  } else {
    // 5. Otherwise, just use the assembled context without matching
    return {
      context: contextInput.inheritedContext || '',
      matches: []
    };
  }
}
```

### Dual Context Tracking

From ADR:005, we maintain dual context tracking when both inheritance and accumulation are active:

1. "Regular" inherited context and "accumulated" context are tracked separately
2. These can be concatenated for use but remain distinct types of input
3. This allows for clean separation between different context sources

### "subset" Mode Implementation

The "subset" mode should be implemented in the following way:

1. When `inherit_context="subset"`, the Evaluator still includes the parent context in the ContextGenerationInput
2. The Memory System's associative matching is responsible for filtering the parent context based on relevance to the task
3. This filtered context is then combined with any accumulated data and fresh context (if enabled)

### Input Structure for Associative Matching

From ADR:005, we clarify the structure of input to getRelevantContextFor():

```typescript
interface ContextGenerationInput {
  taskText: string;              // Always included - the task description
  inheritedContext?: string;     // Included based on inherit_context setting
  previousOutputs?: string;      // Included if accumulate_data is true
}
```

Each section has standardized headings from system configuration, allowing the Memory System to properly identify and process different context components.

### Memory Operation Error Handling

From ADR:002, we maintain a simple error handling approach for memory operations:

1. Memory operations that fail should be retried once
2. If retry fails, surface an informative error
3. Ensure that error propagation follows the standard error taxonomy

### Warning Behavior

From ADR:002, we maintain that resource warning thresholds are purely informative:

1. Warnings do not affect execution flow
2. They serve as indicators of approaching limits
3. Only hard limits trigger execution changes
4. This maintains a clear separation between warnings and errors

### Partial Results Handling

When a subtask fails within an operator:

1. For Sequential tasks with `accumulate_data="true"`:
   - All outputs from completed steps should be preserved
   - These should be included in the error result's `notes.partialResults` field
   - This allows higher-level tasks to handle partial success appropriately

2. For Reduce tasks:
   - If any inner_task fails, the partial results up to that point should be preserved
   - The error should include which input caused the failure
   - No automatic retry should be attempted

## Relationship to Existing ADRs

This ADR partially supersedes but incorporates key principles from:

1. **ADR:002-context-management**:
   - Preserves: Extension vs. merging, warning behavior, memory operation error handling
   - Enhances: Context selection with explicit fresh_context control
   - Replaces: Ambiguous context inheritance for non-sequential operators

2. **ADR:005-context-handling**:
   - Preserves: Dual context tracking, memory system scope limitations, configuration timing
   - Enhances: Operator-specific context support with standardized model
   - Replaces: Limited context inheritance descriptions

Both ADRs remain valuable for historical context and rationale but should be read in conjunction with this standardization ADR for current implementation guidance.

## Examples

### Basic Sequential Task

```xml
<task type="sequential">
    <description>Process data in steps</description>
    <context_management>
        <inherit_context>full</inherit_context>
        <accumulate_data>true</accumulate_data>
        <accumulation_format>notes_only</accumulation_format>
        <fresh_context>enabled</fresh_context>
    </context_management>
    <steps>
        <!-- Steps definition -->
    </steps>
</task>
```

### Isolated Sequential Task

```xml
<task type="sequential">
    <description>Process data in isolation</description>
    <context_management>
        <inherit_context>none</inherit_context>
        <accumulate_data>true</accumulate_data>
        <accumulation_format>full_output</accumulation_format>
        <fresh_context>enabled</fresh_context>
    </context_management>
    <steps>
        <!-- Steps definition -->
    </steps>
</task>
```

### Reduce with Custom Context

```xml
<task type="reduce">
    <description>Process multiple inputs</description>
    <context_management>
        <inherit_context>subset</inherit_context>
        <fresh_context>enabled</fresh_context>
    </context_management>
    <inner_task>
        <context_management>
            <inherit_context>none</inherit_context>
            <fresh_context>disabled</fresh_context>
        </context_management>
        <!-- Inner task definition -->
    </inner_task>
    <reduction_task>
        <context_management>
            <inherit_context>full</inherit_context>
            <fresh_context>enabled</fresh_context>
        </context_management>
        <!-- Reduction task definition -->
    </reduction_task>
</task>
```

### Minimal Context Atomic Task

```xml
<task type="atomic">
    <description>Simple transformation</description>
    <context_management>
        <inherit_context>none</inherit_context>
        <fresh_context>disabled</fresh_context>
    </context_management>
    <!-- Task definition -->
</task>
```

## Related Documents

- [Pattern:ContextFrame:1.0] - Context frame patterns
- [ADR:002-context-management] - Initial context management decisions
- [ADR:005-context-handling] - Context handling clarifications
- [ADR:004-sequential-context-management] - Sequential task context management
</file>
<file path="./system/architecture/decisions/11-subtask-spawning.md" project="">
# Architecture Decision Record: Subtask Spawning Mechanism

## Status
Accepted

## Context
The system needs a standardized mechanism for LLM-to-LLM delegation (subtasks). This differs from tool calls, which are Handler-managed deterministic operations with rigid APIs. The subtask mechanism is essential for adaptive task decomposition, dependency discovery, verification through testing, and dynamic workflow execution.

Current inconsistencies exist in how we handle context management for subtasks, data passing between parent and child tasks, and the relationship between subtasks and other patterns like Director-Evaluator.

## Decision
We will implement a standardized subtask spawning mechanism based on structured continuation requests that fully integrates with our three-dimensional context management model and uses direct parameter passing rather than environment variables.

## Specification

### 1. Subtask Request Structure

Tasks request subtask spawning by returning a result with:

```typescript
interface TaskResult {
    content: string;
    status: "CONTINUATION";
    notes: {
        subtask_request: SubtaskRequest;
        [key: string]: any;
    };
}

interface SubtaskRequest {
    type: string;           // Type of subtask (e.g., "verification")
    description: string;    // Human-readable description
    inputs: {               // Inputs to provide to the subtask
        [key: string]: any;
    };
    template_hints?: string[]; // Optional hints for template matching
    return_expectations?: string; // Expected output format
    max_depth?: number;     // Optional depth override
    subtype?: string;       // Optional subtype for atomic tasks (defaults to "subtask")
    file_paths?: string[];  // Optional list of specific files to include in context
}
```

### 2. Context Management Integration

Subtasks fully integrate with our three-dimensional context management model:

```xml
<task>
    <context_management>
        <inherit_context>full|none|subset</inherit_context>
        <accumulate_data>true|false</accumulate_data>
        <accumulation_format>notes_only|full_output</accumulation_format>
        <fresh_context>enabled|disabled</fresh_context>
    </context_management>
</task>
```

Default settings for subtasks:
- `inherit_context`: "none"
- `accumulate_data`: "false" 
- `fresh_context`: "enabled"

These defaults follow the mutual exclusivity constraint: When `fresh_context` is "enabled", `inherit_context` must be "none".

These defaults can be overridden in subtask templates.

### Explicit File Selection

When creating subtasks, parent tasks can explicitly specify which files to include:

```typescript
// The file_paths field takes precedence over associative matching
subtask_request = {
  type: "atomic",
  description: "Analyze specific modules",
  inputs: { /* parameters */ },
  context_management: { inherit_context: "subset" },
  file_paths: ["/src/main.py", "/src/utils.py"]
}
```

This feature:
- Creates a special file-specific context mode outside the standard three-dimensional model
- Always includes specified files in the context, regardless of other settings
- Takes precedence over associative matching when determining context subset

### Subtask Context Management

When creating a subtask request:

1. The `subtype` field defaults to "subtask" for atomic tasks created via CONTINUATION
2. Default context settings for subtasks are:
   - `inherit_context`: "none"
   - `accumulate_data`: "false"
   - `fresh_context`: "enabled"

3. These defaults can be overridden by explicitly specifying context_management settings
4. All context settings must follow the mutual exclusivity constraint:
   - If `inherit_context` is "full" or "subset", then `fresh_context` must be "disabled"
   - If `fresh_context` is "enabled", then `inherit_context` must be "none"

5. For atomic tasks with `subtype="subtask"`, context settings automatically follow subtask defaults
   unless explicitly overridden

### 3. Data Flow Between Tasks

Subtasks use direct parameter passing rather than environment variables:

```mermaid
flowchart LR
    A[Parent Task] -->|"TaskResult{status:CONTINUATION,\n notes:{subtask_request}}"| B[Task System]
    B -->|"Template Selection\n& Direct Input Passing"| C[Subtask]
    C -->|"TaskResult"| D[Task System]
    D -->|"Resume with\n{subtask_result:TaskResult}"| A
```

### 4. Execution Flow

When a task returns with CONTINUATION status and a subtask_request:

```typescript
// Simplified execution flow
async function executeTask(task, inputs, depth = 0) {
    // Check depth limits
    if (depth >= (task.maxDepth || systemConfig.maxTaskDepth)) {
        return createMaxDepthError();
    }
    
    // Get or create Handler for this task
    const handler = getHandlerForTask(task);
    
    // Execute initial prompt
    const result = await handler.executePrompt(task, inputs);
    
    if (result.status === "CONTINUATION" && result.notes?.subtask_request) {
        const subtaskRequest = result.notes.subtask_request;
        const subtaskTemplate = await findMatchingTemplate(subtaskRequest);
        
        // Execute subtask with incremented depth
        const subtaskResult = await executeTask(subtaskTemplate, subtaskRequest.inputs, depth + 1);
        
        // Add subtask result as a tool response to parent's Handler session
        handler.addToolResponse(
            getToolNameFromRequest(subtaskRequest),
            subtaskResult.content
        );
        
        // Continue parent task execution with the tool response in its history
        return handler.executePrompt(
            task,
            "Continue based on the tool results."
        );
    }
    
    return result;
}
```

### 5. Result Structure Standardization

All task results follow a consistent base structure:

```typescript
interface TaskResult {
    content: string;        // Main output content
    status: ReturnStatus;   // COMPLETE, CONTINUATION, FAILED
    notes: {                // Additional metadata
        [key: string]: any;
    };
}
```

Specialized subtasks may use consistent extensions of this structure for specific needs (evaluation, script execution, etc.).

### 6. Depth Control

To prevent infinite recursion:

- System maintains a depth counter for each task chain
- Default maximum depth: 10 levels (configurable via `maxTaskDepth`)
- Subtasks may specify an override via `max_depth` parameter
- Cycle detection uses hash-based subtask signature tracking
- When max depth is reached, system returns appropriate error

### 7. Error Handling & Partial Results

Subtask failures are passed to the parent task with:
- Full error classification (reason, details)
- Any partial results, when available
- Parent tasks can implement specific recovery strategies based on error type

```typescript
{
  type: 'TASK_FAILURE',
  reason: 'subtask_failure', // Or other specific reason
  message: 'Subtask execution failed',
  details: {
    subtaskType: string,
    partialResults?: any,
    originalError: TaskError
  }
}
```

## Context Combinations Reference

| inherit_context | accumulate_data | fresh_context | Result |
|-----------------|-----------------|---------------|--------|
| full | false | disabled | Subtask has only parent context (DEFAULT) |
| full | false | enabled | Parent context + additional context |
| full | true | disabled | Parent context + accumulated data |
| full | true | enabled | Parent context + accumulated data + additional context |
| none | false | disabled | Minimal context (inputs only) |
| none | false | enabled | Fresh context only |
| none | true | disabled | Accumulated data only |
| none | true | enabled | Accumulated data + fresh context |
| subset | false | disabled | Relevant parent context only |
| subset | false | enabled | Relevant parent context + additional context |
| subset | true | disabled | Relevant parent context + accumulated data |
| subset | true | enabled | Relevant parent context + accumulated data + additional context |

## Relationship to Director-Evaluator Pattern

The Subtask Spawning mechanism and Director-Evaluator Loop are complementary features:

| Director-Evaluator Loop | Subtask Spawning Mechanism |
|-------------------------|----------------------------|
| Specialized higher-level pattern | General-purpose primitive |
| Built for iterative refinement | Ad-hoc dynamic task creation |
| Predefined iteration structure | Flexible composition pattern |
| Built-in termination conditions | Manual continuation control |

**When to use Director-Evaluator Loop:**
- Iterative refinement processes
- Create-evaluate feedback cycles
- Multiple potential iterations
- External validation via scripts

**When to use Subtask Spawning:**
- One-off subtask creation
- Dynamic task composition
- Task flows that aren't primarily iterative
- Complex task trees with varying subtypes

## Implementation Requirements

1. **Task System Enhancements**
   - Add CONTINUATION status processing
   - Implement template matching for subtasks
   - Add depth tracking and enforcement
   - Provide direct input/output mapping

2. **Testing Requirements**
   - Unit tests for depth control mechanisms
   - Integration tests for context inheritance
   - Verification of proper result passing
   - Error propagation tests
   - Resource usage monitoring tests
   - Cycle detection tests

## Migration Guidance

1. Replace environment variable usage with direct parameter passing:
   ```typescript
   // Old approach - using environment variables
   env.set('subtask_result', subtaskResult);
   
   // New approach - direct parameter passing
   return executeTask(task, { 
     ...inputs, 
     subtask_result: subtaskResult 
   });
   ```

2. Update subtask generation to use standardized SubtaskRequest format:
   ```typescript
   // Old approach - varied formats
   return {
     status: "CONTINUATION",
     notes: { task: "verify_code", code: generatedCode }
   };
   
   // New approach - standardized format
   return {
     content: "Generated code",
     status: "CONTINUATION",
     notes: {
       subtask_request: {
         type: "verification",
         description: "Verify generated code",
         inputs: { code: generatedCode },
         template_hints: ["code_verification", "testing"]
       }
     }
   };
   ```

3. Add explicit context_management blocks to templates:
   ```xml
   <!-- Old version - implicit context -->
   <task>
     <description>Subtask description</description>
   </task>
   
   <!-- New version - explicit context management -->
   <task>
     <description>Subtask description</description>
     <context_management>
       <inherit_context>full</inherit_context>
       <accumulate_data>false</accumulate_data>
       <fresh_context>disabled</fresh_context>
     </context_management>
   </task>
   ```

4. Implement depth tracking if not already present:
   ```typescript
   // Add depth parameter to executeTask
   function executeTask(task, inputs, depth = 0) {
     if (depth >= maxTaskDepth) {
       return { 
         status: "FAILED", 
         type: "TASK_FAILURE",
         reason: "max_depth_exceeded" 
       };
     }
     
     // Execute subtasks with incremented depth
     const subtaskResult = await executeTask(
       subtask, 
       subtaskInputs, 
       depth + 1
     );
   }
   ```

## Performance Considerations

- Monitor context size in deep subtask chains
- Cache frequently used subtask templates
- Ensure proper cleanup after subtask completion
- Set appropriate depth limits based on performance testing
- Consider implementing selective context preservation to reduce token usage
- Implement subtask signature tracking to prevent cycles
- Optimize template matching for commonly used subtask types
- Consider adding timeouts for long-running subtask chains

## Related ADRs
- **Depends on**: [ADR 7: Context Management Standardization], [ADR 8: Error Taxonomy]
- **Extends**: [ADR 9: Partial Results Policy]
- **Related to**: [ADR 10: Evaluator-to-Director Feedback Flow]

## Consequences

### Positive
- Standardized mechanism for dynamic task composition
- Clear data flow between parent and subtasks
- Consistent context management across all task types
- Explicit depth control prevents runaway execution
- Direct parameter passing improves clarity and debugging
- Template hints enable flexible subtask selection
- Structured error handling improves recovery options
- The simplified tool-response based approach offers several advantages:
  - No special resumption methods required
  - Parent Handler session preserved throughout execution
  - From the LLM's perspective, subtask results appear as regular tool responses
  - Cleaner error handling with standard patterns
  - More intuitive control flow with clearer component boundaries

### Negative
- Increased implementation complexity
- Additional depth tracking overhead
- More complex debugging for deep task chains
- Potential performance impact with deeply nested subtasks
- Learning curve for understanding context combinations
- Requires careful management of template libraries
</file>
<file path="./system/architecture/decisions/13-json-output.md" project="">
# Architecture Decision Record: Output Standardization

## Status
Proposed

## Context
The current system architecture allows tasks to return outputs in arbitrary text formats. While flexible, this approach has several limitations:

1. **Type Ambiguity**: When binding task outputs to variables, there's no reliable way to determine the intended data type.
2. **Structure Inconsistency**: Tasks that need to return complex data (lists, objects) lack a standardized format.
3. **Parsing Burden**: Downstream tasks must implement custom parsing logic to extract structured data.
4. **Error Proneness**: String-based data exchange is error-prone and difficult to validate.
5. **Poor Composability**: Without structured outputs, it's challenging to compose tasks effectively.

These issues hinder task composition, variable binding, and reliable data flow between tasks. As the system evolves toward a function-based model, the need for structured data exchange becomes increasingly important.

## Decision
We will implement an optional but well-supported JSON-based output standardization mechanism with the following characteristics:

1. **Optional JSON Declaration**: Templates can explicitly specify JSON output format using an `<output_format>` element.
   ```xml
   <task>
     <description>List files in directory</description>
     <output_format type="json" schema="string[]" />
   </task>
   ```

2. **Content Field for Output**: Structured outputs will remain in the TaskResult.content field, preserving the existing interface.

3. **Automatic Format Detection**: The evaluator will attempt to detect and parse JSON outputs even when not explicitly declared.

4. **Basic Type Validation**: The system will validate that outputs match the expected basic type (object, array, string, number, boolean).

5. **Backward Compatibility**: Free-text outputs will continue to be supported as the default option.

6. **Integration with Function Model**: For the function-based template model, a simplified `returns` attribute will provide type information.
   ```xml
   <template name="get_file_info" params="filepath" returns="object">
   ```

## Consequences

### Positive
1. **Type Safety**: Enables reliable binding of task outputs to variables with correct types.
2. **Structured Data Exchange**: Facilitates passing complex data between tasks.
3. **Basic Validation**: Ensures outputs match expected types.
4. **Enhanced Composition**: Improves task composition through predictable data formats.
5. **Language Integration**: Provides natural mapping to native types in the Evaluator.
6. **Backward Compatibility**: Existing templates continue to work without modification.

### Negative
1. **Some Type Ambiguity**: Basic type validation only; no property or structure validation.
2. **Performance Overhead**: JSON parsing adds computational overhead.
3. **Implementation Effort**: Requires updates to parser, evaluator, and error handling.

## Implementation Guidance

### JSON Detection Algorithm

The Evaluator will implement a JSON detection algorithm:

```typescript
function detectAndParseJson(content: string): { isParsed: boolean; value: any } {
  // Skip detection if clearly not JSON
  if (typeof content !== 'string') {
    return { isParsed: false, value: content };
  }
  
  const trimmed = content.trim();
  
  // Check for JSON-like patterns
  if (trimmed.startsWith('{') || 
      trimmed.startsWith('[') || 
      /^(true|false|null|\d)/.test(trimmed)) {
    try {
      const parsed = JSON.parse(trimmed);
      return { isParsed: true, value: parsed };
    } catch (e) {
      // Not valid JSON despite appearance
      return { isParsed: false, value: content };
    }
  }
  
  return { isParsed: false, value: content };
}
```

### Basic Type Validation

For type validation, we'll implement a lightweight validator focused on the core JSON types:

```typescript
function validateType(value: any, expectedType: string): boolean {
  if (expectedType === 'object') return typeof value === 'object' && value !== null;
  if (expectedType === 'array') return Array.isArray(value);
  if (expectedType === 'string') return typeof value === 'string';
  if (expectedType === 'number') return typeof value === 'number';
  if (expectedType === 'boolean') return typeof value === 'boolean';
  return false; // Unknown type
}
```

### Error Handling

Output validation errors will fit into the existing error taxonomy as a specific type of `TASK_FAILURE`:

```typescript
{
  type: 'TASK_FAILURE',
  reason: 'output_format_failure',
  message: `Expected output of type "${expectedType}" but got "${actualType}"`,
  details: {
    expectedType: expectedType,
    actualType: typeof value
  }
}
```

### TaskResult Extension

The TaskResult interface will be minimally extended:

```typescript
interface TaskResult {
  content: string;
  status: ReturnStatus;
  notes: {
    [key: string]: any;
  };
  // New field
  parsedContent?: any;  // Present if content was parsed as JSON
}
```

### AST Extensions

The AST will be extended with minimal changes to support output format declarations:

```typescript
interface TaskNode extends ASTNode {
  // Existing properties...
  outputFormat?: {
    type: "json" | "text";
    schema?: string;  // Basic type only: "object", "array", "string", etc.
  };
}

// Extension for function-based model
interface TemplateNode extends ASTNode {
  // Existing properties...
  returns?: string; // Simple type declaration
}
```

## Compatibility Considerations

### Template Evolution

Existing templates without output format declarations will continue to work as before. The Evaluator will attempt to auto-detect JSON even without explicit declarations.

### XML Schema Evolution

The `<output_format>` element will be added to the schema in a backward-compatible way, keeping it optional for all task types.

## Integration with Other Features

### Function-Based Template Model

This feature integrates with the proposed function-based template model through the `returns` attribute, which specifies a simple return type.

### Error Taxonomy

The new error type for output validation integrates with the existing error taxonomy from ADR 8 (Error Taxonomy for Context Issues), using the established pattern of specific failure reasons within the `TASK_FAILURE` type.

## Future Extensions

This initial implementation focuses on the core functionality. In the future, we may consider:

1. **Advanced Schema Validation**: Property-level validation for objects and item validation for arrays.
2. **Custom Validation Rules**: Additional validation beyond basic types.
3. **Schema Registry**: Central repository for common schema types.

However, these extensions will only be implemented if there's evidence of need based on real usage patterns.

## Related Decisions

- This builds on [ADR 8: Error Taxonomy for Context Issues] for error handling consistency.
- This complements the proposed "Function-Based Template Model" ADR for the `returns` attribute integration.

## Affected Documentation

After acceptance, these documents will need updates:

1. **system/contracts/protocols.md**: Add `<output_format>` to XML schema
2. **components/task-system/spec/types.md**: Add minimal interface extensions
3. **components/evaluator/README.md**: Document parsing and binding behavior
4. **components/task-system/spec/behaviors.md**: Define expected behaviors for outputs
5. **system/architecture/patterns/errors.md**: Add new error category
6. **components/task-system/impl/xml-processing.md**: Update validation rules
7. **system/README.md**: Update examples to demonstrate structured outputs
</file>
<file path="./system/architecture/decisions/006-context-types.md" project="">
<!-- This file is currently under construction.
It will cover decisions on the precise data types for context representation.
For now, refer to the context management ADRs (002 and 005).
-->
</file>
<file path="./system/architecture/questions.md" project="">
# Open Architecture Questions

This document now lists only *unresolved* questions. Many items previously listed have been answered in the ADRs or clarified in the patterns. For the MVP, the matching logic is uniform for atomic tasks, and no operator‑specific matching or versioning considerations are applied.

## Unresolved Questions

1. **Context Generation Failures:** When associative matching fails, should partial outputs be preserved for re‑try? (See ADRs 002 and 005 for related discussion.)

2. **Subtask Spawning Mechanism:** How exactly should subtasks be created dynamically (beyond the Director‑Evaluator pattern)? This remains an open design area.

For additional background, see also [decisions/005-context-handling.md](decisions/005-context-handling.md) and [questions.md](questions.md) in previous revisions.
</file>
<file path="./components/evaluator/README.md" project="">
# Evaluator Component

## Overview

The **Evaluator** is the unified task-execution component of the system. It is responsible for:

1. **Controlling AST processing and execution**  
2. **Managing failure recovery** via standard task return statuses (`COMPLETE`, `CONTINUATION`, `FAILED`)
3. **Tracking resource usage** (in coordination with Handlers)
4. **Handling reparse/decomposition requests** when tasks fail (e.g. due to resource exhaustion or invalid output)

## Core Responsibilities

### AST Execution Controller  
- Orchestrates the step-by-step or operator-by-operator execution of tasks represented as an AST
- Calls out to the Handler for LLM-specific interactions and resource tracking
- Interacts with the Compiler when re-parsing or decomposition is required

### Failure Recovery  
- Detects or receives error signals when tasks fail or exceed resources  
- Initiates "reparse" tasks or alternative decomposition approaches if the system's policies allow
- Surfaces errors back to the Task System or parent contexts

### Resource Usage Coordination  
- Integrates with Handler's resource tracking
- Aware of usage or limit errors and decides whether to attempt decomposition or fail outright

### Context and Environment Handling  
- Ensures proper propagation of parameters and context in multi-step tasks
- Uses direct parameter passing between tasks rather than environment variables
- Leverages Memory System 3.0 for associative context retrieval
- Manages all dimensions of the context management model

### Template Substitution
- Solely responsible for resolving all template variables before passing tasks to the Handler
- Handles different resolution rules for function vs. standard templates
- Ensures all execution happens with fully resolved inputs

## Integration Points

- **Task System**: Receives structured tasks for execution
- **Handler**: Delegates LLM interactions and resource tracking
- **Memory System**: Retrieves context via associative matching
- **Compiler**: Requests reparsing when needed

## References

For detailed implementation design, see [components/evaluator/impl/design.md](./impl/design.md).

For interface definitions, see [components/evaluator/api/interfaces.md](./api/interfaces.md).

For error handling patterns, see [Pattern:Error:1.0](../../system/architecture/patterns/errors.md).

For context management, see [Pattern:ContextFrame:1.0](../../system/architecture/patterns/context-frames.md).
</file>
<file path="./components/memory/README.md" project="">
# Memory System Component [Version 3.0]

## Overview

The Memory System provides context management and associative matching services for task execution. It maintains a global metadata index to support context retrieval while delegating actual file operations to Handler tools. The system focuses purely on metadata management and context retrieval - it does not store file content, perform file operations, track resources, or rank matches.

## Core Architecture

The Memory System follows a read-only context model with a clear separation of responsibilities:

- **Metadata Management**: Maintains file paths and descriptive metadata
- **Associative Matching**: Provides context retrieval based on task relevance
- **No File Operations**: Delegates all file I/O to Handler tools
- **Bulk Index Updates**: Supports global index management through bulk operations

## Core Interface

The Memory System provides a standardized interface as defined in [Interface:Memory:3.0]. This interface supports:

- Global metadata index management through bulk operations
- Context retrieval through associative matching
- Clear separation between metadata management and file operations

**IMPORTANT: The Memory System NEVER performs file I/O operations (reading, writing, deletion). All file operations are exclusively handled by Handler tools.**

**NOTE: As of version 3.0, the Memory System follows a read-only context model. The updateContext method has been removed, and all context must be managed through the appropriate context management mechanisms in the Task System.**

For the complete interface specification, method signatures, and type definitions, see [components/memory/api/interfaces.md](./api/interfaces.md).

## Integration Points

The Memory System integrates with other components through clear boundaries:

- **Task System**: Receives context requests and provides associative matching results
- **Evaluator**: Processes context for task execution
- **Handler**: Provides file access tools for content retrieval
- **Error Handling**: Maps failures to standard TASK_FAILURE errors

## Usage

The Memory System is typically used for two main purposes: managing the global metadata index and retrieving context for task execution. Here's a typical usage pattern:

```typescript
// Initialize and update index
const memory = new MemorySystem();
await memory.updateGlobalIndex(new Map([
    ['data.txt', 'Experimental results from 2024-02'],
    ['config.json', 'System configuration parameters']
]));

// Request context for task execution
const result = await memory.getRelevantContextFor({
    taskText: "Analyze recent experimental results",
    inheritedContext: "Previous analysis focused on temperature variance",
    previousOutputs: "Initial data validation complete"
});
```

## Related Documents

For detailed implementation specifications and patterns, refer to:
- [Pattern:ContextFrame:1.0](../../system/architecture/patterns/context-frames.md)
- [ADR:Memory:1.0](../../system/architecture/decisions/needs_update/001-memory-system.md)
</file>
<file path="./components/memory/api/interfaces.md" project="">
# Memory System Interfaces [Interface:Memory:3.0]

> This document is the authoritative source for the Memory System public API.

## Overview

The Memory System provides metadata management and associative matching services for task execution. It follows a read-only context model with no direct file operations.

**IMPORTANT: The Memory System manages ONLY metadata about files (paths and descriptive strings).
It does NOT perform any file I/O operations - all file reading, writing, and deletion
is handled exclusively by Handler tools.**

**NOTE: As of version 3.0, the Memory System follows a read-only context model.
The updateContext method has been removed to enforce better architectural boundaries.**

## Interface Methods

```typescript
/**
 * Memory System Interface [Interface:Memory:3.0]
 */
interface MemorySystem {
    /**
     * Get global file metadata index
     * @returns Promise resolving to the global index
     */
    getGlobalIndex(): Promise<GlobalIndex>;
    
    /**
     * Update global file metadata index
     * @param index New index to set
     * @returns Promise resolving when update is complete
     */
    updateGlobalIndex(index: GlobalIndex): Promise<void>;
    
    /**
     * Get relevant context for a task
     * 
     * The Memory System does NOT perform ranking or prioritization of matches.
     * It only provides associative matching based on the input structure.
     * It does NOT read file contents - it only returns file paths and metadata.
     *
     * @param input - The ContextGenerationInput containing task context
     * @returns Promise resolving to associative match result
     * @throws {INVALID_INPUT} If the input structure is malformed or missing required fields
     */
    getRelevantContextFor(input: ContextGenerationInput): Promise<AssociativeMatchResult>;
}
```

## Integration Points

- **Handler**: Uses file paths from AssociativeMatchResult to read files via tools
- **Task System**: Uses Memory System for context retrieval during task execution
- **Evaluator**: Coordinates context retrieval for task execution

## Type References

For Memory System specific types, see [Type:Memory:3.0] in `/components/memory/spec/types.md`.
For system-wide types, see [Type:System:1.0] in `/system/contracts/types.md`.

## Contract References

For integration contract details, see [Contract:Integration:TaskMemory:3.0] in `/system/contracts/interfaces.md`.
</file>
<file path="./components/task-system/impl/examples.md" project="">
# Task System Implementation Examples

> **Important Note:** The examples in this document demonstrate that templates and function-based templates can define any task type (atomic, sequential, reduce, etc.). While all task types can be defined as templates, only atomic task templates participate in the template matching process used for task selection based on natural language descriptions. This distinction is important for understanding how tasks, templates, and functions interact in the system.

> **Note:** For a detailed description of the Director‑Evaluator pattern, refer to [system/architecture/patterns/director-evaluator.md](../system/architecture/patterns/director-evaluator.md).

## Basic Task Execution
```typescript
// Initialize TaskSystem
const taskSystem = new TaskSystem({
  maxTurns: 10,
  maxContextWindowFraction: 0.8,
  systemPrompt: "Default system prompt"
});

// Register handlers
taskSystem.onError((error) => {
  console.error('Task error:', error);
});

taskSystem.onWarning((warning) => {
  console.warn('XML validation warning:', warning.message);
});

// Execute task
const result = await taskSystem.executeTask(
  "analyze data",
  memorySystem
);

// Check XML parsing status
if (!result.outputs.some(output => output.wasXMLParsed)) {
  console.warn("XML parsing failed, using fallback string output");
}
```

## Task Management
```typescript
const taskDef: TaskDefinition = {
  name: "process_data",
  type: "atomic",
  provider: "anthropic",
  model: "claude-3-sonnet",
  body: {
    type: "atomic",
    content: `<task>
      <description>Process data using specific format</description>
      <inputs>
        <input name="raw_data">
          <description>Load and validate input data</description>
          <expected_output>
            Validated data in standard format:
            - Field validations complete
            - Type conversions applied
            - Missing values handled
          </expected_output>
        </input>
      </inputs>
      <expected_output>
        Processed data meeting format requirements:
        - Correct structure
        - Valid field types
        - Complete required fields
      </expected_output>
    </task>`
  },
  metadata: {
    isManualXML: true,
    disableReparsing: true
  }
};

// Register the task
await taskSystem.registerTask(taskDef);

// Validate task
const validation = taskSystem.validateTask(taskDef);

if (!validation.valid) {
  console.warn('Task validation warnings:', validation.warnings);
}

// Find matching tasks
const matches = await taskSystem.findMatchingTasks(
  "analyze peak patterns",
  memorySystem
);

console.log('Found matching tasks:', 
  matches.map(m => ({
    score: m.score,
    type: m.task.type,
    name: m.task.name
  }))
);
```

## Resource Management
```typescript
// Configure with resource limits
const taskSystem = new TaskSystem({
  maxTurns: 5,
  maxContextWindowFraction: 0.5,
  systemPrompt: "Resource-constrained execution"
});

try {
  const result = await taskSystem.executeTask(
    "process large dataset",
    memorySystem
  );
} catch (error) {
  if (error.type === 'RESOURCE_EXHAUSTION') {
    console.log('Resource limit exceeded:', error.resource);
    console.log('Usage metrics:', error.metrics);
  }
}
```

## Output Format Examples

### Task with JSON Output Format
```xml
<task type="atomic">
  <description>List files in directory</description>
  <output_format type="json" schema="string[]" />
</task>
```

### Template with Return Type
```xml
<template name="get_file_info" params="filepath" returns="object">
  <task>
    <description>Get metadata for {{filepath}}</description>
    <output_format type="json" schema="object" />
  </task>
</template>
```

### Function Call with JSON Result
```xml
<task type="sequential">
  <steps>
    <task>
      <description>Get directory listing</description>
    </task>
    <call template="get_file_info">
      <arg>result[0]</arg>
    </call>
  </steps>
</task>
```

### TypeScript Example
```typescript
// Execute task with JSON output format
const result = await taskSystem.executeTask(
  "<task><description>List files</description><output_format type='json' schema='string[]'/></task>",
  memorySystem
);

// Check if content was parsed as JSON
if (result.parsedContent) {
  // Use the parsed content as a typed value
  const files: string[] = result.parsedContent;
  console.log(`Found ${files.length} files`);
  
  // Process the files array
  const textFiles = files.filter(file => file.endsWith('.txt'));
  console.log(`Text files: ${textFiles.join(', ')}`);
} else {
  // Fall back to string content
  console.log(`Raw output: ${result.content}`);
}
```

## Task Definition and Function Calling

### Basic Function-Style Task Definition
```xml
<task name="validate_input" type="atomic" parameters="data,rules">
  <description>Validate {{data}} against {{rules}}</description>
  <context_management>
    <inherit_context>none</inherit_context>
    <fresh_context>enabled</fresh_context>
  </context_management>
</task>
```

### Context Management Mutual Exclusivity Examples

```xml
<!-- Valid: inherit_context="none" and fresh_context="enabled" -->
<task type="atomic">
  <description>Process data with fresh context only</description>
  <context_management>
    <inherit_context>none</inherit_context>
    <accumulate_data>false</accumulate_data>
    <fresh_context>enabled</fresh_context>
  </context_management>
</task>

<!-- Valid: inherit_context="full" and fresh_context="disabled" -->
<task type="atomic">
  <description>Process data with inherited context only</description>
  <context_management>
    <inherit_context>full</inherit_context>
    <accumulate_data>false</accumulate_data>
    <fresh_context>disabled</fresh_context>
  </context_management>
</task>

<!-- Invalid: inherit_context="full" and fresh_context="enabled" -->
<!-- This would cause validation failure -->
<task type="atomic">
  <description>Invalid combination</description>
  <context_management>
    <inherit_context>full</inherit_context>
    <accumulate_data>false</accumulate_data>
    <fresh_context>enabled</fresh_context>
  </context_management>
</task>
```

### Function Call with Variable Arguments
/**
 * Function Call XML Syntax
 * 
 * Arguments are evaluated in the caller's environment before being passed to the task:
 */
```xml
<call task="validate_input">
  <arg>user_input</arg>        <!-- Resolved as variable if possible -->
  <arg>validation_schema</arg>  <!-- Or used as literal if no variable matches -->
</call>
```

### Function-Style Task with Return Type
```xml
<task name="extract_metrics" type="atomic" parameters="log_data" returns="object">
  <description>Extract performance metrics from {{log_data}}</description>
  <output_format type="json" schema="object" />
</task>
```

### Complex Function Composition
```xml
<task type="sequential">
  <steps>
    <task>
      <description>Load input data</description>
    </task>
    <call task="validate_input">
      <arg>loaded_data</arg>
      <arg>{"required": ["name", "email"], "format": {"email": "email"}}</arg>
    </call>
    <call task="process_validated_data">
      <arg>validation_result</arg>
      <arg>processing_options</arg>
    </call>
  </steps>
</task>
```

/**
 * Function-Style Task and Call Example
 */
```typescript
// 1. Register function-style task with explicit parameters
await taskSystem.registerTask({
  name: "analyze_data",
  type: "atomic",
  parameters: ["dataset", "config"],  // Explicitly declared parameters
  body: {
    type: "atomic",
    content: "Analyze {{dataset}} using {{config}}",
  },
  returns: "object",
  provider: "anthropic"
});

// 2. Setup caller environment with variables
const callerEnv = new Environment({
  data_file: "sensor_readings.csv",
  analysis_options: {method: "statistical", outliers: "remove"}
});

// 3. Execute call with arguments evaluated in caller's environment
const result = await taskSystem.executeCall({
  taskName: "analyze_data",  // Changed from templateName to taskName
  arguments: [
    "data_file",         // Resolved to "sensor_readings.csv" from callerEnv
    "analysis_options"   // Resolved to the object from callerEnv
  ]
}, callerEnv);

// 4. Example with mixed variable/literal arguments
const mixedResult = await taskSystem.executeCall({
  taskName: "analyze_data",  // Changed from templateName to taskName
  arguments: [
    "data_file",                     // Variable lookup
    {method: "custom", limit: 100}   // Direct literal (no lookup)
  ]
}, callerEnv);
```

## Global Index Example
```typescript
// Minimal memory object focusing on file metadata
const memory = {
  getGlobalIndex() {
    return new Map([
      ['data.txt', 'metadata'],
      ['config.json', 'metadata'],
      ['history.log', 'metadata']
    ]);
  },
  updateGlobalIndex(index) {}
};

// Now we can pass this memory object to the taskSystem
const result = await taskSystem.executeTask("analyze recent changes", memory);
```

## Specialized Task Types

### Reparse Task Example
```typescript
const reparseTemplate: TaskTemplate = {
  taskPrompt: `<task type="reparse">
    <description>Decompose large task into smaller units</description>
    <failed_task>
      <error type="RESOURCE_EXHAUSTION">
        <resource>context</resource>
        <message>Context window limit exceeded</message>
      </error>
      <original_prompt>Process entire codebase</original_prompt>
    </failed_task>
  </task>`,
  systemPrompt: "Decomposition specialist",
  model: "claude-3-sonnet",
  isManualXML: true
};

try {
  const result = await taskSystem.executeTask(
    reparseTemplate.taskPrompt,
    memorySystem,
    "reparse"
  );
} catch (error) {
  console.error('Reparse failed:', error);
}
```

### Memory Task Example
```typescript
const memoryTemplate: TaskTemplate = {
  taskPrompt: `<task type="associative_memory">
    <description>Find relevant context for implementation</description>
    <query>error handling patterns</query>
    <constraints>
      <max_results>3</max_results>
      <relevance_threshold>0.8</relevance_threshold>
    </constraints>
  </task>`,
  systemPrompt: "Context retrieval specialist",
  model: "claude-3-sonnet"
};

const memoryResult = await taskSystem.executeTask(
  memoryTemplate.taskPrompt,
  memorySystem,
  "associative_memory"
);

console.log('Retrieved context:', memoryResult.content);
```

---

// Example of error handling with simplified structure
try {
  const result = await taskSystem.executeTask(taskDefinition, memorySystem);
  // Success case: content is complete
  console.log("Task completed successfully:", result.content);
} catch (error) {
  if (error.type === 'RESOURCE_EXHAUSTION') {
    console.log(`Resource limit exceeded: ${error.resource}`);
  } else if (error.type === 'TASK_FAILURE') {
    // Access partial content directly from content field
    console.log(`Partial output before failure: ${error.content}`);
    console.log(`Execution stage: ${error.notes.executionStage || "unknown"}`);
    console.log(`Completion: ${error.notes.completionPercentage || 0}%`);
  }
}

<!-- Example: Sequential Task with Default Context Management -->
<task type="sequential">
    <description>Process and analyze data</description>
    <!-- No context_management block - using defaults:
         inherit_context: full
         accumulate_data: true
         accumulation_format: notes_only
         fresh_context: enabled -->
    <steps>
        <task>
            <description>Load dataset</description>
            <inputs>
                <input name="data_file" from="csv_file_path"/>
            </inputs>
        </task>
        <task>
            <description>Filter invalid rows</description>
        </task>
    </steps>
</task>

<!-- Example: Sequential Task with Custom Context Management -->
<task type="sequential">
    <description>Process and analyze data</description>
    <context_management>
        <inherit_context>none</inherit_context>
        <accumulate_data>true</accumulate_data>
        <accumulation_format>full_output</accumulation_format>
        <fresh_context>enabled</fresh_context>
    </context_management>
    <steps>
        <task>
            <description>Load dataset</description>
            <inputs>
                <input name="data_file" from="csv_file_path"/>
            </inputs>
        </task>
        <task>
            <description>Filter invalid rows</description>
        </task>
    </steps>
</task>

<!-- Example: Static Director-Evaluator Loop with Script Execution -->
<task type="director_evaluator_loop">
  <description>Process and evaluate code</description>
  <max_iterations>3</max_iterations>
  <context_management>
    <inherit_context>none</inherit_context>
    <accumulate_data>true</accumulate_data>
    <accumulation_format>notes_only</accumulation_format>
    <fresh_context>enabled</fresh_context>
  </context_management>
  <director>
    <description>Generate Python code to solve problem: {{problem_statement}}</description>
    <inputs>
      <input name="problem_statement" from="user_query"/>
      <input name="feedback" from="evaluation_feedback"/>
      <input name="iteration" from="current_iteration"/>
    </inputs>
  </director>
  <script_execution>
    <command>python3 -c "{{script_input}}"</command>
    <timeout>5</timeout>
    <inputs>
      <input name="script_input" from="director_result"/>
    </inputs>
  </script_execution>
  <evaluator>
    <description>Evaluate code quality and execution results</description>
    <inputs>
      <input name="code" from="director_result"/>
      <input name="execution_output" from="script_output"/>
      <input name="execution_errors" from="script_errors"/>
      <input name="exit_code" from="script_exit_code"/>
    </inputs>
  </evaluator>
  <termination_condition>
    <condition>evaluation.success === true || iteration >= 3</condition>
  </termination_condition>
</task>

## Subtask Spawning Examples

### Basic Subtask Request Example

```typescript
// Parent task implementation
async function processComplexTask(input: string): Promise<TaskResult> {
  // Determine if we need to spawn a subtask
  if (isComplexInput(input)) {
    // Return a continuation with subtask request
    return {
      content: "Need to process complex input with a specialized subtask",
      status: "CONTINUATION",
      notes: {
        subtask_request: {
          type: "atomic",
          description: "Process complex data structure",
          inputs: {
            data: input,
            format: "json",
            validation_rules: { required: ["id", "name"] }
          },
          template_hints: ["data_processor", "validator"],
          context_management: {
            inherit_context: "subset",
            fresh_context: "enabled"
          }
        }
      }
    };
  }
  
  // Process simple input directly
  return {
    content: `Processed: ${input}`,
    status: "COMPLETE",
    notes: { dataUsage: "Simple processing completed" }
  };
}

// Example of subtask tool execution
async function executeWithSubtasks() {
  // Create a Handler with session preservation
  const handler = new Handler(handlerConfig);
  
  // Register subtask tool
  handler.registerSubtaskTool("analyzeData", ["data_analysis", "statistical"]);
  
  // Execute the task (internal handling of continuations)
  const result = await taskSystem.executeTask("process complex data", memorySystem);
  
  // TaskSystem automatically:
  // 1. Detects CONTINUATION status
  // 2. Executes the subtask
  // 3. Adds result as tool response to parent's session
  // 4. Continues parent execution
  
  console.log("Final result:", result.content);
}

// Example of what happens internally in taskSystem.executeTask
async function internalTaskSystemFlow(task, context) {
  // Get or create a Handler
  const handler = this.getHandlerForTask(task);
  
  // Execute the initial prompt
  const result = await handler.executePrompt(task.taskPrompt);
  
  // Check for continuation
  if (result.status === "CONTINUATION" && result.notes?.subtask_request) {
    // Execute the subtask
    const subtaskResult = await this.executeSubtask(result.notes.subtask_request);
    
    // Add subtask result as a tool response to parent's session
    handler.addToolResponse(
      this.getToolNameFromRequest(result.notes.subtask_request),
      subtaskResult.content
    );
    
    // Continue parent execution
    return handler.executePrompt("Continue based on the tool results.");
  }
  
  return result;
}

### Example with Explicit File Paths

```typescript
// Subtask with explicit files
return {
  status: "CONTINUATION",
  notes: {
    subtask_request: {
      type: "atomic",
      description: "Analyze code modules",
      inputs: { analysis_depth: "detailed" },
      context_management: { inherit_context: "subset" },
      file_paths: ["/src/main.py", "/src/utils.py"]
    }
  }
};
```
```

### Error Handling Example

```typescript
// Simplified error handling example
try {
  const result = await taskSystem.executeTask("analyze complex document");
  console.log("Analysis complete:", result.content);
} catch (error) {
  // Standard error types without complex partial results
  if (error.type === 'RESOURCE_EXHAUSTION') {
    console.log(`Resource limit exceeded: ${error.resource}`);
  } else if (error.type === 'TASK_FAILURE') {
    console.log(`Task failed: ${error.message}`);
    
    // If this was a subtask failure, the error contains the original request
    if (error.reason === 'subtask_failure') {
      console.log(`Failed subtask: ${error.details.subtaskRequest.description}`);
    }
  }
}
```

### Context Integration Example

```xml
<!-- Parent task that spawns a subtask -->
<task type="atomic">
  <description>Analyze code repository structure</description>
  <context_management>
    <inherit_context>full</inherit_context>
    <fresh_context>enabled</fresh_context>
  </context_management>
  
  <!-- This task will return CONTINUATION status with a subtask_request -->
</task>

<!-- Example of how the subtask request would be structured -->
<!-- This is not XML that would be written directly, but represents
     the structure that would be in the subtask_request -->
<task type="atomic">
  <description>Analyze specific module dependencies</description>
  <context_management>
    <inherit_context>subset</inherit_context>
    <fresh_context>enabled</fresh_context>
  </context_management>
  <inputs>
    <input name="module_path" from="parent_analysis.target_module"/>
    <input name="depth" from="parent_analysis.analysis_depth"/>
  </inputs>
</task>
```

```typescript
// TypeScript implementation showing context flow
async function executeWithContextIntegration() {
  // Execute parent task
  const parentResult = await taskSystem.executeTask(
    "<task><description>Analyze code repository structure</description></task>",
    memorySystem
  );
  
  // Check for continuation with subtask request
  if (parentResult.status === "CONTINUATION" && 
      parentResult.notes.subtask_request) {
    
    const subtaskRequest = parentResult.notes.subtask_request;
    
    // Context management settings from request (or defaults)
    const contextSettings = subtaskRequest.context_management || {
      inherit_context: "subset",
      fresh_context: "enabled"
    };
    
    // Get appropriate context based on settings
    let subtaskContext;
    if (contextSettings.inherit_context === "full") {
      subtaskContext = parentContext;
    } else if (contextSettings.inherit_context === "subset") {
      // Get relevant subset via associative matching
      subtaskContext = await memorySystem.getRelevantContextFor({
        taskText: subtaskRequest.description,
        inheritedContext: parentContext
      });
    } else {
      // No inherited context
      subtaskContext = null;
    }
    
    // Execute subtask with appropriate context
    const subtaskResult = await taskSystem.executeSubtask(
      subtaskRequest,
      subtaskContext
    );
    
    // Resume parent task with subtask result
    const finalResult = await taskSystem.resumeTask(
      parentResult,
      { subtask_result: subtaskResult }
    );
    
    console.log("Final analysis:", finalResult.content);
  }
}
```

<!-- Example: Context Management Patterns -->

<!-- Pattern 1: Clear All Context (Fresh Start) -->
<task type="atomic">
    <description>Start with completely fresh context</description>
    <context_management>
        <inherit_context>none</inherit_context>
        <accumulate_data>false</accumulate_data>
        <fresh_context>enabled</fresh_context>
    </context_management>
</task>

## Unified Tool Interface Examples

### Direct vs. Subtask Tool Examples

```typescript
// Example of Handler registering both tool types
handler.registerDirectTool("readFile", async (path) => {
  return await fs.readFile(path, 'utf8');
});

handler.registerSubtaskTool("analyzeData", ["data_analysis", "statistical"]);

// LLM usage looks similar for both
const fileContent = tools.readFile("data.csv");
const analysis = tools.analyzeData({ 
  data: fileContent,
  method: "statistical"
});

// But implementations differ:
// Direct tool implementation (in Handler)
async function executeReadFile(path) {
  return await fs.readFile(path, 'utf8');
}

// Subtask tool implementation (via CONTINUATION)
async function executeAnalyzeData(params) {
  return {
    status: "CONTINUATION",
    notes: {
      subtask_request: {
        type: "atomic",
        description: `Analyze data using ${params.method}`,
        inputs: params,
        template_hints: ["data_analysis"]
      }
    }
  };
}
```

<!-- Pattern 2: Rebuild Context While Preserving History -->
<task type="sequential">
    <description>Rebuild context while keeping step history</description>
    <context_management>
        <inherit_context>none</inherit_context>
        <accumulate_data>true</accumulate_data>
        <accumulation_format>notes_only</accumulation_format>
        <fresh_context>enabled</fresh_context>
    </context_management>
    <steps>
        <task>
            <description>First step</description>
        </task>
        <task>
            <description>Second step with fresh context but notes from first step</description>
        </task>
    </steps>
</task>

<!-- Pattern 3: Complete Context Preservation -->
<task type="sequential">
    <description>Preserve all context</description>
    <context_management>
        <inherit_context>full</inherit_context>
        <accumulate_data>true</accumulate_data>
        <accumulation_format>full_output</accumulation_format>
        <fresh_context>disabled</fresh_context>
    </context_management>
    <steps>
        <task>
            <description>First step</description>
        </task>
        <task>
            <description>Second step with all context preserved</description>
        </task>
    </steps>
</task>

```typescript
// Example of context pattern usage in TypeScript

// Pattern 1: Clear All Context (Fresh Start)
const clearContextResult = await taskSystem.executeTask(
  "<task type='atomic'><description>Fresh context task</description><context_management><inherit_context>none</inherit_context><accumulate_data>false</accumulate_data><fresh_context>enabled</fresh_context></context_management></task>",
  memorySystem
);

// Pattern 2: Rebuild Context While Preserving History
const rebuildWithHistoryResult = await taskSystem.executeTask(
  "<task type='sequential'><description>Sequential with history</description><context_management><inherit_context>none</inherit_context><accumulate_data>true</accumulate_data><fresh_context>enabled</fresh_context></context_management></task>",
  memorySystem
);

// Pattern 3: Complete Context Preservation
const preserveAllContextResult = await taskSystem.executeTask(
  "<task type='sequential'><description>Preserve all context</description><context_management><inherit_context>full</inherit_context><accumulate_data>true</accumulate_data><fresh_context>disabled</fresh_context></context_management></task>",
  memorySystem
);
```

## File Paths Examples

### Atomic Task with File Paths

```xml
<!-- Example: Atomic Task with File Paths -->
<task type="atomic">
    <description>Analyze the main source files</description>
    <context_management>
        <inherit_context>none</inherit_context>
        <fresh_context>disabled</fresh_context>
    </context_management>
    <file_paths>
        <path>./src/main.py</path>
        <path>./src/utils.py</path>
    </file_paths>
</task>
```

### TypeScript Usage Example

```typescript
// Execute atomic task with specific file paths
const result = await taskSystem.executeTask(
  "<task type='atomic'><description>Analyze source files</description><file_paths><path>./src/main.py</path><path>./src/utils.py</path></file_paths></task>",
  memorySystem
);

// Create subtask request with file paths
return {
  status: "CONTINUATION",
  notes: {
    subtask_request: {
      type: "atomic",
      description: "Analyze specific modules",
      inputs: { level: "detailed" },
      context_management: { inherit_context: "subset" },
      file_paths: ["./src/main.py", "./src/utils.py"]
    }
  }
};
```

### Combining File Paths with Context Management

```xml
<!-- Example: Combining File Paths with fresh_context -->
<task type="atomic">
    <description>Analyze code with base context and specific files</description>
    <context_management>
        <inherit_context>none</inherit_context>
        <fresh_context>enabled</fresh_context>
    </context_management>
    <file_paths>
        <path>./src/main.py</path>
        <path>./src/utils.py</path>
    </file_paths>
</task>
```

This combination:
1. Doesn't inherit any parent context
2. Explicitly includes the specified files
3. Uses associative matching to find additional relevant context
</file>
<file path="./components/task-system/impl/xml-processing.md" project="">
# XML Processing Implementation

> **Overview and References:** This document focuses on the Task System's XML processing implementation. For the complete XML schema definition and template guidelines, please refer to [system/contracts/protocols.md](../system/contracts/protocols.md).

## Schema Validation

### Core Schema Requirements
- Based on [Contract:Tasks:TemplateSchema:1.0]
- Required elements:
  * `instructions` (maps to taskPrompt)
  * `system` (maps to systemPrompt)
  * `model` (maps to model)
- Optional elements:
  * `inputs` with named input definitions
  * `manual_xml` flag
  * `disable_reparsing` flag

### Validation Rules
- All required fields must be present
- Input names must be unique
- Boolean fields must be "true" or "false"
- Model must be a valid LLM identifier

## Template Processing

### Template Validation
- Schema conformance checking
- Required field validation
- Type checking for known fields
- Warning generation for non-critical issues

### Manual XML Tasks
- Direct structure usage without reparsing
- Schema validation still applies
- Support for disable_reparsing flag
- No automatic restructuring

### Function Template Processing

#### Template Parsing
- When a `<template>` element is encountered, it's parsed into a TemplateNode
- The `name` attribute becomes the template name
- The `params` attribute is split into individual parameter names
- The body task is parsed recursively
- The template is automatically registered in the TaskLibrary

#### Function Call Parsing
- When a `<call>` element is encountered, it's parsed into a FunctionCallNode
- The `template` element value becomes the templateName
- Each `<arg>` child element is parsed into an ArgumentNode
- String values in arguments are evaluated to check if they represent variables

#### Template Validation
- Template names must be unique within the TaskLibrary
- Parameter lists must use valid identifiers
- Templates must have a valid body task
- Function calls must reference existing templates
- Argument counts must match parameter counts

## Output Processing

### XML Generation
```xml
<!-- Example Output Structure -->
<task type="sequential">
    <description>Task description</description>
    <steps>
        <task>
            <description>Step description</description>
            <inputs>
                <input name="input_name">
                    <task>
                        <description>Input task</description>
                    </task>
                </input>
            </inputs>
        </task>
    </steps>
</task>
```

### Output Validation
- Structure validation against schema
- Required field presence checking
- Type validation for known fields
- XML well-formedness checking

### Output Format Validation
- Format specification via `<output_format>` element
- JSON detection process:
  * Attempts to parse content as JSON
  * Validates parsed content against schema attribute
  * Returns original content if parsing fails
- Type validation against schema attribute:
  * "object" - Validates as JavaScript object
  * "array" or "[]" - Validates as array
  * "string[]" - Validates as array of strings
  * "number" - Validates as numeric value
  * "boolean" - Validates as boolean value
- Error handling for format violations:
  * Generates TASK_FAILURE with reason "output_format_failure"
  * Includes expected vs actual type information
  * Preserves original output in error details
- Template return type validation:
  * Function templates can specify return types
  * Return types are validated against actual output
  * Type mismatches generate validation errors

Example XML showing proper usage:
```xml
<task>
  <description>Get repository statistics</description>
  <output_format type="json" schema="object" />
</task>

<!-- With template return type -->
<template name="get_stats" params="repo_path" returns="object">
  <task>
    <description>Get statistics for {{repo_path}}</description>
    <output_format type="json" schema="object" />
  </task>
</template>
```

### Fallback Behavior
- Return unstructured string on parse failure
- Collect and surface warnings
- Maintain original content
- Include parsing error details

## Error Handling

### Validation Errors
```typescript
interface XMLError {
  type: 'XML_PARSE_ERROR' | 'VALIDATION_ERROR';
  message: string;
  location?: string;
  violations?: string[];
}
```

### Recovery Strategies
- Attempt partial content recovery
- Generate fallback string output
- Preserve original content
- Surface all validation issues

## Integration Points

### Template Management
- Load and validate schemas
- Process template definitions
- Handle manual XML flags
- Track template versions

### Task Execution
- Validate output structure
- Handle parsing failures
- Surface warnings appropriately
- Maintain execution context
</file>
<file path="./components/task-system/impl/resource-management.md" project="">
# Resource Management Implementation

> **Further Reading:** For an architectural overview of resource management principles, see [system/architecture/patterns/resource-management.md](../system/architecture/patterns/resource-management.md).

## Core Principles
- No resource usage prediction
- No task decomposition optimization
- Handler-based resource tracking
- Clear resource ownership boundaries

## Turn Counter Management

### Implementation Details
- Per-Handler turn tracking
- Atomic increment operations
- Strict limit enforcement
- No cross-Handler pooling

### Turn Management Rules
- Turn tracking owned by Handler instance
- Turn limit passed during Handler initialization
- Interactive sessions count against turn limits

### Usage Tracking
```typescript
// Using canonical ResourceMetrics definition from spec/types.md
// See [Type:ResourceMetrics:1.0]

class TurnCounter {
  private metrics: ResourceMetrics['turns'];
  
  increment(): void {
    if (this.metrics.used >= this.metrics.limit) {
      throw new ResourceExhaustionError('turns');
    }
    this.metrics.used++;
    this.metrics.lastTurnAt = new Date();
  }
}
```

### Configuration
- Default turn limits from config
- Per-task limit overrides
- Warning at 80% threshold
- Error at limit reached

## Context Window Management

### Size Tracking
- Token-based calculation
- Window size monitoring
- Fraction-based limits
- No content optimization

### Implementation
```typescript
// Using canonical ResourceMetrics definition from spec/types.md
// See [Type:ResourceMetrics:1.0]

class ContextManager {
  private metrics: ResourceMetrics['context'];
  
  addContent(content: string): void {
    const tokens = this.countTokens(content);
    if (this.metrics.used + tokens > this.metrics.limit) {
      throw new ResourceExhaustionError('context');
    }
    this.metrics.used += tokens;
    this.metrics.peakUsage = Math.max(this.metrics.peakUsage, this.metrics.used);
  }
}
```

### Monitoring
- Continuous token tracking
- Peak usage recording
- Threshold warnings
- Limit enforcement

## Resource Cleanup

### Handler Termination
- Clean session shutdown
- Resource accounting completion
- Metric collection
- No state preservation

### Memory Management
- Context cleanup
- Reference clearing
- Memory release
- State invalidation

## Integration Points

### Handler Integration
- Resource initialization
- Usage monitoring
- Limit enforcement
- Cleanup coordination

/**
 * Implementation of session-based resource management
 */
class HandlerSession {
  private systemPrompt: string;
  private messages: Message[] = [];
  private turnCounter: TurnCounter;
  private contextManager: ContextManager;
  private config: HandlerConfig;
  
  constructor(config: HandlerConfig) {
    this.config = config;
    this.systemPrompt = config.systemPrompt;
    this.turnCounter = new TurnCounter({
      limit: config.maxTurns,
      used: 0,
      lastTurnAt: new Date()
    });
    this.contextManager = new ContextManager({
      limit: Math.floor(config.maxContextWindowFraction * this.getModelMaxTokens(config.defaultModel)),
      used: 0,
      peakUsage: 0
    });
  }
  
  addUserMessage(content: string): void {
    this.messages.push({ 
      role: "user", 
      content, 
      timestamp: new Date() 
    });
    this.contextManager.addContent(content);
    // No turn increment for user messages
  }
  
  addAssistantMessage(content: string): void {
    this.messages.push({ 
      role: "assistant", 
      content, 
      timestamp: new Date() 
    });
    this.contextManager.addContent(content);
    this.turnCounter.increment(); // Increment turn counter for assistant responses
  }
  
  /**
   * Constructs a payload for the LLM using fully resolved content
   * @param task The resolved task template with all variables already substituted
   * @returns A complete HandlerPayload ready for LLM submission
   */
  constructPayload(task: TaskTemplate): HandlerPayload {
    // Note: All template variables should already be resolved by the Evaluator
    return {
      systemPrompt: this.systemPrompt,
      messages: [...this.messages, { 
        role: "user", 
        content: task.taskPrompt, // Already fully resolved
        timestamp: new Date()
      }],
      context: this.contextManager.getCurrentContext(),
      tools: this.getAvailableTools(),
      metadata: {
        model: this.config.defaultModel,
        resourceUsage: this.getResourceMetrics()
      }
    };
  }
  
  // resolveTemplatePlaceholders method has been removed
  // Template substitution is now an Evaluator responsibility
  
  getResourceMetrics(): ResourceMetrics {
    return {
      turns: this.turnCounter.getMetrics(),
      context: this.contextManager.getMetrics()
    };
  }
  
  private getModelMaxTokens(model: string): number {
    // Return model-specific token limits
    // Implementation details omitted
    const modelTokenLimits = {
      "claude-3-opus": 200000,
      "claude-3-sonnet": 180000,
      "claude-3-haiku": 150000,
      "gpt-4": 128000,
      "gpt-4-turbo": 128000,
      "gpt-3.5-turbo": 16000
    };
    
    return modelTokenLimits[model] || 100000; // Default fallback
  }
  
  private getAvailableTools(): ToolDefinition[] {
    // Return registered tools
    // Implementation details omitted
    return []; // Placeholder - actual implementation would return registered tools
  }
}

### Memory System
- Read-only metadata access
- Global index management
- Associative matching services
- No file operations or content storage
- Clear interface boundaries with Handler tools

### Error Handling
- Resource exhaustion detection
- Clean termination
- Metric reporting
- State cleanup
</file>
<file path="./components/task-system/impl/design.md" project="">
# Implementation Design

## Terminology and References

 - **Handler** and **Evaluator** definitions are standardized in [spec/types.md](../spec/types.md).
 - XML schema definitions are available in [system/contracts/protocols.md](../system/contracts/protocols.md).
 - For detailed resource tracking implementation (including turn counter and context window monitoring), see [resource-management.md](./resource-management.md).
 - For XML processing details (parsing, validation, and fallback behavior), refer to [xml-processing.md](./xml-processing.md).

## Handler Implementation

### Session Management Strategy
- Handler creates a HandlerSession for each task execution
- Session maintains complete conversation state and message history
- Provider-agnostic HandlerPayload structure for LLM interactions
- Clean session termination on completion
- Tool-based approach for user input requests
  
### Resource Tracking Implementation
- Turn counter integrated with HandlerSession
- Turns incremented only for assistant messages
- Context window tracks all messages and context
- Token usage monitored across full conversation
- Resource metrics available via session.getResourceMetrics()
- Limits enforced during session operations

### Payload Construction
- HandlerPayload provides unified structure for LLM requests
- Includes: systemPrompt, messages, context, tools, metadata
- Provider-specific adapters transform to appropriate format
- Session constructs payload via constructPayload() method
- Full conversation history included in structured format

### Error Propagation Design
- Standard error type system
- Immediate propagation on detection
- Clean resource release
- No retry attempt handling
- Complete error context preservation

### Interactive Session Support
- Input detection capabilities
- Agent-controlled input requests
- Resource tracking during interaction
- Input timeout handling
- Cancellation support

## Template Management
### Storage Implementation
- XML file-based storage
- Disk-based persistence
- Directory organization by type
- Template versioning support
- Schema validation enforcement
  
### Validation Implementation
- Basic XML structure validation
- Schema conformance checking
- Warning generation for issues
- Template field validation
- Model availability checking
  
### Matching Algorithm Design
- Scoring based on prompt results
- Top-N candidate selection
- Separate matching for human input vs AST
- Score normalization
- Clear ordering requirements
  
### XML Processing Details
- Lenient parsing with fallback
- Warning collection
- Graceful degradation
- Partial parsing support
- Clear error locations

## Context Management Implementation

### Hybrid Configuration Approach

The Task System implements a hybrid configuration approach with operator-specific defaults and explicit overrides:

```typescript
// Default context management settings by operator type and subtype
const DEFAULT_CONTEXT_SETTINGS = {
  atomic: {
    standard: {
      inheritContext: 'full',
      accumulateData: false,
      accumulationFormat: 'notes_only',
      freshContext: 'disabled'
    },
    subtask: {
      inheritContext: 'none',
      accumulateData: false,
      accumulationFormat: 'notes_only',
      freshContext: 'enabled'
    },
    // Default to standard if no subtype specified
    default: {
      inheritContext: 'full',
      accumulateData: false,
      accumulationFormat: 'notes_only',
      freshContext: 'disabled'
    }
  },
  sequential: {
    inheritContext: 'full',
    accumulateData: true,
    accumulationFormat: 'notes_only',
    freshContext: 'disabled'
  },
  reduce: {
    inheritContext: 'none',
    accumulateData: true,
    accumulationFormat: 'notes_only',
    freshContext: 'enabled'
  },
  script: {
    inheritContext: 'full',
    accumulateData: false,
    accumulationFormat: 'notes_only',
    freshContext: 'disabled'
  },
  director_evaluator_loop: {
    inheritContext: 'none',
    accumulateData: true,
    accumulationFormat: 'notes_only',
    freshContext: 'enabled'
  }
};

// File Path Processing
function processFilePaths(task: TaskDefinition, context: TaskContext): Promise<TaskContext> {
  if (!task.file_paths || task.file_paths.length === 0) {
    return Promise.resolve(context);
  }
  
  return fetchAndAddFiles(task.file_paths, context);
}

async function fetchAndAddFiles(filePaths: string[], context: TaskContext): Promise<TaskContext> {
  const fileContents: Array<{path: string, content: string}> = [];
  const warnings: string[] = [];
  
  // Process each file path
  for (const path of filePaths) {
    try {
      // Resolve relative paths to absolute
      const absolutePath = resolvePath(path);
      
      // Fetch file content using Handler tools
      const content = await handler.tools.readFile(absolutePath);
      
      fileContents.push({ path: absolutePath, content });
    } catch (error) {
      warnings.push(`Warning: Failed to read file ${path}: ${error.message}`);
    }
  }
  
  // Format file contents with XML tags
  const formattedContent = fileContents.map(fc => 
    `<file path="${fc.path}">\n${fc.content}\n</file>`
  ).join('\n\n');
  
  // Add to context
  const enhancedContext = {
    ...context,
    fileContent: (context.fileContent || '') + formattedContent
  };
  
  // Store warnings
  if (warnings.length > 0) {
    enhancedContext.warnings = (enhancedContext.warnings || []).concat(warnings);
  }
  
  return enhancedContext;
}

function resolvePath(path: string): string {
  if (path.startsWith('/')) {
    // Already absolute
    return path;
  }
  
  // Resolve relative to repo root
  return joinPath(REPO_ROOT, path);
}

// Template processing with merged settings
function processTemplate(template) {
  const operatorType = template.type;
  
  // Get defaults based on type and subtype
  let defaults;
  if (operatorType === 'atomic' && template.subtype) {
    defaults = DEFAULT_CONTEXT_SETTINGS[operatorType][template.subtype] || 
               DEFAULT_CONTEXT_SETTINGS[operatorType].default;
  } else {
    defaults = DEFAULT_CONTEXT_SETTINGS[operatorType];
  }
  
  // The Evaluator stores the TaskResult's notes field (and optionally content) based on the task's accumulation_format setting.
  // No additional summary generation is performed - the notes field as generated by the LLM is preserved directly.
  
  // Special case: For subtask requests via CONTINUATION, use subtask defaults
  if (operatorType === 'atomic' && template.source === 'continuation') {
    defaults = DEFAULT_CONTEXT_SETTINGS.atomic.subtask;
  }
  
  // If context_management is present, merge with defaults
  if (template.contextManagement) {
    const mergedSettings = {
      ...defaults,
      ...template.contextManagement
    };
    
    // Validate mutual exclusivity constraint
    if ((mergedSettings.inheritContext === 'full' || mergedSettings.inheritContext === 'subset') 
        && mergedSettings.freshContext === 'enabled') {
      throw new Error('Invalid context management configuration: fresh_context="enabled" cannot be combined with inherit_context="full" or inherit_context="subset"');
    }
    
    return mergedSettings;
  }
  
  // Otherwise use defaults
  return defaults;
}

// Context Constraint Validation
function validateContextSettings(settings) {
  // Check for mutual exclusivity violation
  if ((settings.inheritContext === 'full' || settings.inheritContext === 'subset') 
      && settings.freshContext === 'enabled') {
    return {
      valid: false,
      error: 'Context constraint violation: fresh_context="enabled" cannot be combined with inherit_context="full" or inherit_context="subset"'
    };
  }
  
  // Check for empty context warning
  if (settings.inheritContext === 'none' && !settings.accumulateData && settings.freshContext === 'disabled') {
    return {
      valid: true,
      warning: 'Warning: Task will execute with minimal context (no inheritance, no accumulation, no fresh context)'
    };
  }
  
  return { valid: true };
}

async function getContextForTask(task: TaskDefinition, parentContext: ExecutionContext): Promise<ExecutionContext> {
  // Start with base context according to inherit_context setting
  let context = getBaseContext(task.context_management?.inherit_context, parentContext);
  
  // If file_paths is specified, fetch those files
  if (task.file_paths && task.file_paths.length > 0) {
    context = await fetchAndAddFiles(task.file_paths, context);
  }
  
  // If fresh_context is enabled, still do associative matching
  if (task.context_management?.fresh_context === 'enabled') {
    const freshContext = await memorySystem.getRelevantContextFor({
      taskText: task.description,
      inheritedContext: context.inherit_context === 'none' ? undefined : context
    });
    
    // Combine fresh context with existing context
    context = combineContexts(context, freshContext);
  }
  
  return context;
}
```

During task execution, the final merged configuration is passed to the Evaluator, which applies the settings accordingly.

## Task Template Matching

Template matching is a selection process that occurs before and separate from execution:

- **Selection Process**: Matches natural language task descriptions to appropriate atomic task templates
- **Scoring Mechanism**: Uses associative matching to compute similarity scores
- **Context Awareness**: May use task context to improve matching accuracy
- **No Execution Connection**: Completely separate from execution environment or variable binding
- **Scope Limitation**: Applies only to atomic task templates, not composite task templates

Template matching exclusively answers "which atomic task template should handle this task?" and has no role in variable resolution or execution. While the TaskLibrary can store templates for any task type (atomic, sequential, reduce, etc.), only atomic task templates participate in the template matching process.

For further details on context handling and related design decisions, see [ADR 002 - Context Management](../../system/architecture/decisions/002-context-management.md), [ADR 005 - Context Handling](../../system/architecture/decisions/005-context-handling.md), and [ADR 14 - Operator Context Configuration](../../system/architecture/decisions/14-operator-ctx-config.md).

#### Matching Call Chain

```mermaid
flowchart TD
    A[User provides task description]
    B[Construct ContextGenerationInput]
    C{Disable Context?}
    C -- Yes --> D[Omit inheritedContext]
    C -- No --> E[Include parent context]
    D & E --> F[Call MemorySystem.getRelevantContextFor]
    F --> G[Receive AssociativeMatchResult]
    G --> H[Compute similarity scores for each candidate]
    H --> I[Select highest-scoring atomic task template]
    I --> J[Return template for execution]
```

### Context Constraint Validation

The system enforces a mutual exclusivity constraint between fresh context generation and context inheritance:

```typescript
function validateContextSettings(settings) {
  // Check for mutual exclusivity violation
  if ((settings.inheritContext === 'full' || settings.inheritContext === 'subset') 
      && settings.freshContext === 'enabled') {
    return {
      valid: false,
      error: 'Context constraint violation: fresh_context="enabled" cannot be combined with inherit_context="full" or inherit_context="subset"'
    };
  }
  
  // Check for empty context warning
  if (settings.inheritContext === 'none' && !settings.accumulateData && settings.freshContext === 'disabled') {
    return {
      valid: true,
      warning: 'Warning: Task will execute with minimal context (no inheritance, no accumulation, no fresh context)'
    };
  }
  
  return { valid: true };
}
```

## Resource Management

The Task System enforces resource limits via a per‑Handler turn counter and context window monitoring. For the complete low‑level implementation (including code examples and configuration details), please refer to [resource-management.md](./resource-management.md).
  
### Context Window Management
- Token counting approach
- Size limit enforcement
- No optimization strategy
- Window usage monitoring
- Clear limit boundaries
  
### Limit Enforcement Strategy
- Immediate termination on violation
- Resource exhaustion error generation
- Clean session cleanup
- Resource usage reporting
- Clear violation metrics
  
### Error Detection Mechanisms
- Resource limit monitoring, progress tracking, output and XML structure validation, and input validation.

## Variable and Parameter Management

The system maintains clear separation between two distinct mechanisms:

### 1. Lexical Environment
- **Purpose**: Maintains DSL variable bindings and scoping
- **Structure**: Chain of environments with parent references
- **Operations**: Variable lookups, environment extension
- **Scope**: Isolated between function calls

### 2. Direct Parameter Passing
- **Purpose**: Transfers data between caller and template
- **Mechanism**: Arguments evaluated in caller's scope, bound to parameters in template's scope
- **Execution**: Templates operate only on their parameters
- **Isolation**: No implicit access between scopes

This clean separation prevents unexpected variable leakage and ensures predictable execution.

## Template Substitution Process

The system implements a standardized template substitution process in the Evaluator component, which resolves `{{variable_name}}` placeholders before dispatching tasks to the Handler:

### 1. Function-Based Templates
- For templates with declared parameters (`<template name="example" params="param1,param2">`):
- Substitution is limited to only the declared parameters
- The template has no access to other variables in the parent scope
- Example: `<description>Process {{param1}} with {{param2}}</description>`

### 2. Standard Templates
- For templates without explicit parameter declarations:
- Substitution uses variables from the current lexical environment
- Variables are resolved through the Environment.find() method
- Example: `<description>Process {{data_file}} with {{options}}</description>`

### Implementation in Evaluator

The Evaluator has a formal substitution phase in its execution pipeline:

```typescript
// In Evaluator component
async function executeTask(task: Task, environment: Environment): Promise<TaskResult> {
  try {
    // Process and resolve all template variables
    const resolvedTask = resolveTemplateVariables(task, environment);
    
    // Execute task using Handler with fully resolved content
    const result = await handler.executePrompt(
      resolvedTask.systemPrompt,
      resolvedTask.taskPrompt
    );
    
    // Process and return results
    return processResult(result, task);
  } catch (error) {
    if (error.message.includes('Variable resolution error')) {
      return createTaskFailure(
        'template_resolution_failure',
        error.message,
        { task: task.description }
      );
    }
    throw error;
  }
}

// Template variable resolution
function resolveTemplateVariables(task: Task, env: Environment): Task {
  const resolvedTask = {...task};
  
  if (task.isFunctionTemplate && task.parameters) {
    // For function templates, create isolated environment with only parameters
    const funcEnv = new Environment({});
    for (const param of task.parameters) {
      funcEnv.bindings[param] = env.find(param);
    }
    resolvedTask.taskPrompt = substituteVariables(task.taskPrompt, funcEnv);
  } else {
    // For standard templates, use the full environment
    resolvedTask.taskPrompt = substituteVariables(task.taskPrompt, env);
  }
  
  return resolvedTask;
}
```

The Evaluator handles variable resolution before passing fully resolved content to the Handler. This ensures all placeholders are substituted prior to LLM execution, with appropriate error handling for missing variables.

For Director-Evaluator loops, parameters are passed explicitly:
```typescript
// Director-Evaluator Loop with Evaluator handling template substitution
async function executeDirectorEvaluatorLoop(task, inputs) {
  // In the Evaluator component:
  
  // 1. Prepare director inputs
  const directorInputs = {
    ...inputs,
    feedback: previousEvaluation?.feedback,
    current_iteration: currentIteration
  };
  
  // 2. Resolve all template variables in the director task
  const resolvedDirectorTask = resolveTemplateVariables(task.director, directorInputs);
  
  // 3. Execute director with fully resolved content
  const directorOutput = await handler.executePrompt(
    resolvedDirectorTask.systemPrompt,
    resolvedDirectorTask.taskPrompt
  );
  
  // 4. Prepare evaluator inputs
  const evaluatorInputs = {
    solution: directorOutput.content,
    original_prompt: inputs.original_prompt
  };
  
  // 5. Resolve all template variables in the evaluator task
  const resolvedEvaluatorTask = resolveTemplateVariables(task.evaluator, evaluatorInputs);
  
  // 6. Execute evaluator with fully resolved content
  const evaluationResult = await handler.executePrompt(
    resolvedEvaluatorTask.systemPrompt,
    resolvedEvaluatorTask.taskPrompt
  );
  
  // Resume loop with new parameters
  return continueExecution(task, {
    ...inputs,
    director_result: directorOutput,
    evaluation_result: evaluationResult
  });
}
```

## Subtask Spawning Implementation

The Task System implements a standardized subtask spawning mechanism that enables dynamic task creation and composition.

### Request Structure

```typescript
interface SubtaskRequest {
  // Required fields
  type: TaskType;                      // Type of subtask to spawn
  description: string;                 // Description of the subtask
  inputs: Record<string, any>;         // Input parameters for the subtask
  
  // Optional fields
  template_hints?: string[];           // Hints for template selection
  context_management?: {               // Override default context settings
    inherit_context?: 'full' | 'none' | 'subset';
    accumulate_data?: boolean;
    accumulation_format?: 'notes_only' | 'full_output';
    fresh_context?: 'enabled' | 'disabled';
  };
  max_depth?: number;                  // Override default max nesting depth
  subtype?: string;                    // Optional subtype for atomic tasks
}
```

### Execution Flow

The subtask spawning process follows four main steps:

1. **Validation**
   - Validates the SubtaskRequest structure
   - Checks nesting depth against maximum allowed
   - Performs cycle detection to prevent recursive spawning
   - Validates input parameters

2. **Template Matching**
   - Uses the description and template_hints for associative matching
   - Selects the highest-scoring template that matches the request
   - Falls back to default templates if no specific match is found

3. **Subtask Creation**
   - Creates a new execution environment with direct parameter passing
   - Applies context management settings (defaults or overrides)
   - Prepares resource tracking linked to the parent task

4. **Execution and Result Handling**
   - Executes the subtask with appropriate resource limits
   - Passes the complete TaskResult back to the parent task
   - Handles errors with standardized error structures
   - Ensures proper cleanup of resources

### Depth Control Implementation

To prevent infinite recursion and resource exhaustion, the system implements depth control:

```typescript
async function executeTaskWithDepthControl(
  request: SubtaskRequest, 
  parentContext: ExecutionContext,
  currentDepth: number = 0
): Promise<TaskResult> {
  // Check maximum nesting depth
  const maxDepth = request.max_depth ?? DEFAULT_MAX_NESTING_DEPTH;
  if (currentDepth >= maxDepth) {
    throw new Error({
      type: 'TASK_FAILURE',
      reason: 'execution_halted',
      message: `Maximum nesting depth (${maxDepth}) exceeded`
    });
  }
  
  // Perform cycle detection
  if (detectCycle(request, parentContext.executionPath)) {
    throw new Error({
      type: 'TASK_FAILURE',
      reason: 'execution_halted',
      message: 'Cycle detected in subtask spawning'
    });
  }
  
  // Get context based on settings, with support for explicit file paths
  const contextSettings = getContextSettings(request);
  if (contextSettings.inheritContext === 'subset' && 
      request.file_paths && request.file_paths.length > 0) {
    // Use explicit file paths when provided
    parentContext.context = await getContextForFiles(request.file_paths, parentContext);
  }
  
  // Execute subtask with incremented depth
  try {
    return await executeTask(request, {
      ...parentContext,
      nestingDepth: currentDepth + 1,
      executionPath: [...parentContext.executionPath, getTaskSignature(request)]
    });
  } catch (error) {
    // Wrap error in standardized subtask failure structure
    throw {
      type: 'TASK_FAILURE',
      reason: 'subtask_failure',
      message: `Subtask "${request.description}" failed`,
      details: {
        subtaskRequest: request,
        subtaskError: error,
        nestingDepth: currentDepth + 1,
        partialOutput: error.details?.partialOutput
      }
    };
  }
}
```

### Context Management Integration

Subtasks use a hybrid configuration approach with default settings that can be overridden:

```typescript
// Default context management settings for subtasks
const DEFAULT_SUBTASK_CONTEXT_SETTINGS = {
  inherit_context: 'subset',
  accumulate_data: false,
  accumulation_format: 'notes_only',
  fresh_context: 'enabled'
};

/**
 * Retrieves context for specific file paths
 * @param filePaths Array of file paths to include in context
 * @param parentContext Parent execution context
 * @returns Context containing only the specified files
 */
async function getContextForFiles(
  filePaths: string[], 
  parentContext: ExecutionContext
): Promise<ExecutionContext> {
  // Filter the parent context to only include the specified files
  // This implementation assumes the memory system can retrieve specific files
  const fileContexts = await Promise.all(
    filePaths.map(path => memorySystem.getFileContext(path))
  );
  
  // Combine the file contexts into a single context
  return {
    ...parentContext,
    files: fileContexts.filter(Boolean) // Remove any null/undefined entries
  };
}

function getContextSettings(request: SubtaskRequest): ContextSettings {
  // If context_management is provided in the request, merge with defaults
  if (request.context_management) {
    return {
      ...DEFAULT_SUBTASK_CONTEXT_SETTINGS,
      ...request.context_management
    };
  }
  
  // Otherwise use defaults
  return DEFAULT_SUBTASK_CONTEXT_SETTINGS;
}
```

### Parent-Child Communication

When a parent task spawns a subtask, the communication follows this pattern:

1. Parent task returns a CONTINUATION status with subtask_request in notes
2. Task System validates the request and spawns the subtask
3. Subtask executes with its own resource tracking
4. Subtask result is passed back to the parent task when it resumes
5. Parent task continues execution with the subtask result available as a parameter

This approach ensures clear data flow and explicit dependencies between parent and child tasks.

For historical context and decision rationale behind this implementation approach, see [ADR 11: Subtask Spawning Mechanism].

### Script Execution Implementation
The system now supports executing external scripts as part of a static director-evaluator workflow. When a script_execution element is specified:

1. The script receives the Director's output as direct input
2. Script execution captures stdout, stderr, and exit code
3. These outputs are passed as direct parameters to the Evaluator
4. No environment variables are used in this data flow

This design ensures that the director's output flows seamlessly through the script execution step before final evaluation, using explicit parameter passing throughout.

## Integration Points
### Memory System Interaction

#### Clear Responsibility Boundaries
- **Memory System** (read-only metadata):
  - Maintains metadata index (file paths and descriptions)
  - Provides associative matching based on metadata
  - Never performs file I/O operations
  - Does not store or parse file contents
  - Follows read-only context model (no updateContext capability)

- **Handler** (all file operations):
  - Reads/writes files when needed
  - For Anthropic models: Configures computer use tools (optional)
  - For other models: Uses appropriate file access mechanisms
  - Handles all file system interaction
  
### Compiler Integration
- Task parsing services
- XML validation
- Schema conformance
- Error surfacing
- Validation feedback
  
### Evaluator Support
- Error surfacing
- Reparse template support
- No retry management
- State preservation
- Recovery guidance
  
### LLM Session Management
- Handler encapsulation
- Resource tracking
- Model selection support
- Clean termination
- Session isolation
</file>
<file path="./components/task-system/spec/requirements.md" project="">
# Task System Requirements

This document outlines the high-level functional and non‑functional requirements for the Task System.

## Key Requirements Summary

1. **XML-based Task Templates:**  
   - All task templates must conform to [Contract:Tasks:TemplateSchema:1.0].  
   - Both LLM‑generated and manual XML structures are supported, with options to disable reparsing.

2. **Handler and Resource Management:**  
   - One Handler is created per task execution with immutable configuration and strict resource tracking.
   - Resource limits (turn counts, context window) are enforced by the Handler. For details, see [Pattern:ResourceManagement:1.0].

3. **Task Matching:**  
   - The system matches natural language inputs and AST nodes to candidate task templates (up to 5 candidates) using numeric scoring.

4. **Memory Integration:**  
   - The Task System uses a read‑only Memory System interface for context retrieval. See [Interface:Memory:3.0].

5. **Script Execution Support:**  
   - XML schema extensions support specifying external command details for script tasks. Clear input/output contracts must be defined.

For detailed operational behavior, see the Behaviors document. Additional system‑level details are available in the central contracts and architecture documents.

## See Also

 - [Task System Behaviors](behaviors.md)
 - [Task System Interfaces](interfaces.md)
 - System-level contracts: [Contract:Tasks:TemplateSchema:1.0] and [Pattern:ResourceManagement:1.0]
</file>
<file path="./components/task-system/spec/behaviors.md" project="">
# Task System Behaviors

This document describes the runtime behaviors of the Task System.

## Overview

The Task System is responsible for managing LLM task execution, including:
 - Template matching and management,
 - Lifecycle handling for Handlers,
 - XML processing with graceful degradation, and
 - Error detection and recovery.

## Key Behavior Highlights

1. **Template Management:**  
   - Task templates are stored and validated against the XML schema defined in [Contract:Tasks:TemplateSchema:1.0].  
   - The system matches both natural language inputs and AST nodes to candidate templates (up to 5 candidates), using numeric scoring.

2. **Template Variable Substitution:**
   - The Evaluator is solely responsible for all template variable substitution.
   - This includes resolving all {{variable_name}} placeholders before passing tasks to Handlers.
   - Different resolution rules apply for function templates vs. standard templates.
   - Variable resolution errors are detected early and handled at the Evaluator level.

3. **Handler Lifecycle:**  
   - A new Handler is created for each task execution with an immutable configuration.
   - The Handler enforces resource limits (turn counts, context window limits) as described in [Pattern:ResourceManagement:1.0].
   - Handlers receive fully resolved content with no remaining template variables.

4. **XML Processing:**  
   - Basic structural validation is performed, with warnings generated for non‑critical issues.
   - In cases of partial XML parsing failure, the original content is preserved and error details are included in the task notes.

5. **Error Handling:**  
   - Errors such as RESOURCE_EXHAUSTION, TASK_FAILURE, and template resolution failures are surfaced according to the rules defined in [Pattern:Error:1.0].
   - The Evaluator delegates error recovery (e.g., reparse tasks) and includes partial outputs when available.
   - Template variable resolution errors are handled specifically with `template_resolution_failure` reason codes.

## See Also

 - [Task System Requirements](requirements.md)
 - [Task System Interfaces](interfaces.md)
 - System-level error handling: [Pattern:Error:1.0]

### Interactive Sessions
- Handler implements a standardized tool-based approach for user input requests
- The system registers a `requestUserInput` tool during Handler initialization
- Input Flow:
  ```
  LLM -> Calls requestUserInput tool -> Handler detects tool call ->
  onRequestInput called with prompt -> User input returned ->
  Handler adds user message to session -> Conversation continues
  ```
- Sessions track all conversation turns and message history
- User messages are added to the session but don't increment turn counters
- Assistant messages increment turn counters and are tracked for resource limits
- Context window includes full conversation history managed by HandlerSession
- HandlerPayload includes all context, messages, and available tools

### XML Processing
- Basic structural validation only
- Warning generation for validation issues
- Lenient output parsing with fallback
- Graceful handling of partially valid XML
- Support for manual XML task flag

## Task Execution

### Standard Task Execution
- Process tasks using appropriate templates
- Return both output and notes sections in TaskResult structure
- Surface errors for task failure or resource exhaustion
- Support specialized execution paths for reparsing and memory tasks
- Implement lenient XML output parsing with fallback to single string
- Generate warnings for malformed XML without blocking execution
- Include 'data usage' section in task notes as specified by system prompt

### Output Format Handling
- Format declaration via `<output_format>` element with required `type` attribute
- Supported format types:
  * "json" - Structured JSON data
  * "text" - Plain text (default)
- Optional `schema` attribute for type validation:
  * "object" - JSON object
  * "array" or "[]" - JSON array
  * "string[]" - Array of strings
  * "number" - Numeric value
  * "boolean" - Boolean value
- Automatic JSON detection:
  * Attempts to parse content as JSON when type="json"
  * Adds parsed content to TaskResult as parsedContent property
  * Falls back to original string content if parsing fails
- Type validation process:
  * Validates parsed content against schema attribute
  * Generates error if type mismatch occurs
  * Preserves original content in error details
- Template return type validation:
  * Function templates can specify return types via returns attribute
  * Return types are validated against actual output
  * Type mismatches generate validation errors

Example error structure for output_format_failure:
```typescript
{
  type: 'TASK_FAILURE',
  reason: 'output_format_failure',
  message: 'Expected output of type "array" but got "object"',
  details: {
    expectedType: "array",
    actualType: "object",
    content: "..." // The original output is now in content field
  }
}
```

## Output Structure

The Task System uses a simplified output structure:

### Atomic Tasks
- `content`: Contains all output (complete or partial)
- `notes`: Contains only metadata (never content)
- `status` indicates completion:
  * `COMPLETE`: Content is final
  * `FAILED`: Content may be partial
  * `CONTINUATION`: Content is intermediate

### Sequential Tasks
- Structure preserves step outputs with proper separation:
  ```typescript
  partialResults: [
    {
      stepIndex: number,
      content: string,      // Step output
      metadata: {           // Step metadata
        status: string,
        [key: string]: any
      }
    }
  ]
  ```

### Reduce Tasks
- Structure preserves processed inputs with proper separation:
  ```typescript
  processedResults: [
    {
      inputIndex: number,
      content: string,      // Processing output
      metadata: {           // Processing metadata
        status: string,
        [key: string]: any
      }
    }
  ]
  ```

This structure maintains clear separation between content and metadata at all levels.

### Task Template Matching

#### Human Input Matching
- Input: Natural language text + initial environment
- Process:
  - Uses matching specialized task from library
  - Considers provided files in environment
  - No access to previous executions/history
- Output: 
  - Up to 5 highest-scoring templates
  - Each match includes numeric score and template
  - Templates ordered by descending score

#### AST Node Matching
- Input: Single AST node
- Process:
  - Uses node-specific matching task
  - Only considers node-level context
  - No traversal of parent/child nodes
- Output:
  - Up to 5 highest-scoring templates
  - Each match includes numeric score and template
  - Templates ordered by descending score

**Additional Matching Details:**
- **Heuristic Matching:** The matching is performed heuristically using user-defined associative matching tasks. These tasks compute similarity scores based on fixed input/output conventions without a fixed system-wide metric.
- **Disable Context Flag:** Atomic tasks have an optional "disable context" flag available in their ContextGenerationInput. When enabled, inherited context is omitted from the matching process.
- **Optional Success Score:** Task execution results may include an optional success score in the `notes` field. This score is used for future adaptive template matching (although adaptive behavior is not part of the MVP).

#### Matching Behavior Summary

```mermaid
flowchart TD
    A[User Input: Task Description]
    B[Create ContextGenerationInput]
    C{Disable Context Flag?}
    C -- Yes --> D[Omit inherited context]
    C -- No --> E[Include inherited context]
    D & E --> F[Invoke Associative Matching Task]
    F --> G[Compute Similarity Scores]
    G --> H[Select Highest-Scoring Atomic Template]
    H --> I[Return Selected Template]
```

### Specialized Task Types

#### Reparse Tasks
- Triggered by:
  - Resource exhaustion errors
  - Invalid output errors
  - Progress failure errors
- Uses specialized templates from separate directory
- Returns new task structure or alternative approach

#### Memory Tasks
- Direct memory system access
- Uses associative matching templates
- Returns relevant context selections

## Memory System Integration

### File Operations
- **Memory System**: Manages ONLY metadata (file paths and descriptive strings)
- **Handler**: Performs ALL file I/O operations (reading, writing, deletion)
- For Anthropic models: Handler configures and uses Anthropic's computer use tools
  * computer_20250124 (or 20241022)
  * text_editor_20250124 (or 20241022)
  * bash_20250124 (or 20241022)
- All file content access is always handled by the Handler, never the Memory System

Note: The Memory System (version 3.0) follows a read-only context model with no updateContext capability.

### Context Management
- Context accessed via async getRelevantContextFor
- File metadata accessed via GlobalIndex
- Existing context preserved during task execution
- Structure/parsing handled by associative memory tasks
- Context clearing and regeneration handled through context_management settings
- Three-dimensional model (inherit_context, accumulate_data, fresh_context) provides complete control
- No additional flags or mechanisms needed for context operations

#### Context Operation Patterns

1. **Context Clearing**: Achieved by setting `inherit_context="none"` and `fresh_context="enabled"`
2. **Context Regeneration**: Achieved through `fresh_context="enabled"` with appropriate inheritance settings
3. **Complete Preservation**: Achieved with `inherit_context="full"`, `accumulate_data="true"`, and `fresh_context="disabled"`

### State Management
- No complex file metadata tracking
- Minimal state maintenance between tasks
- Clear task execution boundaries
- Simple file modification tracking

## Delegation Mechanisms

## Tool Interface
The LLM interacts with a unified tool system that presents consistent patterns regardless of implementation mechanism:

```typescript
// All tools appear similar to the LLM
tools.readFile("path/to/file");  // Direct implementation
tools.analyzeData({ data: content });  // Subtask implementation
```

### Direct Tool Implementation
- Handled by Handler component
- Deterministic with fixed APIs
- No continuation mechanism
- Used for file operations, APIs, script execution
- Examples: file access, bash commands, computer tools
- Direct execution without complex context management
- Resource tracking handled by Handler

### Subtask Tool Implementation
- LLM-to-LLM interactions
- Use CONTINUATION status with SubtaskRequest
- Full context management via Memory System
- Support complex reasoning and creative tasks
- Follow ADR 14 context management model
- Depth tracking to prevent infinite recursion
- Template selection via associative matching

See [Pattern:ToolInterface:1.0] for selection criteria and implementation details.

## File Operations

### Clear Responsibility Boundaries
- **Memory System**: Manages ONLY metadata (file paths and descriptive strings)
- **Handler**: Performs ALL file I/O operations (reading, writing, deletion)

### Implementation Details
- Handler configures and uses provider-appropriate tool mechanisms
- Tool selection is determined by provider capabilities and task requirements
- Each provider implementation handles the mapping of abstract tool types to provider-specific formats
- All file content access is always handled by the Handler, never the Memory System
- The Memory System provides only file paths that may be relevant (via associative matching)
- Context Management System decides which files to read based on policy

## Subtask Spawning Behavior

The Task System implements a standardized approach to subtask spawning:

### Request Processing

1. **Validation and Preparation**
   - Validates the SubtaskRequest structure for required fields
   - Checks nesting depth against maximum allowed (default: 5)
   - Performs cycle detection to prevent recursive spawning
   - Prepares context according to context management settings
   - Prioritizes explicit file_paths when provided with inherit_context="subset"

2. **Template Selection and Execution**
   - Uses the description and template_hints for associative matching
   - Selects the highest-scoring template that matches the request
   - Creates a new execution environment with direct parameter passing
   - Executes the subtask with appropriate resource tracking

3. **Result Handling**
   - Adds subtask result as a tool response to parent's Handler session
   - Parent task continues execution with the tool result in its conversation history
   - No special resumption methods or partial results tracking needed
   - Handler session remains preserved throughout the subtask execution
   - From the LLM's perspective, subtasks appear as normal tool calls and responses

### Explicit File Selection

When creating subtasks, parent tasks can explicitly specify which files to include:

```typescript
// The file_paths field takes precedence over associative matching
subtask_request = {
  type: "atomic",
  description: "Analyze specific modules",
  inputs: { /* parameters */ },
  context_management: { inherit_context: "subset" },
  file_paths: ["/src/main.py", "/src/utils.py"]
}
```

### Context Management Defaults

Subtasks have specific default context management settings:

| Setting | Default Value | Description |
|---------|---------------|-------------|
| inherit_context | subset | Inherits only relevant context from parent |
| accumulate_data | false | Does not accumulate previous step outputs |
| accumulation_format | notes_only | Stores only summary information |
| fresh_context | enabled | Generates new context via associative matching |

These defaults can be overridden through explicit configuration in the SubtaskRequest.

### Error Handling

The system uses standard error types (`RESOURCE_EXHAUSTION`, `TASK_FAILURE`) without complex partial results tracking. Errors contain essential information needed for debugging without overengineering the error structure. The parent task receives clear error information and can implement appropriate recovery strategies based on error type and context.

When a subtask fails, a standardized error structure is generated:

```typescript
{
  type: 'TASK_FAILURE',
  reason: 'subtask_failure',
  message: 'Subtask execution failed',
  details: {
    subtaskRequest: {
      type: 'atomic',
      description: 'Process data',
      inputs: { /* original inputs */ }
    },
    subtaskError: {
      type: 'TASK_FAILURE',
      reason: 'execution_halted',
      message: 'Failed to process data'
    },
    nestingDepth: 2
  }
}
```

This structure provides the essential error context, allowing the parent task to implement recovery strategies if needed.

## Context Management Delegation

The Task System implements a hybrid configuration approach with operator-specific defaults and explicit overrides:

| Operator Type | inherit_context | accumulate_data | accumulation_format | fresh_context |
|---------------|-----------------|-----------------|---------------------|---------------|
| atomic        | full            | false           | notes_only          | enabled       |
| sequential    | full            | true            | notes_only          | enabled       |
| reduce        | none            | true            | notes_only          | enabled       |
| script        | full            | false           | notes_only          | disabled      |
| director_evaluator_loop | none  | true            | notes_only          | enabled       |

These defaults apply when no explicit context_management block is provided. When present, the explicit settings override the defaults:

```xml
<context_management>
    <inherit_context>full|none|subset</inherit_context>
    <accumulate_data>true|false</accumulate_data>
    <accumulation_format>notes_only|full_output</accumulation_format>
    <fresh_context>enabled|disabled</fresh_context>
</context_management>
```

The Task System delegates **all context management execution** to the Evaluator according to the final merged configuration. In other words:

1. The Task System's role is to:
   - Define task structure (sequential, map, reduce, etc.)
   - Process context management configuration (defaults + overrides)
   - Signal the Evaluator to execute steps with the final configuration

2. The Evaluator manages all three dimensions of context:
   - **inherit_context**: Controls parent context inheritance ("full", "none", or "subset").
   - **accumulate_data**: Controls accumulation of previous step outputs (true/false).
   - **accumulation_format**: Specifies storage format for accumulated data ("notes_only" or "full_output").
   - **fresh_context**: Controls whether new context is generated via associative matching ("enabled" or "disabled").

3. The Evaluator decides how and when to call `MemorySystem.getRelevantContextFor()` based on these settings.
4. The Handler remains focused on resource tracking (turns, tokens).
5. No direct context accumulation logic occurs in the Task System itself.

## File Paths Feature

The system supports an orthogonal file selection mechanism through the `file_paths` feature:

### Feature Behavior
- Available on all atomic tasks (both direct and subtasks)
- Operates outside the standard three-dimensional context model
- Specified files are always included in the context, regardless of other settings
- Files are fetched by the Handler before task execution

### Implementation Details
- Files are retrieved using Handler tools
- Both absolute paths and paths relative to repo root are supported
- Files are presented in the context with XML tags indicating their paths
- Invalid paths generate warnings but don't prevent task execution

### Integration with Context Management
- When used with `inherit_context="subset"`, specified files become the subset
- When used with `fresh_context="enabled"`, specified files are forcibly included while associative matching still runs
- The feature does not replace or disable other context management settings

### XML Representation
```xml
<task type="atomic">
  <description>Task description</description>
  <context_management>
    <!-- Standard context management settings -->
  </context_management>
  <file_paths>
    <path>./src/main.py</path>
    <path>/absolute/path/file.txt</path>
  </file_paths>
</task>
```

## Error Handling

### Error Detection and Response

#### Resource Exhaustion
- Handler detects limit violations
- Immediate task termination
- Surfaces through standard error type
- Resource accounting completion
- Clean Handler termination
- No retry attempts

#### Invalid Output
- Structural validation failure
- Format validation errors
- No semantic validation
- Preserves partial outputs for diagnostic purposes
- Returns both output and notes sections
- Detailed error reporting with context

#### Progress Failure
- Handler detects stalled execution
- Task-specific progress indicators
- No internal progress tracking
- Comprehensive error reporting
- State preserved in error response
- No automatic retry

#### XML Validation
- Immediate failure on invalid structure
- No warning collection
- Clear error messages
- Location information when available
- May trigger reparse if not disabled

#### Partial Results Preservation

The system preserves partial results for different task types to provide diagnostic context:

1. **Atomic Tasks**
   - Stores partial content in `notes.partialOutput`
   - Preserves as much of the generated output as possible
   - No structure guarantees for partial content
   - Used for error reporting and diagnostics

2. **Sequential Tasks**
   - Stores step-by-step outputs in `details.partialResults`
   - Includes metadata such as `failedStep` and `totalSteps`
   - Each step result includes `stepIndex`, `output`, and optional `notes`
   - Provides execution trace for debugging purposes

3. **Reduce Tasks**
   - Stores processed input results in `details.partialResults`
   - Includes the current accumulator state in `details.currentAccumulator`
   - Tracks processed inputs in `details.processedInputs`
   - Records the failed input index in `details.failedInputIndex`
   - Enables detailed error analysis

#### Format Control

The `accumulation_format` setting in the task's `context_management` block controls how much data is preserved:

```xml
<context_management>
    <inherit_context>full</inherit_context>
    <accumulate_data>true</accumulate_data>
    <accumulation_format>notes_only|full_output</accumulation_format>
    <fresh_context>enabled</fresh_context>
</context_management>
```

- `notes_only`: Only the notes field is preserved (default for memory efficiency)
- `full_output`: Both content and notes fields are preserved (with size limits)

#### Size Management

To prevent memory issues with partial results:
- Individual outputs are kept reasonably sized
- When accumulated data becomes too large, older results may be summarized or truncated
- The system indicates when truncation has occurred in the error details
- A maximum size cap applies regardless of settings

### Error Response

#### Error Surfacing
- Uses standard error type system (see types.md)
- Includes relevant task notes
- Preserves partial outputs when useful
- Includes resource metrics where applicable
- Clear error messages and locations

#### Handler Cleanup
- Clean termination of LLM session
- Resource accounting completion
- No state preservation
- Context window cleanup
- Turn counter finalization

#### Recovery Delegation
- No retry attempts
- Delegates to evaluator
- Provides complete error context
- Includes notes for recovery guidance
</file>
<file path="./components/task-system/spec/qa.md" project="">
# Task System FAQ

This FAQ provides concise answers to common questions about the Task System.

**Q: How many Handlers should be created per task execution?**  
A: One Handler per task execution. See [Task System Requirements](requirements.md) for details.

**Q: Who is responsible for sending prompts to the LLM?**  
A: The Handler is responsible for all LLM interactions.

**Q: How are resource limits enforced?**  
A: Resource limits are passed to the Handler at initialization and enforced during execution. For more details, see [Task System Behaviors](behaviors.md).

For additional questions, please refer to system‑level documentation or contact the Task System maintainers.
</file>
<file path="./components/task-system/spec/interfaces.md" project="">
# Task System Interfaces

// For core type definitions (e.g. TaskResult, TaskTemplate, TaskType, AtomicTaskSubtype),
// please refer to components/task-system/spec/types.md.

import { MemorySystem } from "../../memory/api/interfaces";

/**
 * TaskSystem Interface
 * 
 * Provides methods to execute tasks, validate templates, and find matching templates.
 */
export interface TaskSystem {
    /**
     * Execute a task, automatically handling continuations via tool responses
     * 
     * If a task returns CONTINUATION status with a subtask_request:
     * 1. The subtask is executed
     * 2. The result is added as a tool response to the parent's session
     * 3. The parent task continues execution with the tool result available
     * 
     * @param task - The task to execute
     * @param memory - The Memory System instance
     * @param taskType - Optional task type override
     * @returns Promise resolving to the final task result
     */
    executeTask(
        task: string,
        memory: MemorySystem,
        taskType?: "atomic" | "sequential" | "reduce" | "script"
    ): Promise<TaskResult>;

    validateTemplate(template: TaskTemplate): boolean;
    
    /**
     * findMatchingTasks
     *
     * Finds matching templates based on a provided input string.
     *
     * Note: Matching applies *only* to atomic task templates. The function evaluates the input
     * against atomic task templates using a heuristic scoring mechanism.
     * 
     * @param input - The natural language task description.
     * @param context - The MemorySystem instance providing context data.
     * @returns An array of matching candidates with their associated scores.
     */
    findMatchingTasks(
        input: string,
        context: MemorySystem
    ): Promise<Array<{
        template: TaskTemplate;
        score: number;
        taskType: "atomic" | "sequential" | "reduce" | "script";
    }>>;
    registerTask(taskDef: TaskDefinition): void;
    executeFunctionCall(funcCall: FunctionCall, env: Environment): Promise<any>;
    
    /**
     * Register a template in the TaskLibrary
     * 
     * @param template - The template to register
     * @returns Promise resolving to registration result
     */
    registerTemplate(template: TemplateNode): Promise<void>;
    
    /**
     * Execute a function call
     * 
     * @param call - The function call to execute
     * @param env - The environment for argument evaluation
     * @returns Promise resolving to the function result
     */
    executeCall(call: FunctionCallNode, env: Environment): Promise<TaskResult>;
export interface Environment {
    bindings: Record<string, any>;
    outer?: Environment;
    /**
     * Perform a lexical lookup for varName.
     * Returns the value if found; otherwise, throws an error.
     */
    find(varName: string): any;
    executeScriptTask(scriptTask: ScriptTask, env: Environment): Promise<ScriptTaskResult>;
    
    /**
     * Create a new child environment with additional bindings
     * 
     * @param bindings - New variable bindings to add
     * @returns A new Environment with the added bindings
     */
    extend(bindings: Record<string, any>): Environment;
}

// Handler interface details are maintained in external documentation.
 * Memory System interface - Version 3.0
 * Provides metadata management and context retrieval
 * Follows a read-only context model (no updateContext capability)
 */
type FileMetadata = string;

type GlobalIndex = Map<string, FileMetadata>;

type FileMatch = [string, string | undefined];

interface AssociativeMatchResult {
    context: string;      // Unstructured data context
    matches: FileMatch[]; // Relevant file matches
}

interface MemorySystem {
    // Get global file metadata index
    getGlobalIndex(): Promise<GlobalIndex>;
    
    // Update global file metadata index
    updateGlobalIndex(index: GlobalIndex): Promise<void>;
    
    // Retrieve context using associative matching
    getRelevantContextFor(input: ContextGenerationInput): Promise<AssociativeMatchResult>;
}
```

### Example Definitions

**ContextGenerationInput Example:**
```json
{
    "taskText": "Analyze experimental data",
    "inheritedContext": "Optional inherited context from previous tasks (if not disabled)",
    "previousOutputs": "Optional string summarizing prior outputs"
}
```

**AssociativeMatchResult Example:**
```json
{
    "context": "Relevant retrieved context information",
    "matches": [
        ["fileA.txt", "metadata details"],
        ["fileB.txt", null]
    ]
}
```

### Handler Interface
```typescript
/**
 * Types specific to Handler interface
 */
interface HandlerConfig {
    maxTurns: number;
    maxContextWindowFraction: number;
    defaultModel?: string;
    systemPrompt: string;
}

/**
 * LLM interaction interface
 * Uses [Type:TaskSystem:ResourceMetrics:1.0], [Type:TaskSystem:ResourceLimits:1.0]
 */
interface Handler {
    /**
     * Execute a prompt with the LLM
     * Note: All template substitution should be performed by the Evaluator before calling
     * @param systemPrompt - System-level context and instructions (fully resolved)
     * @param taskPrompt - Task-specific input (fully resolved)
     * @returns Promise resolving to LLM response
     */
    executePrompt(
        systemPrompt: string,
        taskPrompt: string
    ): Promise<string>;

    /**
     * Register a direct tool that will be executed by the Handler
     * @param name - Unique tool name
     * @param handler - Function that implements the tool
     */
    registerDirectTool(name: string, handler: Function): void;

    /**
     * Register a subtask tool that will be implemented via CONTINUATION
     * @param name - Unique tool name
     * @param templateHints - Hints for template selection
     */
    registerSubtaskTool(name: string, templateHints: string[]): void;

    /**
     * Add a tool response to the session
     * Used for adding subtask results to parent tasks
     * @param toolName - Name of the tool that produced the response
     * @param response - The tool response content
     */
    addToolResponse(toolName: string, response: string): void;

    /**
     * Callback for handling agent input requests
     * @param agentRequest - The agent's request for user input
     * @returns Promise resolving to user's input
     */
    onRequestInput: (agentRequest: string) => Promise<string>;
}
```
</file>
<file path="./components/task-system/spec/types.md" project="">
# Task System Types [Type:TaskSystem:1.0]

> This document is the authoritative source for Task System specific types.

## Core Types

```typescript
/**
 * Task execution result
 * [Type:TaskSystem:TaskResult:1.0]
 */
interface TaskResult {
    /**
     * Output content from the task execution.
     * Contains complete output if status is COMPLETE.
     * May contain partial output if status is FAILED or CONTINUATION.
     */
    content: string;
    
    /**
     * Execution status of the task.
     * COMPLETE: Task finished successfully, content is complete
     * CONTINUATION: Task needs additional steps, content may be partial
     * FAILED: Task encountered an error, content may be partial
     */
    status: ReturnStatus;
    
    /**
     * Optional free-form description used for dynamic evaluation template selection.
     */
    criteria?: string;
    
    /**
     * Parsed content if output was successfully parsed as JSON.
     */
    parsedContent?: any;
    
    /**
     * Task-level metadata about execution, resource usage, and status.
     * Contains ONLY metadata, never content.
     */
    notes: {
        dataUsage?: string;
        successScore?: number;
        [key: string]: any;
    };
}

/**
 * Base task definition interface
 * [Type:TaskSystem:BaseTaskDefinition:1.0]
 */
interface BaseTaskDefinition {
    description: string;
    type: TaskType;
    subtype?: string;
    
    /**
     * Specific files to include in task context.
     * These files will always be included regardless of other context settings.
     * Paths can be absolute or relative to repo root.
     * Invalid paths will generate warnings but execution will continue.
     */
    file_paths?: string[];
    
    context_management?: ContextManagement;
    inputs?: Record<string, any>;
}

/**
 * Task Template Interface
 * [Type:TaskSystem:TaskTemplate:1.0]
 * 
 * Any task type (atomic, sequential, reduce, script, etc.) can be defined as a template.
 * However, only atomic task templates participate in the template matching process.
 * Composite tasks can be either defined directly, defined as templates, or 
 * constructed by combining multiple atomic task templates.
 */
interface TaskTemplate {
    readonly taskPrompt: string;      // Maps to <instructions> in schema
    readonly systemPrompt: string;    // Maps to <system> in schema
    readonly provider?: string;       // Maps to <provider> in schema
    readonly model?: string;          // Maps to <model> in schema
    readonly inputs?: Record<string, string>;
    readonly isManualXML?: boolean;   // Maps to <manual_xml> in schema
    readonly disableReparsing?: boolean; // Maps to <disable_reparsing> in schema
    readonly taskType?: TaskType;     // The type of task this template defines
    readonly atomicSubtype?: AtomicTaskSubtype; // Only for atomic task templates
}

/**
 * Represents a sequential task which has its own context management block
 * and multiple steps of subtasks.
 * [Type:TaskSystem:SequentialTask:1.0]
 */
interface SequentialTask extends BaseTaskDefinition {
    type: 'sequential';
    contextManagement: ContextManagement;
    steps: BaseTaskDefinition[];
}

/**
 * Represents a function call expression
 * [Type:TaskSystem:FunctionCallNode:1.0]
 */
interface FunctionCallNode {
    type: "call";
    templateName: string;
    arguments: ArgumentNode[];  // Evaluated in caller's environment
}

/**
 * Represents an argument to a function call
 * [Type:TaskSystem:ArgumentNode:1.0]
 */
interface ArgumentNode {
    type: "argument";
    value: string | any;  // String for variables/literals, object for nested
}

/**
 * Represents a template node in the AST
 * [Type:TaskSystem:TemplateNode:1.0]
 */
interface TemplateNode {
    name: string;
    parameters: string[];
    returns?: string;
    body: BaseTaskDefinition;
}

/**
 * Subtask Request - Used for LLM-to-LLM delegation via continuation
 * [Type:TaskSystem:SubtaskRequest:1.0]
 */
interface SubtaskRequest {
    /** Type of subtask to spawn */
    type: TaskType;
    
    /** Description of the subtask */
    description: string;
    
    /** Input parameters for the subtask */
    inputs: Record<string, any>;
    
    /** Optional hints for template selection */
    template_hints?: string[];
    
    /** Optional context management overrides */
    context_management?: {
        inherit_context?: 'full' | 'none' | 'subset';
        accumulate_data?: boolean;
        accumulation_format?: 'notes_only' | 'full_output';
        fresh_context?: 'enabled' | 'disabled';
    };
    
    /** Optional maximum nesting depth override */
    max_depth?: number;
    
    /** Optional subtype for atomic tasks */
    subtype?: string;
  
    /** 
     * Specific files to include in subtask context.
     * These files will always be included regardless of other context settings.
     * Paths can be absolute or relative to repo root.
     * Invalid paths will generate warnings but execution will continue.
     */
    file_paths?: string[];
}

/**
 * Specialized result structure for evaluator feedback
 * [Type:TaskSystem:EvaluationResult:1.0]
 */
interface EvaluationResult extends TaskResult {
    notes: {
        success: boolean;        // Whether the evaluation passed
        feedback: string;        // Human-readable feedback message
        details?: {              // Optional structured details
            metrics?: Record<string, number>; // Optional evaluation metrics
            violations?: string[];            // Specific validation failures
            suggestions?: string[];           // Suggested improvements
            [key: string]: any;               // Extension point
        };
        scriptOutput?: {         // Present when script execution is involved
            stdout: string;      // Standard output from script
            stderr: string;      // Standard error output from script
            exitCode: number;    // Exit code from script
        };
    };
}

/**
 * Script execution configuration
 * [Type:TaskSystem:ScriptExecution:1.0]
 */
interface ScriptExecution {
    command: string;
    timeout?: number;
    inputs: Record<string, string>;
}

/**
 * Represents the result of executing a script task
 * [Type:TaskSystem:ScriptTaskResult:1.0]
 */
interface ScriptTaskResult extends TaskResult {
    stdout: string;
    stderr: string;
    exitCode: number;
}
```

## Type References

For system-wide types, see [Type:System:1.0] in `/system/contracts/types.md`, which includes:
- TaskType and AtomicTaskSubtype enums
- ReturnStatus enum
- ContextManagement interface
- TaskError types
- ResourceMetrics interface

## Cross-References

- For XML schema definitions, see [Contract:Tasks:TemplateSchema:1.0] in `/system/contracts/protocols.md`
- For interface implementations, see `/components/task-system/spec/interfaces.md`
- For public API surface, see [Interface:TaskSystem:1.0] in `/components/task-system/api/interfaces.md`
</file>
<file path="./components/task-system/README.md" project="">
# Task System Component

## Overview

The Task System orchestrates LLM task execution through structured XML templates and handlers. It proovides template-based task definition, resource tracking, and an XML-based interface with the LLM.

## Core Architecture

The system manages task execution through isolated Handler instances, with one Handler per task to enforce resource limits and manage LLM interactions. 

Task definitions use an XML-based template system that supports both manual and LLM-generated structures. 

## Core Interface

```typescript
interface TaskSystem {
    executeTask(
        task: string,
        context: MemorySystem,
        taskType?: TaskType
    ): Promise<TaskResult>;

    validateTemplate(template: TaskTemplate): boolean;
    
    findMatchingTasks(
        input: string,
        context: MemorySystem
    ): Promise<Array<{
        template: TaskTemplate;
        score: number;
        taskType: TaskType;
    }>>;
}
```

## Task Types and Execution

The system supports several task types for different execution patterns. Atomic tasks provide direct LLM execution with resource tracking and output validation. Sequential tasks enable ordered execution with context management between steps. Reduce tasks handle iterative data processing with accumulator management. Script tasks support external command execution with output capture and evaluation flow integration.

For all task types, the Evaluator is responsible for resolving template variables (e.g., `{{variable_name}}` placeholders) before passing fully-resolved content to the Handler for execution. This ensures consistent variable resolution across all task types and execution paths.

All task types (atomic, sequential, reduce, script) can be defined as templates and functions in the TaskLibrary. However, template matching (the process of selecting an appropriate template based on a natural language description) applies only to atomic task templates. Composite tasks can be defined directly, defined as reusable templates, or assembled from matched atomic task templates.

Each task type can specify its context management requirements and explicit file paths through XML configuration:

```xml
<task>
    <description>Task description</description>
    <context_management>
        <inherit_context>none|full|subset</inherit_context>
        <accumulate_data>true|false</accumulate_data>
        <accumulation_format>notes_only|full_output</accumulation_format>
        <fresh_context>enabled|disabled</fresh_context>
    </context_management>
    <file_paths>
        <path>./src/main.py</path>
        <path>/absolute/path/file.txt</path>
    </file_paths>
    <inputs>
        <input name="input_name" from="source_var"/>
    </inputs>
</task>
```

### Function Templates

The system supports function-based templates with explicit parameter declarations:

```xml
<template name="analyze_data" params="dataset,config">
  <task>
    <description>Analyze {{dataset}} using {{config}}</description>
  </task>
</template>
```

These templates are called with positional arguments:

```xml
<call template="analyze_data">
  <arg>weather_data</arg>
  <arg>standard_config</arg>
</call>
```

Key characteristics:

- Templates can only access explicitly passed parameters
- Arguments are evaluated in the caller's environment
- Function calls create a new lexical scope
- Template registration happens automatically during parsing

## Delegation Mechanisms

The system exposes a unified tool interface with two implementation approaches:

1. **Unified Tool Interface**: What the LLM sees and interacts with
   - Consistent invocation patterns for all operations
   - Standardized parameter schemas
   - Unified error handling

2. **Implementation Mechanisms**: How tools are executed
   - **Direct Implementation**: Synchronous Handler execution for simple operations
   - **Subtask Implementation**: Asynchronous execution via CONTINUATION

See [Pattern:ToolInterface:1.0] for complete details on this approach.

## Integration and Dependencies

The Task System integrates with several core components. It uses the Memory System for context access and management, Handler Tools for file and system operations, the Compiler for task parsing and transformation, and the Evaluator for error recovery and task decomposition. These integrations enable comprehensive task execution while maintaining clean component boundaries.

## Usage

Here's how the Task System would be instantiated:

```typescript
const taskSystem = new TaskSystem({
    provider: "anthropic",  // Default provider
    maxTurns: 10,
    maxContextWindowFraction: 0.8,
    systemPrompt: "Default system prompt"
});

// Execute a task
const result = await taskSystem.executeTask(
    "analyze data",
    memorySystem
);

// Execute with provider override
const resultWithOverride = await taskSystem.executeTask(
    "analyze data",
    memorySystem,
    { provider: "openai" }  // Override for this task
);

// Execute task with specific file paths
const resultWithFiles = await taskSystem.executeTask(
    "<task type='atomic'><description>Analyze files</description><file_paths><path>./src/main.py</path></file_paths></task>",
    memorySystem
);

// Validate a template
const validation = taskSystem.validateTemplate({
    taskPrompt: "<task>...</task>",
    systemPrompt: "System context",
    model: "claude-3-sonnet",
    isManualXML: false
});

// Register a template
const templateResult = await taskSystem.registerTemplate({
  name: "process_data",
  parameters: ["input_file", "options"],
  body: {
    type: "atomic",
    description: "Process {{input_file}} with options {{options}}",
    // Additional task properties
  }
});

// Call a template with arguments
const callResult = await taskSystem.executeCall({
  templateName: "process_data",
  arguments: ["data.csv", {format: "standard"}]
});
```

## Error Handling

The system handles several error types during execution, including resource exhaustion (for turns, context, or output), invalid output structure, XML parsing or validation errors, and general task execution failures. Each error type includes relevant context and metrics to aid in recovery and debugging. Errors will be surfaced to the Evaluator, which will use them for control flow.

## Resource Management

Resource management follows strict constraints with fixed context window sizes and limited turn counts. The system ensures clean resource release after task execution and prevents cross-Handler resource sharing. Handler configuration controls these limits:

```typescript
interface HandlerConfig {
    maxTurns: number;
    maxContextWindowFraction: number;
    defaultModel?: string;
    systemPrompt: string;
}
```

For detailed implementation specifications and patterns, refer to the component-level documentation and system contracts.
</file>
<file path="./components/task-system/api/interfaces.md" project="">
# Task System Public API [Interface:TaskSystem:1.0]

> This document is the authoritative source for the Task System public API.

## Overview

The Task System provides task execution, template management, and resource tracking services. It orchestrates the execution of tasks through structured XML templates and handlers.

## Core Interface

```typescript
/**
 * Primary task execution interface
 * [Interface:TaskSystem:1.0]
 */
interface TaskSystem {
    /**
     * Execute a task with the given parameters
     * 
     * @param task - The task to execute (XML string or task description)
     * @param memory - The Memory System instance
     * @param options - Optional execution options
     * @returns Promise resolving to the task result
     */
    executeTask(
        task: string,
        memory: MemorySystem,
        options?: {
            taskType?: TaskType;
            provider?: string;
            model?: string;
        }
    ): Promise<TaskResult>;

    /**
     * Validate a task template
     * 
     * @param template - The template to validate
     * @returns Whether the template is valid
     */
    validateTemplate(template: TaskTemplate): boolean;
    
    /**
     * Find matching templates based on input
     * 
     * Note: Matching applies *only* to atomic task templates.
     * 
     * @param input - The natural language task description
     * @param memory - The Memory System instance
     * @returns Promise resolving to matching templates with scores
     */
    findMatchingTasks(
        input: string,
        memory: MemorySystem
    ): Promise<Array<{
        template: TaskTemplate;
        score: number;
        taskType: TaskType;
    }>>;
    
    /**
     * Register a template in the TaskLibrary
     * 
     * @param template - The template to register
     * @returns Promise resolving to registration result
     */
    registerTemplate(template: TemplateNode): Promise<void>;
    
    /**
     * Execute a function call
     * 
     * @param call - The function call to execute
     * @param env - The environment for argument evaluation
     * @returns Promise resolving to the function result
     */
    executeCall(call: FunctionCallNode, env: Environment): Promise<TaskResult>;
}
```

## Type References

For Task System specific types, see [Type:TaskSystem:1.0] in `/components/task-system/spec/types.md`.
For system-wide types, see [Type:System:1.0] in `/system/contracts/types.md`.

## Integration Points

- **Memory System**: Used for context retrieval via [Interface:Memory:3.0]
- **Handler**: Used for LLM interactions and resource enforcement
- **Evaluator**: Used for task execution and error recovery
- **Compiler**: Used for task parsing and transformation

## Contract References

For XML schema definitions, see [Contract:Tasks:TemplateSchema:1.0] in `/system/contracts/protocols.md`.
For integration contract details, see [Contract:Integration:TaskSystem:1.0] in `/system/contracts/interfaces.md`.
</file>
<file path="./components/compiler/README.md" project="">
# Compiler Component

## Overview

The Compiler handles translation and transformation of tasks into executable formats. It parses natural language inputs into structured XML or AST representations that can be processed by the Evaluator.

## Core Architecture

The Compiler provides:
- Task parsing from natural language to XML/AST
- XML schema validation
- Template validation
- AST transformation capabilities

## Component Interface

```typescript
interface Compiler {
    /**
     * Parse natural language into structured XML or AST
     */
    parse(input: string): ASTNode;
    
    /**
     * Validate XML against schema
     */
    validateXML(xml: string): ValidationResult;
    
    /**
     * Transform AST nodes
     */
    transform(node: ASTNode, transformations: Transformation[]): ASTNode;
}
```

For detailed interface definitions, see [components/compiler/api/interfaces.md](./api/interfaces.md).

## Integration Points

- **Task System**: Receives parsed tasks for execution
- **Evaluator**: Uses AST nodes for execution
- **XML Schema**: Validates against [Contract:Tasks:TemplateSchema:1.0]

## Usage

```typescript
const compiler = new Compiler();

// Parse natural language to XML/AST
const ast = compiler.parse("Analyze the data in file.csv");

// Validate XML
const validationResult = compiler.validateXML(xmlString);
if (!validationResult.valid) {
    console.error("Validation errors:", validationResult.errors);
}

// Transform AST
const transformedAST = compiler.transform(ast, [
    { type: "optimize", options: { level: "basic" } }
]);
```

For detailed requirements, see [components/compiler/spec/requirements.md](./spec/requirements.md).
</file>
<file path="./TODOS.md" project="">
# TODOS and Implementation Plan

This document categorizes all remaining tasks and provides a prioritized implementation plan. The goals are to ensure system consistency, complete documentation, and implement remaining features in a logical order.

## Status Categories

Tasks are organized into four categories:

1. **Complete**: Implemented and documented in the system architecture
2. **Incomplete**: Well-defined goals that need implementation
3. **Inconsistent**: Items that conflict with other components or decisions
4. **Unclear**: Items requiring clarification before implementation

## Complete ✓

### Interface and Memory System
- Memory System interface version defined [Interface:Memory:3.0]
- File content type handling standardized (Handler tools)
- Unified storage approach defined in ADR
- Memory Component Separation with clear interfaces and integration documentation
- File tracking ownership decided (Memory System owns metadata, Handler owns file operations)
- Resource tracking responsibilities documented in Pattern:ResourceManagement:1.0
- File operation responsibility boundaries clearly defined:
  * Memory System: Manages ONLY metadata (file paths and descriptive strings)
  * Handler: Performs ALL file I/O operations using appropriate tools (including Anthropic's computer use tools for Anthropic models)
  * Clear separation of concerns between metadata management and file operations
- Memory System version consistently referenced as 3.0 across all documentation
- updateContext method references removed; documentation consistently reflects read-only context model
- Tool calls vs. subtasks boundaries clearly defined:
  * Tool calls: Handler-managed deterministic operations with rigid APIs
  * Subtasks: LLM-to-LLM interactions using continuation mechanism
  * Clear component responsibilities established for each type

### Context Management
- Document best practices (ADR 004, ADR 14)
- Define efficient subtask patterns (sequential)
- Extend `inherit_context` to map/reduce operators (ADR 14)
- Provide partial-result guidance via accumulation_format (ADR 9, ADR 14)
- Consolidate environment usage across context types
- Add context reuse mechanisms via context_management block
- Update error taxonomy for context failures (ADR 8)

### Task Type System & Patterns
- Basic task types defined and implemented
- Subtask support via standardized mechanism (ADR 11)
- Map operator implementation
- Director-Evaluator pattern with script execution integration
- Function-based templates with parameter declaration (ADR 12)
- Task continuation protocol via CONTINUATION status
- JSON-based output standardization (ADR 13)
- Partial results policy implemented (ADR 9)

## Incomplete ❌

### Phase 1: Documentation and Interface Consistency
1. **Update Task System implementation to use Memory System 3.0**
   - Ensure all Task System documentation references Memory System 3.0
   - Update interface method references to match current Memory System interface
   - Verify correct usage of file operations vs. metadata operations

2. **Remove or reconcile deprecated memory structures**
   - Remove references to `updateContext` method (deprecated in Memory System 3.0)
   - Update code examples to use current interface methods
   - Ensure consistent handling of read-only context model

3. **Align version numbers across documentation**
   - Audit all component version references
   - Ensure consistency in interface versions
   - Update any outdated references to match current architecture

4. **Update cross-references systematically**
   - Fix broken references between documents
   - Ensure cross-references point to canonical sources
   - Validate all [Interface:X:Y.Z] references

### Phase 2: Core Implementation Improvements
5. **Interface updates to align with current patterns**
   - Review interface definitions for consistency with architecture decisions
   - Update type signatures and method names as needed
   - Ensure interfaces follow established patterns (Memory System is read-only, etc.)

6. **Document context management patterns for common scenarios**
   - Create comprehensive examples for context clearing and regeneration
   - Update documentation with recommended settings for different use cases
   - Ensure consistent guidance across all component documentation

7. **Summary output handling in evaluator**
   - Implement mechanism for efficient summary outputs between tasks
   - Define standard summary format
   - Document integration with context management

8. **Document subtask usage patterns with context**
   - Create comprehensive documentation for subtask patterns
   - Include examples of context inheritance models
   - Demonstrate common subtask usage patterns

9. **Provide complete implementation examples**
   - Create end-to-end examples of task execution
   - Include error handling, context management, and file operations
   - Demonstrate component interactions

### Phase 3: Architecture and Use Cases
10. **Architecture Documentation standardization**
    - Ensure consistent documentation structure across components
    - Standardize terminology and diagram formats
    - Create consistent interaction descriptions

11. **Add missing cross-component documentation**
    - Document how components interact
    - Create diagrams showing data flow between components
    - Ensure integration points are clearly described

12. **Write user stories and examples**
    - Create comprehensive user stories showing system usage
    - Include examples of different workflows
    - Demonstrate system capabilities through concrete examples

### Phase 4: Advanced Features
13. **Multiple tries/selection of best candidate result**
    - Design mechanism for generating multiple solution candidates
    - Implement evaluation criteria
    - Create selection process for best results

14. **Agent file storage and recall mechanisms**
    - Design system for agents to store and recall information
    - Define file format and organization
    - Implement access methods

15. **Advanced debugging capabilities**
    - Implement comprehensive logging
    - Add step-by-step execution tracking
    - Create context inspection tools

16. **Agent features (history storage, REPL)**
    - Design and implement storage for agent conversation history
    - Create REPL interface for interactive task execution
    - Integrate with existing components

17. **Multi-LLM support**
    - Design abstraction layer for different LLM providers
    - Implement adapters for each provider's API
    - Ensure consistent behavior across models

18. **DSL optimization or alternative language support**
    - Evaluate current DSL performance and usability
    - Identify optimization opportunities
    - Consider alternative syntax or language support

## Inconsistent ⚠️

~~2. **Task-subtask context inheritance**~~
   ~~- Multiple conflicting descriptions exist~~
   ~~- Standardize on the approach defined in ADR 14~~
   
   *Completed: Implemented atomic task subtypes with different context management defaults. Standard atomic tasks inherit context but don't generate fresh context, while subtasks don't inherit context but do generate fresh context.*

## Unclear ❓

1. **"Genetic" behavior implementation**
   - Clarify how multiple tries and result selection would work
   - Define evaluation criteria for selecting best solutions

2. **Subtask spawning and evaluator interaction**
   - Detail exactly how subtasks spawned by task system work with evaluator
   - Specify control flow and context handling

3. **Architecture documentation "self-similarity"**
   - Define what "self-similarity in structure" means for documentation
   - Provide concrete examples of desired documentation patterns

4. **Agent-style workflow patterns**
   - Clarify implementation of "conversation → json → map spec prompts" pattern
   - Provide examples of intended workflow

5. **Chat queue implementation**
   - Specify requirements for chat queue management
   - Define integration with task system

6. **Shell node integration**
   - Clarify how "shell" nodes fit into the architecture
   - Define interaction with Director-Evaluator pattern

7. **Model temperature/selection handling**
   - Determine whether model parameters should be handled at system level
   - Define configuration approach

8. **Multi-try evaluation criteria**
   - Specify how to implement evaluation criteria for selecting best results
   - Define scoring mechanisms and decision process

## Dependencies and Critical Path

The critical path for completing the system involves:

1. Documentation and interface consistency (Phase 1)
2. Core implementation improvements (Phase 2)
3. Architecture documentation and use cases (Phase 3)
4. Advanced features (Phase 4)

Items within Phase 1 should be completed first as they provide the foundation for all subsequent work. Within each phase, items are listed in priority order based on dependencies and impact.


user story:
make plan docs interactively (or with a prompt queue), write them to file; add them to heritable context; gen spec prompts; subtask -> try to impl spec prompt, up to 3 round of debugging; on failure return to parent task 
CONTINUATION with failure notes; parent tries to debug (either itself or with full context subtask) or maybe something else. 
bonus: failure notes get interpreted as lessons and are used to update plan or project_rules.md

- does handler need a 'dumb' llm backend to prompt the main agent to continue?
- how will git integration work?
- get rid of natural language -> xml path, just use natural language -> dsl. this avoids nested xml ugliness, clarifies the use of xml as formatting for a single prompt
- option to pass list of file paths to subtask

## Resolved Questions

- ✓ RESOLVED: Template substitution is an Evaluator responsibility. The Evaluator resolves all {{variable_name}} placeholders before dispatching tasks to the Handler.
</file>
<file path="./IDL.md" project="">
// === IDL-CREATION-GUIDLINES === // Object Oriented: Use OO Design. // Design Patterns: Use Factory, Builder and Strategy patterns where possible // ** Complex parameters JSON : Use JSON where primitive params are not possible and document them in IDL like "Expected JSON format: { "key1": "type1", "key2": "type2" }" // == !! BEGIN IDL TEMPLATE !! === // === CODE-CREATION-RULES === // Strict Typing: Always use strict typing. Avoid using ambiguous or variant types. // Primitive Types: Favor the use of primitive types wherever possible. // Portability Mandate: Python code must be written with the intent to be ported to Java, Go, and JavaScript. Consider language-agnostic logic and avoid platform-specific dependencies. // No Side Effects: Functions should be pure, meaning their output should only be determined by their input without any observable side effects. // Testability: Ensure that every function and method is easily testable. Avoid tight coupling and consider dependency injection where applicable. // Documentation: Every function, method, and module should be thoroughly documented, especially if there's a nuance that's not directly evident from its signature. // Contractual Obligation: The definitions provided in this IDL are a strict contract. All specified interfaces, methods, and constraints must be implemented precisely as defined without deviation. // =======================

module GenericSystemName {

// Interface for a generic entity
interface EntityName {

    // Action/method definition
    // Preconditions:
    // - Define any preconditions here.
    // - Expected JSON format: { "key1": "type1", "key2": "type2" } 
    // Postconditions:
    // - Define the expected outcomes here.
    returnType methodName(parameterType parameterName);

    // Additional methods...
};

// Another entity or component
interface AnotherEntity {

    // Action/method definition
    // Preconditions:
    // - Define any preconditions here.
    // - Expected JSON format: { "key1": "type1", "key2": "type2" } 
    // Postconditions:
    // - Define the expected outcomes here.
    returnType anotherMethodName(parameterType parameterName);

    // Additional methods...
};
// == !! END IDL TEMPLATE !! ===

// EXAMPLE // === CODE-CREATION-RULES === // Strict Typing: Always use strict typing. Avoid using ambiguous or variant types. // Primitive Types: Favor the use of primitive types wherever possible. // Portability Mandate: Python code must be written with the intent to be ported to Java, Go, and JavaScript. Consider language-agnostic logic and avoid platform-specific dependencies. // No Side Effects: Functions should be pure, meaning their output should only be determined by their input without any observable side effects. // Testability: Ensure that every function and method is easily testable. Avoid tight coupling and consider dependency injection where applicable. // Documentation: Every function, method, and module should be thoroughly documented, especially if there's a nuance that's not directly evident from its signature. // Contractual Obligation: The definitions provided in this IDL are a strict contract. All specified interfaces, methods, and constraints must be implemented precisely as defined without deviation. // ======================= // == !! BEGIN TEMPLATE EXAMPLE !! === interface Tweets { // Preconditions: // - userID exists. // - tweetContent is non-null and within allowable size limits. // Postconditions: // - A new tweet is created and stored. // Expected JSON format: { "userID": "string", "content": "string" } void postTweet(string tweetJSON); // Preconditions: // - userID and tweetID exist. // Postconditions: // - The tweet with tweetID is marked as liked by userID. void likeTweet(string userID, string tweetID); // Preconditions: // - userID and tweetID exist. // Postconditions: // - The tweet with tweetID is marked as retweeted by userID. // Expected JSON format: { "userID": "string", "originalTweetID": "string" } void retweet(string retweetJSON); // Preconditions: // - tweetID exists. // Postconditions: // - Returns the details of the tweet as JSON. string getTweetDetails(string tweetID); // Invariants: // - Filesystem storage maintains a list of tweets, likes, and retweets for each tweetID. }; // == !! END TEMPLATE EXAMPLE !! ===
)
</file>
<file path="./plan.md" project="">
Implementation Priority for Remaining Critical Path Issues
Now that we've addressed Context Management Standardization, here's the recommended order for tackling the remaining critical path issues:
1. Error Taxonomy for Context Issues
Should be resolved first because:

It's foundational to all error handling in the system
Other mechanisms depend on clear error categorization
It affects control flow throughout the architecture
It's relatively straightforward to implement (adding/modifying error types)

Key decisions needed:

Whether to create a distinct CONTEXT_FAILURE error type
How to categorize different context generation failures
How errors propagate through nested operations

2. Partial Results Policy
Should be addressed second because:

It builds directly on the error taxonomy
It's needed for reliable multi-step operations
It determines how the system handles incomplete work
It affects error recovery strategies system-wide

Key decisions needed:

Whether operators should preserve intermediate outputs on failure
How partial results are structured in error responses
Whether partial outputs should be usable for recovery

3. Evaluator-to-Director Feedback Flow
Should be third because:

It's essential for the director-evaluator pattern to function
It establishes a model for task-subtask communication
It needs clear error handling and partial results policies
It's a concrete case that informs the broader subtask mechanism

Key decisions needed:

How evaluator output is passed back to the director
How context is maintained across continuation boundaries
Error handling within the feedback loop

4. Subtask Spawning Mechanism
Should be addressed last because:

It's the most complex of the critical issues
It benefits from resolving the previous issues first
It represents a broader extension of the task system
It requires careful integration with existing patterns

Key decisions needed:

How tasks dynamically create and bind subtasks
How the Evaluator creates subtasks from task notes
Control flow between parent tasks and subtasks

Implementation Strategy
This ordering minimizes rework by resolving foundational issues before their dependencies. A useful approach would be:

Draft ADRs for each issue in the recommended order
Implement each feature incrementally
Test thoroughly before moving to the next priority

This sequence ensures that core error handling is solidified before building more complex interaction patterns on top of it.

------

Implementation Priority for High Priority Items
Following Context Management and the Critical Path issues, here's the recommended order for implementing the High Priority items:
1. Memory System Version Consolidation
Should be addressed first because:

It's primarily a documentation and reference cleanup issue
It creates a clear foundation for all component interactions
It resolves ongoing confusion about Memory System interfaces
It has minimal code impact but high architectural clarity value

Key decisions needed:

Standardize on Memory System 3.0 throughout all docs
Clear separation between file metadata and content operations
Update all cross-references consistently
Document the final Memory System interface contract

Note: The previously considered "rebuild-memory" and "clear-memory" flags have been determined to be redundant with the existing context management model and will not be implemented. The same functionality can be achieved through appropriate configuration of <inherit_context>, <accumulate_data>, and <fresh_context> settings.

2. XML-to-AST Mapping
Should be second because:

It defines how configuration translates to execution structure
It's fundamental to all operator implementations
It establishes clear parsing rules for the context attributes we just defined
It bridges the gap between authored tasks and runtime representation

Key decisions needed:

Formal mapping between XML elements and AST nodes
Handling of attribute inheritance in nested structures
Namespace and versioning considerations
Validation and error reporting during parsing

3. Output Standardization
Should be third because:

It enables reliable data flow between components
It's necessary for variable binding and task composition
It builds on clear AST structures
It's relatively self-contained and can be implemented incrementally

Key decisions needed:

Whether to enforce JSON for all outputs
Schema or structure requirements for outputs
How outputs are parsed and validated
Standard conventions for error reporting in outputs

4. DSL Function Implementation
Should be fourth because:

It relies on standardized outputs for parameter passing
It needs clear XML-to-AST mapping for function definition
It represents a significant feature that builds on the previous foundations
It enables higher-level composition patterns

Key decisions needed:

Task library in-memory representation
Function definition syntax and semantics
Scope and binding rules
How to represent and execute composite functions

5. Tool vs. Subtask Distinction
Should be addressed last because:

It refines execution patterns established by earlier items
It's more concerned with semantics than infrastructure
It may benefit from experience with function implementation
It requires clear context management (which we've already addressed)

Key decisions needed:

Clear boundary definition between tools and subtasks
Interface contracts for each type
Resource management differences
Whether tools should have standardized output formats

Implementation Strategy
This ordering follows a logical dependency chain:

First consolidate the Memory System interface to provide a stable foundation
Then establish the XML-to-AST mapping to ensure consistent parsing
Standardize outputs to enable reliable data flow between components
Implement function mechanisms that depend on standardized I/O
Finally clarify the tool/subtask boundary to refine execution patterns

This sequence builds from low-level infrastructure toward higher-level patterns, minimizing rework and creating a stable foundation for each subsequent feature.
</file>
